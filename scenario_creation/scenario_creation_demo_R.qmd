---
title: Creating simple scenarios
author: Galen Holt
format:
  html:
    df-print: paged
editor: visual
engine: knitr
params:
  REBUILD_DATA: FALSE
---

```{r}
#| echo: false

# params don't get seen in interactive running, so handle that.
if ("params" %in% ls()) {
  REBUILD_DATA <- params$REBUILD_DATA
} else {
  REBUILD_DATA <- FALSE
}
```

```{r}
#| warning: false
#| message: false
library(werptoolkitr)
library(ggplot2)
library(sf)
library(reticulate) # Not strictly necessary but allows easier referencing of objects
```

# Overview

## Toolkit relevance

The creation of flow scenarios is not part of the toolkit proper. Instead, the toolkit expects to ingest hydrographs and then handles the ongoing response models, aggregation, and analyses. Thus, hydrographs are an essential input to the toolkit. The point of this code is to generate those hydrographs.

This notebook creates a minimal set of hydrographs to test and demonstrate the toolkit. The primary needs are multiple guages in multiple catchments (or other spatial units), and scenarios defined by different hydrographs for the same gauge.

## Process

We pull a limited set of gauges for a limited time period to keep this dataset small. Primarily, we identify a set of gauges in two catchments, pull them for a short time period, and adjust them to create two simple modified scenarios, with the original data serving as the baseline scenario. Along the way, we examine the data in various ways to visualise what we're doing and where.

A larger and more complex set of scenarios is created in [the flow scaling demonstration](pull_gauges_to_scale.qmd), without as much visualisation.

## Paths and other data

The shapefiles used to see what we're doing and do the selecting were produced with within the WERP_toolkit package to keep consistency. It's possible we'll add more shapefile creation and move all the canonical versions and their creation to their own data package or repo.

Set the data directory to make that easy to change. These should usually point to external shared directories. For this simple example though, we put the data inside the repo to make it self contained. The [larger example](scaling_scenarios.qmd) sends them externally, which would be more typical.

```{r}
scenario_dir <- 'scenario_example'
hydro_dir <- file.path(scenario_dir, 'hydrographs')
```

::: {#language style="color: gray"}
## Language note

This notebook was originally built using only python, and there is still [a python-only version](scenario_creation_demo_py.qmd), though it is maintained less frequently. Using Python makes a lot of sense because the underlying data here uses python packages. I've moved the active version of this notebook to R, however, when the toolkit became an R package and the [flow scaling demonstration](pull_gauges_to_scale.qmd) ended up using R gauge pullers. There is still some remaining python in here (pulling gauges and some minor EWR functions). This notebook provides an example of how to mix R and python code chunks, which we do fairly frequently.

We can access python objects in R with `py$objectname`and access R objects in python with `r.objectname` .

It takes -forever- to do a type translation on the `DATETIME` column in the gauge data. It's unclear why (can't replicate it with any other datetime py object). We work around that by changing it to something simple while still in python, and change it back to datetime in R.
:::

## Spatial datasets

We use spatial datasets provided by [{werptoolkitr}](https://github.com/MDBAuth/WERP_toolkit), which creates a standard set in `data_creation/spatial_data_creation.qmd`. These are visualised in [a separate notebook](overview/spatial_data.qmd). Relevant to this scenario creation, we are interested in the gauges, (`werptoolkitr::bom_basin_gauges`) since this is what were contained in the EWR tool. We use the `sdl_units` dataset to obtain a subset of gauges for these simple scenarios. Relevant to the case study- the original polygon used was the Macquarie-Castlereagh in the resource_plan_areas, though we seem to use sdl units elsewhere, so I'll use them here.

# Subset the gauges

We need multiple catchments for demos, so let's use the Macquarie, Castlereagh, Namoi, Lachlan.

```{r}
catch_demo <- sdl_units %>% 
  dplyr::filter(SWSDLName %in% c("Macquarieâ€“Castlereagh", "Lachlan", "Namoi"))
```

## Get relevant gauges

Cut the bom_basin_gauges from the whole country to just those four catchments

```{r}
demo_gauges <- st_intersection(bom_basin_gauges, catch_demo)
```

How many are there?

```{r}
demo_gauges %>% nrow()
```

That's a fair number, but they won't all be in the EWR.

### Extract their names

To feed to the gauge puller, we need their gauge numbers.

```{r}
gaugenums <- demo_gauges$gauge
```

### Find those relevant to toolkit

We have the list of gauges, but now we need to cut the list down to those in the EWR tool. There's not any point in pulling gauges that do not appear later in the toolkit.

Which gauges are actually in the EWR tool? The EWR tool has a function, so use that.

::: {#ewr_update style="color: gray"}
**TODO** THIS FAILS AS OF 1.0.4. I have rolled back to ewr version 1.0.1, since the necessary file just doesn't exist in 1.0.4 (and in about half the branches on github). This needs to be updated and tested.

Error messages:

```         
FileNotFoundError: [Errno 2] No such file or directory: 'py_ewr/parameter_metadata/NSWEWR.csv'

Error in py_get_attr_impl(x, name, silent) : 
  AttributeError: module '__main__' has no attribute 'ewrs'
```
:::

A simple {python} chunk works fine in Rstudio and for `quarto render` from command line. But in vscode, the default (and unchangeable at present) is to start a whole new engine that doesn't talk to R, rather than using `reticulate::repl_python` as is done in Rstudio and by quarto itself.

```{{python}}
from py_ewr.data_inputs import get_EWR_table
ewrs, badewrs = get_EWR_table()
```

```{r}
pdi <- import("py_ewr.data_inputs")
ewrs_in_pyewr <- pdi$get_EWR_table() 
names(ewrs_in_pyewr) <- c('ewrs', 'badewrs')
```

What are those gauges, and which are in both the ewr and the desired catchments?

The way that works everywhere but vscode- this seems universal though

```{r}
ewrgauges <- ewrs_in_pyewr$ewr$Gauge
ewr_demo_gauges <- gaugenums[gaugenums %in% ewrgauges]
length(ewr_demo_gauges)
```

47 isn't too many.

### Get all the gauge data

Now we have a list of gauges, we need their hydrographs. We need a reasonable time span to account for temporal variation, but not too long- this is a simple case. Let's choose 10 years.

```{r}
starttime = lubridate::ymd(20100101)
endtime = lubridate::ymd(20191231)
```

Pull the gauges with `mdba_gauge_getter`. The type-translation that happens in here is because translating from python time to R time is extremely slow for this particular case (though not in general).

This again needs a vscode translation

``` python

#| message: false
import mdba_gauge_getter as gg
demo_levs = gg.gauge_pull(r.ewr_demo_gauges, start_time_user = r.starttime, end_time_user = r.endtime)
demo_ids = demo_levs.SITEID.unique()
len(demo_ids)

# I think this will work, the above is running
demo_levs['Date'] = demo_levs['DATETIME'].astype(str)
```

```{r}
gg <- import('mdba_gauge_getter')
demo_levs <- gg$gauge_pull(ewr_demo_gauges, start_time_user = starttime, end_time_user = endtime)
demo_ids <- unique(demo_levs$SITEID)
length(demo_ids)
```

Do a bit of cleanup- for some reason `demo_levs['VALUE']` is an object and not numeric, and `'DATETIME'` needs to be named `Date` for the EWR tool to read it. I copy the py object to R for this manipulation and visualisation, but we could just proceed in python if we wanted.

Again, we need to change things to run on vs.

``` r

demo_levs <- py$demo_levs

demo_levs$VALUE = as.numeric(demo_levs$VALUE)

# # In python, we just need to change the name of the date column. Here, we need to change the python datetime.date objects to R dates
# 
# # Really slow
# # MUCH faster to just make the dates characters in python, and back to dates here.
# rdates <- purrr::map(demodates, py_to_r) %>% 
#   tibble(.name_repair = ~'Date') %>%  
#   unnest(cols = Date)
# 
# demo_levs <- bind_cols(rdates, demo_levs)
demo_levs <- dplyr::select(demo_levs, -DATETIME) %>% 
  dplyr::mutate(Date = lubridate::ymd(Date))
```

The new version has to use the `reticulate::py_to_r`, because when we're not in the repl, we have an R object to work with for every line of python. Should probably just change to {vicwater}, like we do for flow scaling.

```{r}
demo_levs <- demo_levs |> 
  dplyr::mutate(VALUE = as.numeric(VALUE))

# 47 seconds- doesn't matter if preallocated
system.time(
  Date <- purrr::map(demo_levs$DATETIME, reticulate::py_to_r)
)

# # 50 seconds
# system.time(
#   Date <- lapply(demo_levs$DATETIME,FUN = reticulate::py_to_r)
# )

Date <- tibble::tibble(Date) |>
  tidyr::unnest(cols = Date)

demo_levs <- demo_levs |>
  dplyr::select(-DATETIME) |> 
  dplyr::bind_cols(Date) 

```

### Map the gauges

```{r}
demo_geo = bom_basin_gauges %>% dplyr::filter(gauge %in% demo_ids)
```

Looks reasonable. Probably overkill for testing, but can do a cut down version too.

The azure boxes have old GDAL, which can't read WKT2. Need to fix, but in the meantime, [force with the crs number](https://github.com/r-spatial/sf/issues/1419).

```{r}
if (grepl('npd-dat', Sys.info()['nodename'])) {
  st_crs(basin) <- 4283
  st_crs(catch_demo) <- 4283
  st_crs(demo_geo) <- 4283
}
```

```{r}
(ggplot() + 
geom_sf(data = basin, fill = 'lightsteelblue') +
geom_sf(data = catch_demo, mapping = aes(fill = SWSDLID)) +
geom_sf(data = demo_geo, color = 'black') +
scale_fill_brewer(type = 'qual', palette = 8))
```

# Make test scenarios

To generate simple and striking scenarios, we multiply the baseline gauge data by 4 or 0.25. This section could easily be modified for other simple scenarios. More complex scenarios are created in [scaling_scenarios](scaling_scenarios.qmd).

```{r}
down4 = demo_levs
up4 = demo_levs

down4$VALUE = down4$VALUE * 0.25
up4$VALUE = up4$VALUE * 4
```

### Make the data look like IQQM

The EWR tool has the capacity to pull gauge data directly, but because we have modified these data, they need to enter the EWR tool through `scenario_handling`, and so need to have a format the EWR tool can parse as output from one of the scenario generating tools it uses (IQQM, Source, etc). This could be avoided by splitting up `process_scenarios` into a read-in bit and a bit that calls `evaluate_EWRs.calc_sorter`. In the meantime, we modify this data to work if we tell the EWR tool it is IQQM.

Telling EWR the data is IQQM allows using `scenario_handling`. This requries having a date column `Date` and other columns with gauge numbers for names. The EWR tool can have multiple gauges, with each having its own column. Werptoolkitr expects directories for each scenario, with any number of csvs of hydrographs inside, (e.g. it could be one csv with columns for each gauge, or each csv could have a single gauge), which is handled by `paths_gauges` internally to the "Scenario controller" part of werptoolkitr. Here, we save a single csv with many gauges for each scenario. The [flow scaling example](scaling_scenarios.qmd) takes the opposite approach, with one gauge per csv due to different time periods.

### Save the output

This structure is useful, but it also may change once we have actual scenarios. And will almost certainly look different for netcdfs. Retaining the dir/file structure even with one file allows easier changes and more flexibility.

```{r}
scenenames <- c('base', 'up4', 'down4')

# the full scenarios
for (x in scenenames) {
      scenedir = file.path(hydro_dir, x)
    if (!dir.exists(scenedir)) {
      dir.create(scenedir, recursive = TRUE)
    }

}

```

Create clean dataframes to save.

```{r}
base <- demo_levs %>% 
  dplyr::select(Date, VALUE, SITEID) %>% 
  tidyr::pivot_wider(id_cols = Date, names_from = SITEID, values_from = VALUE)

up4 <- up4 %>% 
  dplyr::select(Date, VALUE, SITEID) %>% 
  tidyr::pivot_wider(id_cols = Date, names_from = SITEID, values_from = VALUE)

down4 <- down4 %>% 
  dplyr::select(Date, VALUE, SITEID) %>% 
  tidyr::pivot_wider(id_cols = Date, names_from = SITEID, values_from = VALUE) 
```

Save. could do this above easily enough, but getting lots of dots and hard to read.

```{r}
if (REBUILD_DATA) {
      readr::write_csv(base, file.path(hydro_dir, 'base', 'base.csv'))
  
      readr::write_csv(up4, file.path(hydro_dir, 'up4', 'up4.csv'))
      readr::write_csv(down4, file.path(hydro_dir, 'down4', 'down4.csv'))
}

```

That set of hydrographs can now be used as starting data for a demonstration of the toolkit proper.

And, let's assume we would receive scenarios with metadata, or we could create metadata about them. This is very simple for now, but it gives us something to improve on.

```{r}
flowmults <- c(0.25, 1, 4)
  
scenario_meta <- list(
  scenario_name = scenenames,
  flow_multiplier = flowmults
)

# I don't know the format we'll be using, but this works to create yaml metadata
yaml::write_yaml(scenario_meta, file = file.path(hydro_dir, 'scenario_metadata.yml'))
# and this does the same with JSON
jsonlite::write_json(scenario_meta, path = file.path(hydro_dir, 'scenario_metadata.json'))
```

Will there be one scenario metadata file? Or will there be separate ones assigned to each scenario, which then get brought together- e.g. it could be that when we get data from `base/` we read a `base/metadata.json` file that only has `{"scenario_name":["base"],"flow_multiplier":[1]}` in it, and then when they get compared, they get concatenated in a way the data can be used.

Depends on the user and use-case, really, so I'll set up an example of that too.

```{r}

for (i in 1:length(scenenames)) {
  jsonlite::write_json(list(scenario_name = scenenames[i], 
                            flow_multiplier = flowmults[i]), 
                       path = file.path(hydro_dir, scenenames[i], 'metadata.json'))
}
```
