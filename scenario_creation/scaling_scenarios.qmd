---
title: "Clean scaling"
author: "Galen Holt"
format: html
editor: visual
cache: false
params:
  REBUILD_DATA: TRUE
---

I'm using [pull_gauges_to_scale.qmd](pull_gauges_to_scale.qmd) to pull the gauges. Actually getting the scaling relationships is independent of the gauge data, and so doesn't need to happen in the same script. We could do this at the bottom of that script, but I've separated it because that notebook takes a while to run and is susceptible to intermittent API errors from the gauge service. Everything will be more stable if we do the transform here on a static set of hydrographs (or at least with control over when those hydrographs update).

I then read the gauge data in, apply the scaling, and save the scaled data to csvs to feed to the toolkit. I could do that in yet another notebook, but it seems to work in this document, and then doesn't rely on linking additional notebooks.

```{r}
#| message: false
library(werptoolkitr)
library(dplyr)
library(readr)
library(tidyr)
library(lubridate)
library(ggplot2)
# because mapping this causes problems on single processes
library(furrr)
plan(multisession)
```

We do a lot of `purrr`ing with custom functions, I've put them in their own file.

```{r}
source('scenario_creation/scaling_functions.R')
```

We need to set some paths to relevant directories- where we're going to save the hydrographs `hydro_dir` and where the scaling data is that we need to do the scaling here `scaling_dir`. This follows a more usual setup than [the short demo](scenario_creation_demo_R.qmd), in that we set these paths outside this repo. Typically, the hydrograph data would be in some external share directory (e.g. MDBA blob storage). Here, for simplicity for the demo, I just save it up a level.

```{r}
scenario_dir <- '../flow_scaling_data'
hydro_dir <- file.path(scenario_dir, 'hydrographs')
scaling_dir <- file.path(scenario_dir, 'CC_Scenarios_WRPs')
```

## Bring in data

Get the list of scenarios and read them in (to a list of dfs).

```{r}
#| message: false
CCSc_FileList <- list.files(scaling_dir, pattern = '.csv', 
                            full.names = TRUE)

scenario_list <- purrr::map(CCSc_FileList,
                            \(x) read_csv(file = x, id = 'path')) %>% 
  setNames(stringr::str_extract(CCSc_FileList, "SS[0-9]+"))
```

The [pulled hydrographs](pull_gauges_to_scale.qmd)

```{r}
orig_hydro <- readRDS(file.path(hydro_dir, 'extracted_flows.rds'))
```

# Scaling hydrographs

We have a set of runoff scenarios for different climate projections. These are done for 8 scenarios, and give separate results for each SDL unit. We want to create scaled versions of the historical hydrographs from these scenarios. To do that, we

1.  Find the mean quantile values in each scenario in each SDL unit for each month

2.  Find the 'relative change' from the baseline scenario

3.  Pull historical hydrographs for a set of gauges (done in the [gauge pulling notebook](pull_gauges_to_scale.qmd))

4.  Find the quantile for each datapoint in each hydrograph

5.  Adjust each hydrograph using quantile-quantile scaling to represent each of the runoff scenarios.

6.  The above does not change the zero-flow events, so we find a linear regression between the bottom 5% ranked historical flows and baseline runoff

    1.  We use this relationship to predict new flows from all runoff scenarios.

    2.  Because this relationship is linear, predictions can be positive or negative

    3.  Negative predictions are set to 0

    4.  Positive predictions applied to 0-flow are necessarily applied to *all* zero-flows. This is unrealistic, and so for this reason we only look at scenarios with reduced runoff.

7.  Save those adjusted hydrographs for use by the EWR tool

We achieve these goals by writing functions to do these operations, and using `purrr::map` or similar to apply them over all gauges (and relevant SDL units).

## Scenario definitions

Climate scenarios give runoff scenarios for each SDL unit (from David R.). The scenario conditions follow:

Prec and ETp (PE) are historical (not needed here)

SimR0 is simulated historical runnoff using actual historical Prec and ETp (PE)

SimR1 - SimR7 are simulated with +7% PE but different Rainfall:

1.  -20%

2.  -15% ("High change scenario")

3.  -10%

4.  -5% ("Moderate change scenario")

5.  +0%

6.  +5% ("Low change scenario")

7.  +10%

### Make numeric metadata

The format of this may change, but we're going to want something. Lists are going to be easier to yaml than dataframes (though a dataframe is easier to construct).

The scenario yamls are likely to *not* be lists, but single values, ie each one gets their own value to create it, and that's it (which makes sense). We can still make an overview scenario metadata table from them though.

```{r}

rain_multiplier <- seq(from = 0.8, to = 1.1, by = 0.05) %>% 
  setNames(paste0('SimR', 1:7))

scenario_meta <- list(
  PE_multiplier = c(1, 1, rep(1.07, length(rain_multiplier))),
  rain_multiplier = c(1, 1, rain_multiplier),
  scenario_name = c('Historical', 'SimR0', names(rain_multiplier))
)

# I don't know the format we'll be using, but this works to create yaml metadata
yaml::write_yaml(scenario_meta, file = file.path(hydro_dir, 'scaling_metadata.yml'))
```

Make sure directories exist for writing out results

```{r}
#| output: false
purrr::map(scenario_meta$scenario_name, 
           \(x) if (!dir.exists(file.path(hydro_dir, x))) {
             invisible(dir.create(file.path(hydro_dir, x), 
                        recursive = TRUE))
             })
```

# Find scaling relationships

We follow the basic method for q-q scaling from [climate change australia](https://www.climatechangeinaustralia.gov.au/en/obtain-data/application-ready-data/scaling-methods/), with the following modifications

-   Use 2% bins (e.g. 50 bins) instead of 10 + 10 in final

-   Month-matching quantiles (e.g. 90th %ile for June separate from September). The method given might do that too, it's unclear.

-   'Change ratio', $\frac{F-H}{H}$ replaced with 'relative change', $\frac{F}{H}$, and adjustment changed from $Obs(CR) + Obs$ to $Obs(RC)$.

    -   These are algebraically equivalent, and the change ratio seems unnecessarily convoluted

$$
Obs(\frac{F-H}{H}) + Obs = 
Obs(\frac{F}{H}-1}) + Obs = 
Obs(\frac{F}{H}) - Obs + Obs = 
Obs(\frac{F}{H})
$$

## Scaling factors for all SDL units

We need to apply `get_scalings` to every SDL unit in `scenario_list` to get the scaling factors for each scenario in that unit

```{r}
#| message: false
scaled_units <- purrr::map(scenario_list, get_scalings)
```

This `scaled_units` list is a list of dataframes (one for each SDL unit) giving the relative change for each scenario and month that need to be mapped to the observed gauge data.

# Adjust the flows

We now want to adjust the gauges with those relative changes to represent the different scenarios.

## Approach

1.  For each gauge in `orig_hydro`, find the quantile of each value for each group-unit (e.g. Month)
2.  multiply by `relative_change` for each quantile, month, and scenario in the relevant sdl unit in `scaled_units`
3.  do that over all gauges

## Cleanup

Some setup is needed to get the gauge types and sdl units.

It doesn't make sense to scale level gauges, so remove them.

```{r}
cg <- reticulate::import("py_ewr.observed_handling") 
gauge_cats <- cg$categorise_gauges(names(orig_hydro)) 
names(gauge_cats) <- c('flow', 'level', 'stage')    

# if they are level gauges, remove- the scaling doesn't make sense 
orig_hydro[which(names(orig_hydro) %in% gauge_cats$level)] <- NULL
```

In testing (scaling_scenarios_development.qmd), I looked into it and decided to set QC codes \> 150 to NA. That's build-in to the functions, but as an argument so we can change if desired.

## Do the transforms

Loop over the scaling functions to apply them to each gauge and relevant sdl unit, saving the csv results in the format expected by the Controller (and EWR). We also can save individual rds files, which are organised to favour a different dimension (gauge, rather than scenario), and so are easier to [assess the effects of the transforms](flow_scaling_analysis.qmd). Though here, we just return the whole list and save it in the next chunk.

A testing loop for a single or couple gauges

```{r}
#| eval: false
test_list <- purrr::map2(orig_hydro[10:20],
                         names(orig_hydro[10:20]),
                         \(x,y) scale_gauges(x, y,
                                             all_sdl_scenario_list = scenario_list,
                                              savedata = FALSE,
                                              saverds = FALSE, 
                                              returnR = TRUE))
```

This fails with `for`, `foreach %do%` and `purrr::map2` , with the weird exception of breaking it up into separate `for` loops, each with about 20 interations. After a *lot* of testing, it is not possible to isolate the problem (any particular run dies on a different iterations, and all iterations will run, just not all together). It seems to be an issue with R itself and maybe windows and the management of a single process. Parallelising it works, presumably because now it's running on multiple processes, forcing the sort of manual breakup that got the multiple `for` loops to run.

```{r}
#| cache: false
system.time(scaled_hydro <- furrr::future_map2(orig_hydro, 
                                               names(orig_hydro),
                                               \(x,y) scale_gauges(x, y,
                                                                   all_sdl_scenario_list = scenario_list,
                                              savedata = params$REBUILD_DATA,
                                              saverds = FALSE, 
                                              returnR = TRUE)))

```

Save the data all in one go. Useful for [assessing the impacts of the scaling](flow_scaling_analysis.qmd).

```{r}
if (params$REBUILD_DATA) {
  saveRDS(scaled_hydro, file = file.path(hydro_dir, 'scaled_hydrographs.rds'))
}
```
