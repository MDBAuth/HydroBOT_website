---
title: "Finding and applying flow scaling"
author: "Galen Holt"
format:
  html:
    df-print: paged
editor: visual
params:
  REBUILD_DATA: FALSE
---

```{r}
#| echo: false

# params don't get seen in interactive running, so handle that.
if ("params" %in% ls()) {
  REBUILD_DATA <- params$REBUILD_DATA
} else {
  REBUILD_DATA <- FALSE
}
```

I'm using [flow_scaling.qmd](flow_scaling.qmd) to pull the gauges. Actually getting the scaling relationships is independent of the gauge data, and so doesn't need to happen in the same script. We could do this at the bottom of that script, but I've separated it.

I then read the gauge data in, apply the scaling, and save the scaled data to csvs to feed to the toolkit. I could do that in yet another notebook, but it seems to work in this document, and then doesn't rely on linking additional notebooks.

```{r}
#| message: false
library(werptoolkitr)
library(dplyr)
library(readr)
library(tidyr)
library(lubridate)
library(ggplot2)
```

We do a lot of `purrr`ing with custom functions, I've put them in their own file

```{r}
source('scenario_creation/scaling_functions.R')
```

We need to set some paths to relevant directories- where we're going to save the hydrographs `hydro_dir` and where the scaling data is that we need to do the scaling here `scaling_dir`. This follows a more usual setup than [the short demo](scenario_creation_demo_R.qmd), in that we set these paths outside this repo. Typically, the hydrograph data would be in some external share directory (e.g. MDBA blob storage). Here, for simplicity for the demo, I just save it up a level.

```{r}
scenario_dir <- '../flow_scaling_data'
hydro_dir <- file.path(scenario_dir, 'hydrographs')
scaling_dir <- file.path(scenario_dir, 'CC_Scenarios_WRPs')
```

# Scaling hydrographs

We have a set of runoff scenarios for different climate projections. These are done for 8 scenarios, and give separate results for each SDL unit. We want to create scaled historical hydrographs from these scenarios. To do that, we

1.  Find the mean quantile values in each scenario in each SDL unit for each month

2.  Find the 'relative change' from the baseline scenario

3.  Pull historical hydrographs for a set of gauges (done in the [gauge pulling notebook](flow_scaling.qmd))

4.  Find the quantile for each datapoint in each hydrograph

5.  Adjust each hydrograph using quantile-quantile scaling to represent each of the runoff scenarios.

6.  Save those adjusted hydrographs for use by the EWR tool

## Scenario definitions

Climate scenarios give runoff scenarios for each SDL unit (from David R.). The scenario conditions follow:

Prec and ETp (PE) are historical (not needed here)

SimR0 is simulated historical runnoff using actual historical Prec and ETp (PE)

SimR1 - SimR7 are simulated with +7% PE but different Rainfall:

1.  -20%

2.  -15% ("High change scenario")

3.  -10%

4.  -5% ("Moderate change scenario")

5.  +0%

6.  +5% ("Low change scenario")

7.  +10%

### Make numeric metadata

The format of this may change, but we're going to want something. Lists are going to be easier to yaml than dataframes (though a dataframe is easier to construct).

The scenario yamls are likely to *not* be lists, but single values, ie each one gets their own value to create it, and that's it (which makes sense). We can still make an overview scenario metadata table from them though.

```{r}

rain_multiplier <- seq(from = 0.8, to = 1.1, by = 0.05) %>% 
  setNames(paste0('R', 1:7))

scenario_meta <- list(
  PE_multiplier = 1.07,
  rain_multiplier = rain_multiplier,
  scenario_name = names(rain_multiplier)
)

# Don't run yet, since I don't know the format we'll be using, but this works to create yaml metadata
# yaml::write_yaml(scenario_meta, file = 'path/to/file.yml')
```

## Bring in the scenario data

Get the list of files and read them in (to a list of dfs).

```{r}
#| message: false
CCSc_FileList <- list.files(scaling_dir, pattern = '.csv', 
                            full.names = TRUE)

scenario_list <- purrr::map(CCSc_FileList,
                            \(x) read_csv(file = x, id = 'path')) %>% 
  setNames(stringr::str_extract(CCSc_FileList, "SS[0-9]+"))
```

# Find scaling relationships

We follow the basic method for q-q scaling from [climate change australia](https://www.climatechangeinaustralia.gov.au/en/obtain-data/application-ready-data/scaling-methods/), with the following modifications

-   Use 2% bins (e.g. 50 bins) instead of 10 + 10 in final

-   Month-matching quantiles (e.g. 90th %ile for June separate from September). The method given might do that too, it's unclear.

-   'Change ratio', $\frac{F-H}{H}$ replaced with 'relative change', $\frac{F}{H}$, and adjustment changed from $Obs(CR) + Obs$ to $Obs(RC)$.

    -   These are algebraically equivalent, and the change ratio seems unnecessarily convoluted

$$
Obs(\frac{F-H}{H}) + Obs = 
Obs(\frac{F}{H}-1}) + Obs = 
Obs(\frac{F}{H}) - Obs + Obs = 
Obs(\frac{F}{H})
$$

## Break into groups, rank and bin

Use 2% bins (e.g. 50 bins) instead of 10 + 10 in final.

Write functions for finding quantiles and getting their mean that we can apply to each set of runoff scenarios in each SDL unit, as well as historical.

```{r}
# get_q <- function(vals, q_perc) {
#   qs <- quantile(vals, probs = seq(0,1, q_perc), type = 5, na.rm = TRUE)
#   # cut fails with lots of zeros (well, any duplicate bins)
#   # binvec <- cut(vals, qs, include.lowest = TRUE, labels = FALSE)
#   # findInterval is OK with duplicate bins, but combines them, eg. if there are 10 bins that are all 0, it will call them all q10. 
#   binvec <- findInterval(vals, qs, rightmost.closed = TRUE)
#   return(binvec)
# }
```

```{r}
# get_qmean <- function(vals, q_perc = 0.02) {
#   binvec <- get_q(vals, q_perc)
#   qmean <- aggregate(x = vals, by = list(binvec), FUN = mean) %>% 
#     setNames(c('quantile', 'mean'))
# }
```

## Apply to all SDL units

We have `scenario_list`, and we want to find the quantile means and relative changes for each dataframe in it. So write that function and use `purrr::map` over the SDL units.

```{r}
# get_scalings <- function(unitdf) {
#   # Stack
#   stackdf <- unitdf %>% 
#   mutate(sdl = stringr::str_extract(path, "SS[0-9]+")) %>% 
#   select(sdl, Year, Month, Day, starts_with('Sim')) %>% 
#   pivot_longer(cols = starts_with('Sim'), 
#                names_to = 'scenario', values_to = 'runoff')
#   
#   # Quantile means
#   q_s <- stackdf %>% 
#   group_by(scenario, Month) %>%
#     summarize(qmean = get_qmean(runoff)) %>%
#     # reframe is the new way, but needs dplyr 1.1 which breaks lots of the functions
#  # reframe(qmean = get_qmean(runoff)) %>%
#   tidyr::unnest(cols = qmean)
#   
#   # Get the relative change
#   q_s <- q_s %>% 
#   group_by(scenario, Month) %>% 
#   baseline_compare(compare_col = 'scenario', base_lev = 'SimR0', 
#                    values_col = 'mean', 
#                    comp_fun = `/`) %>% 
#   ungroup() %>% 
#   select(scenario = scenario.x, everything(), 
#          relative_change = `/_mean`, 
#          -scenario.y)
# }


```

Now, we need to apply `get_scalings` to every SDL unit in `scenario_list`

```{r}
#| message: false
scaled_units <- purrr::map(scenario_list, get_scalings)
```

This `scaled_units` list is a list of dataframes (one for each SDL unit) giving the relative change for each scenario and month that need to be mapped to the observed gauge data. First, let's do a quick plot check.

### Plots

A few plots to check nothing is going wrong. SS20 is Macquarie-Castlereagh.

#### Quantiles

First, check the quantiles themselves

```{r}
# just look at a couple quantiles
scaled_units$SS20 %>% 
  filter(quantile %in% c(1,25,50)) %>% 
ggplot(mapping = aes(x = scenario, y = mean, fill = as.factor(quantile))) + geom_col(position = position_dodge())
```

Do the quantiles look smooth?

```{r}
scaled_units$SS20 %>% 
ggplot(mapping = aes(x = as.factor(quantile), y = mean, fill = scenario)) + geom_col(position = position_dodge()) + facet_grid(Month ~ scenario)
```

```{r}
scaled_units$SS20 %>% 
  ggplot(mapping = aes(x = quantile, 
                       y = mean, color = scenario)) + 
  geom_line() + 
  facet_wrap('Month')
```

#### Relative change

For reduced precip, the drops at the highest quantiles are larger than the drops at the lower end. For increased precip, the higher quantiles go up more than the lower. That sort of nonlinearity does make sense.

```{r}
scaled_units$SS20 %>% 
  filter(quantile %in% c(1,25,50)) %>% 
ggplot(mapping = aes(x = scenario, 
                     y = relative_change, 
                     fill = as.factor(quantile))) + 
  geom_col(position = position_dodge())
```

Perhaps the best way to look at this is the relative change in each quantile for the different scenarios.

```{r}
scaled_units$SS20 %>% 
  ggplot(mapping = aes(x = quantile, 
                       y = relative_change, color = scenario)) + 
  geom_line() + 
  facet_wrap('Month')
```

Nothing there jumps out as horribly misaligned or major step changes between months, so let's move on.

# Adjust the flows

We now want to adjust the gauges with those relative changes to represent the different scenarios.

We could do this in `flow_scaling.qmd`, where we would read in the `scaled_units` list we just created. That's tempting, because we have the gauges mapped to SDL units there already. However, that script takes a while to run and is susceptible to intermittent API errors from the gauge service, and so everything will be more stable if we do the transform here on a static set of hydrographs (or at least with control over when those hydrographs update).

```{r}
orig_hydro <- readRDS(file.path(hydro_dir, 'extracted_flows.rds'))
```

## Approach

1.  find the quantile of each value in the orig_hydro data for each group-unit (e.g. Month)
2.  multiply by relative_change for each quantile, month, and scenario
3.  do that over all the sdl units

## Quantiles for historical data

We just want to ID the quantile of each value so we know how much to adjust, so use `get_q`. We also need to group and know the sdl unit for the transforms.

### Function to use with map

We want to process each of the gauges with each of the scenarios, so write a map function `scale_gauges` to map over the gauges. Because the gauge data comes in gauge-wise, but the scenarios are all together in the scenario definition, each gauge ends up having all the scenarios calculated at once. This leaves the resulting dataframe for each gauge as a nested df of scenarios, each with times and flow values. That needs to be unpacked, named, and saved separately for each scenario. This nested structure requires a little helper saving function `savefun`.

::: {#save_structure_q}
**Question** do I want to do this differently to save all gauges within scenarios as a single csv? or just do them one at a time? Because the data comes in as single gauges but scenarios all together, that would get convoluted, because we'd have to hold all the gauges and rearrange the data to have gauges inside scenarios instead of vice-versa.

Separate csvs per gauge might be better anyway given the different date ranges. But we can change the organisation steps in `scale_gauges` if we want something different.
:::

Make sure directories exist for writing out results

```{r}
#| output: false
purrr::map(scenario_meta$scenario_name, 
           \(x) if (!dir.exists(file.path(hydro_dir, x))) {
             dir.create(file.path(hydro_dir, x), 
                        recursive = TRUE)
             })
```

```{r}
#| output: false
# savefun <- function(scenario, data) {
#   write_csv(data, file = file.path(hydro_dir, scenario, paste0(names(data)[2], '.csv')))
# }

```

Some setup is needed to get the gauge types and sdl units. The other option would be to just do all the work in [flow_scaling.qmd](flow_scaling.qmd), or to save `geo_gauges` from there to read back in here.

```{r}
thesegauges <- names(orig_hydro)
```

```{python}
from py_ewr.observed_handling import categorise_gauges
catgauges = categorise_gauges(r.thesegauges)
```

```{r}
gauge_units <- bom_basin_gauges %>% 
  filter(gauge %in% names(orig_hydro)) %>% 
  sf::st_intersection(sdl_units)

gauge_cats <- reticulate::py$catgauges %>% 
  setNames(c('flow', 'level', 'stage'))
```

Before transforming, we need to decide which (if any) quality codes to drop. This is important because bad codes are often filled with 0, and so throw off the calculations.

First, look at the codes that are there to get a sense of what should be included.

```{r}
#| message: false
all_codes <- orig_hydro %>% 
  purrr::map(\(x) x %>% 
               group_by(quality_codes, 
                        quality_codes_id) %>% 
               summarise(n_records = n())) %>% 
  bind_rows() %>% 
  group_by(quality_codes,
           quality_codes_id) %>% 
  summarise(n_records = sum(n_records)) %>% 
  arrange(desc(quality_codes_id))

all_codes
```

It looks like they start saying 'Data not available' or similar above 150. So make everything above that NA. I've made this an argument to `scale_gauges` so we can change easily.

Define the main function to apply the scaling to each hydrograph in the list of hydrographs.

```{r}
# scale_gauges <- function(gaugedata, gaugename, qc_limit = 150) {
# 
#   # if they are level gauges, return NULL- the scaling doesn't make sense
#   if (gaugename %in% gauge_cats$level) {return(NULL)}
# 
#   # Get the sdl name
#   sdl_name <- gauge_units$SWSDLID[gauge_units$gauge == gaugename]
# 
#   # If the sdl unit for the gauge isn't in the scaling scenarios, return NULL.
#   # THis only happens for one gauge (422027) in the Barwon-Darling Watercourse (SS19)
#   if (!(sdl_name %in% names(scaled_units))) {
#     return(NULL)
#   }
# 
#   # There is one gauge (currently- 412036) that returns duplicated dates from 1990-2004. I've had a long look, and the values differ on each day, with no apparent pattern. They do tend to track together, but the difference can be large. It's not obvious which to keep (if any). I'll just keep the first of each pair, but this should be revisited.
#   gaugedata <- gaugedata[!duplicated(gaugedata$time), ]
# 
#   # Set bad data to NA
#   gaugedata[gaugedata$quality_codes_id > qc_limit, 'value'] <- NA
#   
#   # There's one gauge 414209 with ALL bad data. return NULL
#   if (all(is.na(gaugedata$value))) {return(NULL)}
# 
#   # make an NA quantile for each scenario so the join works properly
#   if (any(is.na(gaugedata$value))) {
#     nafill <- scaled_units[[sdl_name]] %>%
#       distinct(scenario, Month) %>%
#       mutate(quantile = NA, mean = NA, ref_mean = NA, relative_change = NA)
# 
#     scaled_units[[sdl_name]] <- bind_rows(scaled_units[[sdl_name]], nafill)
#   }
# 
# 
#   # do the transforms
# 
#   gaugedata <- gaugedata %>%
#     # get the time units right
#     mutate(Month = lubridate::month(time)) %>%
#     rename(Date = time) %>%  # To match other inputs
#     group_by(Month) %>%
#     # get quantiles- make a dummy so NA quantiles exist and get join-crossed with scenarios
#     mutate(quantile = get_q(value, q_perc = 0.02)) %>%
#     ungroup() %>%
#     # join to correct sdl unit for the scalings
#     left_join(scaled_units[[sdl_name]],
#               by = c('Month', 'quantile'),
#               multiple = 'all') %>% # Says it's OK to duplicate rows x scenarios
#     # get the adjusted levels
#     mutate(adj_val = value*relative_change) %>%
#     # Just the needed cols
#     dplyr::select(scenario, site, Date, adj_val) %>%
#     # pivot so the gauge name is col name
#     tidyr::pivot_wider(names_from = site, values_from = adj_val) %>%
#     # collapse to a list-tibble with one row per scenario
#     tidyr::nest(.by = scenario)
# 
#   # Save the csvs
#   if (REBUILD_DATA) { 
#     purrr::pmap(gaugedata, savefun)
#   }
#   # Not sure this is a good idea- might want to return NULL
#   return(gaugedata)
# }
```

### Do the transforms

This both saves to disk and returns a list to memory. Takes about 5 minutes.

```{r}
#| cache: false
system.time(scaled_hydro <- purrr::map2(orig_hydro[1:10], 
                                        names(orig_hydro[1:10]), 
                                        \(x,y) scale_gauges(x, y, scaled_units = scaled_units)))
```

I have tried with `furrr`, it is slower, likely because we have to move two big datasets around.

## Plots

Some plots for a gauge to make sure nothing looks wrong.

The `plot_hydrographs` function works, but sure is hard to see with the overplotting. And taking the log doesn't make it pretty, though the 'pseudo-log' works better than 'log10'. Still, we can see things are working as they should.

```{r}
plot_hydrographs(scaled_hydro[['409003']] %>% 
  # filter(scenario %in% c('SimR1', 'SimR4', 'SimR7')) %>%
  unnest(cols = data) %>% 
  pivot_longer(cols = 3, names_to = 'gauge', values_to = 'flow'),
  y_col = 'flow', transy = scales::pseudo_log_trans(sigma = 1, base = 10))
```

Facet it

```{r}
plot_hydrographs(scaled_hydro[['409003']] %>% 
  # filter(scenario %in% c('SimR1', 'SimR4', 'SimR7')) %>%
  unnest(cols = data) %>% 
  pivot_longer(cols = 3, names_to = 'gauge', values_to = 'flow'),
  y_col = 'flow') +
  facet_wrap('scenario')
```

And now we can move on to read in to the toolkit proper.

# TODO

-   Cease-to-flow

    -   Currently, historical values of 0 remain 0, since 0\*scaled = 0. And no *new* cease-to-flow can happen, since a small number \* a small number is still \> 0.

    -   The latter is likely easier to fix with a threshold, but unclear how to do the former without getting fairly complex.

-   Quality cutoff- do we want to discard data with certain quality codes? I've chosen \> 150
