[
  {
    "objectID": "scenario_creation/scenario_creation_overview.html",
    "href": "scenario_creation/scenario_creation_overview.html",
    "title": "Scenario creation",
    "section": "",
    "text": "The toolkit proper begins with hydrographs as inputs. The creation of those hydrographs, and particularly their modification to create scenarios is therefore typically a step that occurs prior to the use of the toolkit. For the demonstrations here, however, we generate some example scenarios from historical hydrographs. For the purposes of capacity demonstration we simply multiply short hydrographs and put them in a standard toolkit input format using the simple scenario notebook.\nA more complex set of scenarios designed to test the toolkit and module sensitivity and be a more complete example is created from scaling historical flows according to future runoff scenarios. This analysis is done in a separate github repo."
  },
  {
    "objectID": "scenario_creation/scenario_creation_overview.html#overview",
    "href": "scenario_creation/scenario_creation_overview.html#overview",
    "title": "Scenario creation",
    "section": "",
    "text": "The toolkit proper begins with hydrographs as inputs. The creation of those hydrographs, and particularly their modification to create scenarios is therefore typically a step that occurs prior to the use of the toolkit. For the demonstrations here, however, we generate some example scenarios from historical hydrographs. For the purposes of capacity demonstration we simply multiply short hydrographs and put them in a standard toolkit input format using the simple scenario notebook.\nA more complex set of scenarios designed to test the toolkit and module sensitivity and be a more complete example is created from scaling historical flows according to future runoff scenarios. This analysis is done in a separate github repo."
  },
  {
    "objectID": "overview/spatial_data.html",
    "href": "overview/spatial_data.html",
    "title": "Spatial data",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "overview/spatial_data.html#visualizing-spatial-data-from-werptoolkitr",
    "href": "overview/spatial_data.html#visualizing-spatial-data-from-werptoolkitr",
    "title": "Spatial data",
    "section": "Visualizing spatial data from werptoolkitr",
    "text": "Visualizing spatial data from werptoolkitr\nThe {werptoolkitr} package provides a standard set of spatial data, generated in data_creation/spatial_data_creation.qmd. Here, we make quick plots of the data so we know what it looks like.\nThe datasets are bom_basin_gauges (points), and basin (the MDB as a single polygon), sdl_units, resource_plan_areas, and cewo_valleys. Relevant to the case study- the original polygon used was the Macquarie-Castlereagh in the sdls. The crs all match from the creation.\n\nBasin\n\nggplot(basin) + geom_sf(fill = 'powderblue')\n\n\n\n\n\n\nResource plan areas\n\nggplot(resource_plan_areas) + geom_sf(aes(fill = SWWRPANAME), show.legend = FALSE) +\n  geom_sf_label(aes(label = SWWRPANAME), size = 3, label.padding = unit(0.1, 'lines')) + \n  colorspace::scale_fill_discrete_qualitative(palette = 'Set2')\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\nThese have ‘SW’ codes\n\nresource_plan_areas\n\n\n\n  \n\n\n\n\n\nSDL plan areas\n\nggplot(sdl_units) + geom_sf(aes(fill = SWSDLName), show.legend = FALSE) +\n  geom_sf_label(aes(label = SWSDLName), size = 3, label.padding = unit(0.1, 'lines')) + \n  colorspace::scale_fill_discrete_qualitative(palette = 'Set2')\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\nThese have ‘SS’ codes.\n\nsdl_units\n\n\n\n  \n\n\n\n\n\nCatchments\n\nggplot(cewo_valleys) + geom_sf(aes(fill = ValleyName), show.legend = FALSE) +\n  geom_sf_label(aes(label = ValleyName), size = 3, label.padding = unit(0.1, 'lines')) + \n  colorspace::scale_fill_discrete_qualitative(palette = 'Set2')\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\nThese have names, ID, and ValleyCodes\n\ncewo_valleys\n\n\n\n  \n\n\n\n\n\nGauges\n\nggplot() + \n  geom_sf(data = basin, fill = 'powderblue') +\n  geom_sf(data = bom_basin_gauges)\n\n\n\n\n\nbom_basin_gauges"
  },
  {
    "objectID": "full_toolkit/full_toolkit_overview.html",
    "href": "full_toolkit/full_toolkit_overview.html",
    "title": "Full toolkit overview",
    "section": "",
    "text": "The toolkit may be used stepwise, that is calling the Controller, Aggregator, and Comparer. But it can also be called in one go, feeding all necessary parameters in at once. In this case, we can think of the Controller as simply having larger scope, passing arguments all the way through instead of just to the modules. This can be done in-memory, or saving outputs at each step. In either case, it can be controlled interactively in notebooks, or with a params.yml file, which currently operates a parameterised notebook, but could be made to simply be ingested by Rscript at the command line.\nIn typical use, we will likely this full-toolkit approach, but making sure we save the output of at least the aggregator. It is very likely that we will want to make different plots for different purposes, and will not necessarily know what they are a-priori. So we’ll want the ability to run additional Comparer notebooks."
  },
  {
    "objectID": "full_toolkit/full_toolkit_overview.html#purpose",
    "href": "full_toolkit/full_toolkit_overview.html#purpose",
    "title": "Full toolkit overview",
    "section": "",
    "text": "The toolkit may be used stepwise, that is calling the Controller, Aggregator, and Comparer. But it can also be called in one go, feeding all necessary parameters in at once. In this case, we can think of the Controller as simply having larger scope, passing arguments all the way through instead of just to the modules. This can be done in-memory, or saving outputs at each step. In either case, it can be controlled interactively in notebooks, or with a params.yml file, which currently operates a parameterised notebook, but could be made to simply be ingested by Rscript at the command line.\nIn typical use, we will likely this full-toolkit approach, but making sure we save the output of at least the aggregator. It is very likely that we will want to make different plots for different purposes, and will not necessarily know what they are a-priori. So we’ll want the ability to run additional Comparer notebooks."
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_params.html",
    "href": "full_toolkit/WERP_toolkit_params.html",
    "title": "WERP_toolkit_demo",
    "section": "",
    "text": "This document provides a template for running the toolkit from a parameters file, as we might do when batch processing. As such, it typically wouldn’t be run through a notebook, but be called with Rscript. That sort of setup could go a lot of different directions depending on use case, so for now we’ll just demonstrate how to set up the parameter file and use it, and leave it to the user to build the script that gets called with Rscript from the command line or as part of an external process.\nIt is also possible to put the parameters in the yaml header of a quarto notebook (which we also do here, as an example. We provide examples of using the parameterised system several different ways below, including from external params.yml files, passed parameters, and the list Quarto generates in parameterised notebooks.\nLoad the package\nlibrary(werptoolkitr)"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_params.html#structure-of-params-files-and-arguments",
    "href": "full_toolkit/WERP_toolkit_params.html#structure-of-params-files-and-arguments",
    "title": "WERP_toolkit_demo",
    "section": "Structure of params files and arguments",
    "text": "Structure of params files and arguments\nThe run_toolkit_params function takes three arguments: yamlpath, which is a path to a yaml params file, passed_args which can come from the command line, and defaults, which is another yaml file, and lets us set most of the params, and only modify a subset with the yamlpath file and passed_args.\nThe package comes with a set of default parameters in system.file('yml/default_params.yml', package = 'werptoolkitr'). Users can however create their own default yaml params file to set a standard set of defaults for a given project. See this file for available parameters.\nThe params.yml file (or any other name, passed to yamlpath) and passed_args and list_args then can be used to modify the default values. The idea is only a small subset of those defaults would be modified for a particular run.\nThe params.R file (pointed to with the yaml parameter aggregation_def) is necessary because aggregation specification needs R objects or syntax.\nFinally, werptoolkitr::param_runner() ingests paths to these files (or passed command line or lists), turns their params into R arguments, and runs the toolkit.\nThe arguments overwrite each other, so list_args has highest precedence, followed by passed_args, yamlpath, and finally defaults.\n\nNote- no comparer\nAt present we do not provide yaml param options for the comparer. This is possible in future, but the possibilities are a bit too wide open at present. It is likely the user will want to explore the output, rather than generate parameterised output, at least until some specific uses are settled on."
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_params.html#additional-parameters",
    "href": "full_toolkit/WERP_toolkit_params.html#additional-parameters",
    "title": "WERP_toolkit_demo",
    "section": "Additional parameters",
    "text": "Additional parameters\nSpecify the aggregation sequence in R and pass the path to that file.\n# aggregation sequences (need to be defined in R)\naggregation_def: 'toolkit_project/agg_params.R'"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_params.html#directories",
    "href": "full_toolkit/WERP_toolkit_params.html#directories",
    "title": "WERP_toolkit_demo",
    "section": "Directories",
    "text": "Directories\n\nInput and output directories\n# Outer directory for scenario\nscenario_dir: 'toolkit_project'\n\n# Preexisting data\n# Hydrographs (expected to exist already)\nhydro_dir: NULL\n\n# Generated data\n# EWR outputs (will be created here in controller, read from here in aggregator)\newr_results: NULL\n\n# outputs of aggregator. There may be multiple modules\n# NULL doesn't save it, but holds in memory.\nagg_results: NULL\nNormally scenario_dir should point somewhere external (though keeping it inside or alongside the hydrograph data is a good idea.). But here, I’m generating test data, so I’m keeping it in the repo.\nSetting the output directories to NULL builds a standard toolkit directory structure, with scenario_dir as the outer directory, holding hydrographs, aggregator_output, and module_output subdirectories."
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_params.html#module-arguments",
    "href": "full_toolkit/WERP_toolkit_params.html#module-arguments",
    "title": "WERP_toolkit_demo",
    "section": "Module arguments",
    "text": "Module arguments\nCurrently, just the EWR tool. More could be exposed here, but this is typically all we need.\n# Model type\nmodel_format: 'IQQM - NSW 10,000 years'\n\n# Climate\nclimate: 'Standard - 1911 to 2018 climate categorisation'\n\n# output and return\noutputType:\n  - summary\n\nreturnType: none"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_params.html#aggregation-settings",
    "href": "full_toolkit/WERP_toolkit_params.html#aggregation-settings",
    "title": "WERP_toolkit_demo",
    "section": "Aggregation settings",
    "text": "Aggregation settings\nThese are additional arguments to read_and_agg.\n# What to aggregate\naggType: summary\n\n# Aggregation settings\nagg_groups: scenario\nagg_var: ewr_achieved\naggReturn: FALSE\nnamehistory: FALSE\nkeepAllPolys: TRUE"
  },
  {
    "objectID": "controller/controller_overview.html",
    "href": "controller/controller_overview.html",
    "title": "Controller",
    "section": "",
    "text": "The toolkit takes hydrographs as input data and then processes it through downstream modules, performs aggregation and analyses, and produces outputs. The ‘Controller’ component of the toolkit points to that input data, and sends it off to the modules with arguments controlling how that happens. It may also determine how ongoing processing occurs.\nIn typical use, the controller simply points to the input data and initiates processing steps according to the user. Examples of this for the controller alone and the whole toolkit are available to illustrate this, as well as a stepthrough to better understand what the controller is doing."
  },
  {
    "objectID": "controller/controller_overview.html#overview",
    "href": "controller/controller_overview.html#overview",
    "title": "Controller",
    "section": "",
    "text": "The toolkit takes hydrographs as input data and then processes it through downstream modules, performs aggregation and analyses, and produces outputs. The ‘Controller’ component of the toolkit points to that input data, and sends it off to the modules with arguments controlling how that happens. It may also determine how ongoing processing occurs.\nIn typical use, the controller simply points to the input data and initiates processing steps according to the user. Examples of this for the controller alone and the whole toolkit are available to illustrate this, as well as a stepthrough to better understand what the controller is doing."
  },
  {
    "objectID": "controller/controller_ewr_stepthrough.html",
    "href": "controller/controller_ewr_stepthrough.html",
    "title": "Scenario controller in detail",
    "section": "",
    "text": "Load the package\nlibrary(werptoolkitr)\nThe controller primarily sets the paths to scenarios, calls the modules, and saves the output and metadata. In normal use, the series of steps below is wrapped. Once we have the directory and set any other needed parameters (e.g. point at a config file), we should just click go, and auto-generate the folder structure, run the ewr, and output the results. I’m stepping through that here so we can see what’s happening and where we need to tweak as formats change; this document is intended to expose some of the inner workings of the black box. Wrapped versions of the controller alone and the whole toolkit are available to illustrate this more normal use."
  },
  {
    "objectID": "controller/controller_ewr_stepthrough.html#set-paths",
    "href": "controller/controller_ewr_stepthrough.html#set-paths",
    "title": "Scenario controller in detail",
    "section": "Set paths",
    "text": "Set paths\nWe need to set the path to this demonstration. This should all be in a single outer directory project_dir, and there should be an inner directory with the input data /hydrographs. These would typically point to external shared directories. For this simple example though, we put the data inside the repo to make it self contained. The saved data goes to project_dir/module_output automatically. The /hydrographs subdirectory could be made automatic as well, but I’m waiting for the input data format to firm up.\n\nproject_dir = file.path('more_scenarios')\nhydro_dir = file.path(project_dir, 'hydrographs')"
  },
  {
    "objectID": "controller/controller_ewr_stepthrough.html#format",
    "href": "controller/controller_ewr_stepthrough.html#format",
    "title": "Scenario controller in detail",
    "section": "Format",
    "text": "Format\nWe need to pass the data format to the downstream modules so they can parse the data. Currently the demo csvs are created in a format that parses like IQQM, and the netcdf will be. The EWR tool (the only current module) has three options currently 1) 'Bigmod - MDBA', 2) 'IQQM - NSW 10,000 years', and 3) 'Source - NSW (res.csv)'. I’m exposing this for the example, but we can auto-set this to the IQQM default in normal use.\n\n# Options\n# 'Bigmod - MDBA'\n# 'IQQM - NSW 10,000 years'\n# 'Source - NSW (res.csv)'\n\nmodel_format = 'IQQM - NSW 10,000 years'"
  },
  {
    "objectID": "controller/controller_ewr_stepthrough.html#climate-info",
    "href": "controller/controller_ewr_stepthrough.html#climate-info",
    "title": "Scenario controller in detail",
    "section": "Climate info",
    "text": "Climate info\nLike the format, allowance and climate are arguments to the EWR tool, so I set them here to be clear what we’re doing, but in general they would be set by default.\n\nMINT &lt;- (100 - 0)/100\nMAXT &lt;- (100 + 0 )/100\nDUR &lt;- (100 - 0 )/100\nDRAW &lt;- (100 -0 )/100\n\n# A named list in R becomes a dict in python\nallowance &lt;- list('minThreshold' = MINT, 'maxThreshold' = MAXT, 'duration' = DUR, 'drawdown' = DRAW)\n\nclimate &lt;- 'Standard - 1911 to 2018 climate categorisation'"
  },
  {
    "objectID": "controller/controller_ewr_stepthrough.html#set-up-output-directories",
    "href": "controller/controller_ewr_stepthrough.html#set-up-output-directories",
    "title": "Scenario controller in detail",
    "section": "Set up output directories",
    "text": "Set up output directories\nWe get the information about the gauges and filepaths project_dir as a dict with make_scenario_info. The scenarios need to be in separate directories inside /hydro_dir, but the files in those directories could come in multiple arrangements. Currently, we allow multiple csvs of single-gauge hydrographs or single csvs of multiple-gauge hydrographs. Once the netcdf format settles down, we will include parsing that. If there are multiple gauges within each csv, they enter the dict as a ‘gauge’ value. If not, the ‘gauge’ value is just the filename, so these need to be gauge numbers. For this example, we have single csvs with multiple gauges.\nThe output directory and subdirs for scenarios is created by make_output_dir, which also returns that outer directory location. The EWR tool needs the paths to the gauge data as a list, so paths_gauges_R just unfolds the dict to give that. There will likely be continued changes to these functions as we settle on standard directory structures and data formats. These functions are currently all python wrapped by R for historical reasons, but as they are just directory creation and querying could be easily re-written in R to tighten up the package.\n\n# Gives file locations as a dict- \nsceneinfodict &lt;- make_scenario_info_R(hydro_dir)\n\nWarning in normalizePath(path.expand(path), winslash, mustWork):\npath[1]=\"C:/Users/galen/Documents/code/WERP/demo_pkg_updates/.venv/Scripts\":\nThe system cannot find the path specified\n\n# make the output directory structure\noutpath &lt;- make_output_dir_R(project_dir, sceneinfodict)\n# unfold the sceneinfodict to make it easy to get the lists of paths and gauges\n# everyhydro = paths_gauges(sceneinfodict)[0]\n\n# The inner level turned into characters in R and needs to stay as a list. This is annoying but needed *only in R*\nfor (i in 1:length(sceneinfodict)) {\n  for (j in 1:2) {\n    sceneinfodict[[i]][[j]] &lt;- as.list(sceneinfodict[[i]][[j]])\n  }\n}\n\n# more list-making to work from R-Python\neveryhydro &lt;- as.list(paths_gauges_R(sceneinfodict)[[1]])"
  },
  {
    "objectID": "controller/controller_ewr_stepthrough.html#run-the-ewr-tool",
    "href": "controller/controller_ewr_stepthrough.html#run-the-ewr-tool",
    "title": "Scenario controller in detail",
    "section": "Run the ewr tool",
    "text": "Run the ewr tool\nNow we run the ewr tool with the parameters given and save the output.\nThere’s an issue with outputType = 'annual' in the version of the EWR tool this was built with. Until I update and test the new EWR tool, skip the annual data. There are still a number of messages printed by the EWR code, about pulling the data and gauge dependencies. Those are useful when using the code, but I’ll suppress printing them here since there are so many.\nThis is not actually run here for speed- the same thing is done in a notebook for the full toolkit.\n\n# To make a list in python, need to have unnamed lists in R\nif (!params$REBUILD_DATA) {\n  outputType &lt;- list('none')\n}\nif (params$REBUILD_DATA) {\n  outputType &lt;- list('summary', 'all')\n}\n\n\newr_out &lt;- run_save_ewrs_R(everyhydro, outpath, model_format = model_format, allowance = allowance, climate = climate, outputType = outputType, datesuffix = FALSE, returnType = list('summary', 'all'))\n\nBriefly, we can see that that has returned dataframes from the EWR (with some leftover pandas datetime environments that we could clean up if we wanted to use this in-memory). Typically, though, we just save this out. Because we do not actually run the chunk above for rendering this site, these dataframes are not available here.\n\nnames(ewr_out)\nstr(ewr_out$summary)\nstr(ewr_out$all)"
  },
  {
    "objectID": "comparer/line_plots.html",
    "href": "comparer/line_plots.html",
    "title": "Line plots (quantitative x)",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)"
  },
  {
    "objectID": "comparer/line_plots.html#scenario-information",
    "href": "comparer/line_plots.html#scenario-information",
    "title": "Line plots (quantitative x)",
    "section": "Scenario information",
    "text": "Scenario information\nGet scenario metadata. This will be auto-found later, but leaving here until it firms up.\n\nscenarios &lt;- jsonlite::read_json(file.path(hydro_dir, \n                                           'scenario_metadata.json')) |&gt; \n  tibble::as_tibble() |&gt; \n  tidyr::unnest(cols = everything())"
  },
  {
    "objectID": "comparer/line_plots.html#subset-for-demo",
    "href": "comparer/line_plots.html#subset-for-demo",
    "title": "Line plots (quantitative x)",
    "section": "Subset for demo",
    "text": "Subset for demo\nWe have a lot of hydrographs, so for this demonstration, we will often use a subset.\n\ngauges_to_plot &lt;- c('412002', '419001', '422028', '421001')"
  },
  {
    "objectID": "comparer/line_plots.html#choosing-example-data",
    "href": "comparer/line_plots.html#choosing-example-data",
    "title": "Line plots (quantitative x)",
    "section": "Choosing example data",
    "text": "Choosing example data\nFirst, we read in the aggregated data. There is example data provided by the toolkit (agg_theme_space and agg_theme_space_colsequence), but to continue with the demonstration, we will use the aggregations created here in the interleaved aggregation notebook.\nNote- to readRDS sf objects, we need to have sf loaded.\n\nagged_data &lt;- readRDS(file.path(agg_dir, 'summary_aggregated.rds'))\n\nThat has all the steps in the aggregation, so we’ll choose one (agged_data$sdl_units) at the SDL unit scale and env_obj theme scale, as this provides the opportunity to consider issues that arise from plottng multiple spatial units and grouped outcome levels. The same ideas would hold at any of the other levels in the aggregation.\nTo make these examples more easily, we create a slightly modified dataframe here, but this isn’t really necessary- small data manipulations are easily piped in to plot_outcomes. The SDL units data is joined to the scenarios dataframe to include the information there about the quantitative meaning of the scenarios, and is given a grouping column that puts the many env_obj variables in groups defined by their first two letters, e.g. EF for Ecosystem Function, which is then used for grouped color palettes.\nIf we had used multiple aggregation functions at any step, we should filter down to the one we want here, but we only used one for this example.\n\n# Create a grouping variable\nobj_sdl_to_plot &lt;- agged_data$sdl_units |&gt;\n  left_join(scenarios, by = c('scenario' = 'scenario_name')) |&gt;\n  dplyr::mutate(env_group = stringr::str_extract(env_obj, '^[A-Z]+')) |&gt;\n  dplyr::filter(!is.na(env_group)) |&gt;\n  dplyr::arrange(env_group, env_obj)"
  },
  {
    "objectID": "comparer/line_plots.html#lines-through-all-data",
    "href": "comparer/line_plots.html#lines-through-all-data",
    "title": "Line plots (quantitative x)",
    "section": "Lines through all data",
    "text": "Lines through all data\nA simple plot would be to look at all the outcomes, separated by color. We’ve given the scenarios different shapes, but that’s not really necessary- they are different along x. Even this simple plot is quite infomative- we can see that the env_obj outcomes are differently sensitive to both decreases and increases in flow, and that this differs across space.\n\n sdl_line &lt;- obj_sdl_to_plot |&gt;\n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'flow_multiplier',\n                          colorgroups = NULL,\n                          colorset = 'env_obj',\n                          pal_list = list('scico::berlin'),\n                          facet_row = 'SWSDLName',\n                          facet_col = '.',\n                          scene_pal = scene_pal,\n                          sceneorder = sceneorder)\n \n sdl_line\n\n\n\n\nWe might not care so much about individual outcomes, but about their groupings, and we can plot those in color by changing colorset = 'env_group'. We need to use point_group here to separate out the points for each env_obj.\nThis plot also demonstrates the use of some additional arguments. We’re also using transx to log the x-axis, which is particularly appropriate for the multiplicative flow scaling in this demonstration. We also log the y-axis with transy since we’re using a comp_fun (relative) to look at the multiplicative shift in each env_obj to baseline. We’re using various *_lab arguments to adjust the labelling. We also need to use the (poorly documented) group_cols argument to specify unique rows. This is historical and only applies to baselining the data. It will be auto-found in a future update.\nScientifically, one important thing to note here is that the range on y (0-10) is much greater than the range on x (0.3 - 3), and so (unsurprisingly), some outcomes are disproportionately impacted by flow. Other outcome values are less than the relative shift in flow, and so there are others that are disproportionately insensitive. These disproportionate responses also depend on whether flows decrease or increase- they are not symmetric.\n\nsdl_line_options &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'flow_multiplier',\n                y_lab = 'Proportion met',\n                x_lab = 'Change in flow',\n                transx = 'log10',\n                transy = 'log10',\n                color_lab = 'Environmental\\ngroup',\n                colorset = 'env_group',\n                pal_list = list('scico::berlin'),\n                point_group = 'env_obj',\n                facet_row = 'SWSDLName',\n                facet_col = '.',\n                scene_pal = scene_pal,\n                sceneorder = sceneorder,\n                base_lev = 'base',\n                comp_fun = 'relative',\n                group_cols = c('env_obj', 'polyID'))\n\nsdl_line_options\n\n\n\n\nWe can also give the groups different palettes, as demonstrated more completely in the bar plots and causal networks. Now, we don’t need point_group anymore, since the colors are assigned to the unique env_objs.\n\n# Create a palette list\ngrouplist = list(EF = 'grDevices::Purp',\n                 NF = 'grDevices::Mint',\n                 NV = 'grDevices::Burg',\n                 OS = 'grDevices::Blues',\n                 WB = 'grDevices::Peach')\n\nsdl_line_groups &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'flow_multiplier',\n                y_lab = 'Proportion met',\n                x_lab = 'Change in flow',\n                transx = 'log10',\n                transy = 'log10',\n                color_lab = 'Environmental\\ngroup',\n                colorgroup = 'env_group',\n                colorset = 'env_obj',\n                pal_list = grouplist,\n                facet_row = 'SWSDLName',\n                facet_col = '.',\n                scene_pal = scene_pal,\n                sceneorder = sceneorder,\n                base_lev = 'base',\n                comp_fun = 'relative',\n                group_cols = c('env_obj', 'polyID'))\n\nsdl_line_groups\n\n\n\n\nFigure 1: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Groups of environmental objectives plotted from different color palettes.\n\n\n\n\nThat’s fairly complex, so we can facet it, as we did with the bars to make the individual env_objs easier to see.\n\nsdl_line_groups_facet &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'flow_multiplier',\n                y_lab = 'Proportion met',\n                x_lab = 'Change in flow',\n                transx = 'log10',\n                transy = 'log10',\n                color_lab = 'Environmental\\ngroup',\n                colorgroup = 'env_group',\n                colorset = 'env_obj',\n                pal_list = grouplist,\n                facet_row = 'SWSDLName',\n                facet_col = 'env_group',\n                scene_pal = scene_pal,\n                sceneorder = sceneorder,\n                base_lev = 'base',\n                comp_fun = 'relative',\n                group_cols = c('env_obj', 'polyID'))\n\nsdl_line_groups_facet\n\n\n\n\nFigure 2: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Groups of environmental objectives plotted from different color palettes and facetted for easier visualisation.\n\n\n\n\nThe above is typically how we would go about this facetting, but it is worth reiterating that these are just ggplots, and so we can post-hoc add facetting. Using the version with only spatial facetting ( Figure 1 ), we can add the env_group facet on, matching Figure 2 . Note that we re-build all the facets here, due to the specification of ggplot2::facet_grid.\n\nsdl_line_groups + facet_grid(SWSDLName ~ env_group)\n\n\n\n\nAs with the bar plots, we can color by any column we want, and the spatial units is a logical choice. We again use point_group, since multiple env_obj rows are mapped to each color. The overplotting gets unreadable here and so I’ve retained the facetting, but if we were looking at a subset, the line colors could be enough (or if we are summarising the data with a smoother- see below).\n\nsdl_line_sdl &lt;- obj_sdl_to_plot |&gt;\n  filter(env_group == 'EF') |&gt;\n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'flow_multiplier',\n                y_lab = 'Proportion met',\n                x_lab = 'Change in flow',\n                transx = 'log10',\n                transy = 'log10',\n                color_lab = 'SDL unit',\n                colorset = 'SWSDLName',\n                pal_list = list(\"ggsci::default_jama\"),\n                point_group = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = sceneorder,\n                base_lev = 'base',\n                comp_fun = 'relative',\n                group_cols = c('env_obj', 'polyID'))\n\nsdl_line_sdl\n\n\n\n\nFigure 3: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Colors indicate SDL unit, each line is an env_obj."
  },
  {
    "objectID": "comparer/line_plots.html#smoothing-fit-lines",
    "href": "comparer/line_plots.html#smoothing-fit-lines",
    "title": "Line plots (quantitative x)",
    "section": "Smoothing (fit lines)",
    "text": "Smoothing (fit lines)\nWe can use smoothing to fit lines through multiple points, e.g. if we want to group data in some way- maybe use it to put a line through the color groups and ignore individual levels. This is dangerous- it’s an aggregation. But it can also be very informative, and we can show the individual data points to avoid misleading information. We demonstrate here using them to illustrate unique outcomes, as well as more typical uses as lines of best fit that aggregate over a number of outcomes.\nTo get smoothed lines, we use smooth = TRUE. By default, that produces a loess fit (as with ggplot2::geom_smooth, but we can also pass smooth_method, which is the method argument to ggplot::geom_smooth, and so allows things like lm and glm fits.\n\nUnique points\nFitting lines through unique points at each scenario level is a bit contrived, but it can be useful if we want to accentuate nonlinear relationships. Linear fits are possible too, though these are typically less useful.\nWith unique points, this just fits a single curved line through each env_obj. Recapitulating the above, we color here from SDL unit.\n\n  sdl_smooth_sdl &lt;- obj_sdl_to_plot |&gt;\n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'flow_multiplier',\n                          y_lab = 'Proportion met',\n                          x_lab = 'Change in flow',\n                          transx = 'log10',\n                          color_lab = 'Catchment',\n                          colorgroups = NULL,\n                          colorset = 'SWSDLName',\n                          point_group = 'env_obj',\n                          pal_list = list('ggsci::default_jama'),\n                          facet_row = 'env_group',\n                          facet_col = '.',\n                          scene_pal = scene_pal,\n                          sceneorder = sceneorder,\n                          base_lev = 'base',\n                          comp_fun = 'difference',\n                          group_cols = c('env_obj', 'polyID'),\n                          smooth = TRUE)\n  \n  suppressWarnings(print(sdl_smooth_sdl))\n\n\n\n\nAnd we can do the same for environmental groupings.\n\n  sdl_smooth_groups &lt;- obj_sdl_to_plot |&gt;\n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'flow_multiplier',\n                          y_lab = 'Proportion met',\n                          x_lab = 'Change in flow',\n                          transx = 'log10',\n                          color_lab = 'Environmental grouping',\n                          colorgroups = NULL,\n                          colorset = 'env_group',\n                          point_group = 'env_obj',\n                          pal_list = list('scico::berlin'),\n                          facet_row = 'env_group',\n                          facet_col = 'SWSDLName',\n                          scene_pal = scene_pal,\n                          sceneorder = sceneorder,\n                          base_lev = 'base',\n                          comp_fun = 'difference',\n                          group_cols = c('env_obj', 'polyID'),\n                          smooth = TRUE)\nsuppressWarnings(print(sdl_smooth_groups))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nUsing smooth_method = 'lm' is a linear fit. It does not recapitulate the simple lines above, however, because it fits the line through all the scenario data points, rather than simply joining them together. I have turned smooth_se = FALSE here because with unique groups the standard errors are enormous.\n\nsdl_lm_groups &lt;- obj_sdl_to_plot |&gt;\n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'flow_multiplier',\n                          y_lab = 'Proportion met',\n                          x_lab = 'Change in flow',\n                          transx = 'log10',\n                          color_lab = 'Environmental grouping',\n                          colorgroups = NULL,\n                          colorset = 'env_group',\n                          point_group = 'env_obj',\n                          pal_list = list('scico::berlin'),\n                          facet_row = 'env_group',\n                          facet_col = 'SWSDLName',\n                          scene_pal = scene_pal,\n                          sceneorder = sceneorder,\n                          base_lev = 'base',\n                          comp_fun = 'relative',\n                          group_cols = c('env_obj', 'polyID'),\n                          smooth = TRUE,\n                  smooth_method = \"lm\", smooth_se = FALSE)\n\nWarning: NaN and Inf introduced in `plot_prep`, likely due to division by zero.\n324 values were lost.\n\nsuppressWarnings(print(sdl_lm_groups))\n\n\n\n\n\n\nFit multiple points\nFitting lines is most often associated with things like regression and loess smoothing, where we use it to aggregate over a number of datapoints to find the line of best fit. We can do that here, simply by not having all points accounted for across the facetting, point_group, and colorset. NOTE- group_cols should still include unique values, because group_cols determines the baselining (e.g. what gets compared), not the plot groupings.\nOne example would be to perform the same analysis as in Figure 3, but instead of plotting each point, fit a line to show the mean change within each SDL unit. We’ve pulled env_obj out of point_group, but left it in group_cols , because we still want each env_obj baselined with itself, not to the mean of env_group. Now, we can look at all the env_groups, because there are far fewer lines and so the overplotting isn’t an issue.\nWe use a small add_eps to avoid zeros and allow all data to be relativised and plotted.\n\nsdl_fit_sdl &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'flow_multiplier',\n                y_lab = 'Proportion met',\n                x_lab = 'Change in flow',\n                transx = 'log10',\n                transy = 'log10',\n                color_lab = 'SDL unit',\n                colorset = 'SWSDLName',\n                pal_list = list(\"ggsci::default_jama\"),\n                facet_wrapper = 'env_group',\n                scene_pal = scene_pal,\n                sceneorder = sceneorder,\n                base_lev = 'base',\n                comp_fun = 'relative',\n                add_eps = min(obj_sdl_to_plot$ewr_achieved[obj_sdl_to_plot$ewr_achieved &gt; 0], \n                              na.rm = TRUE)/2,\n                group_cols = c('env_obj', 'polyID'),\n                smooth = TRUE)\n\nsuppressWarnings(print(sdl_fit_sdl))\n\n\n\n\nFigure 4: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Fits are loess smoothers. Colors indicate SDL unit, which have single lines. Each point is an env_obj.\n\n\n\n\nWe can make a very similar plot, looking at the environmental groups, a smooth fit of Figure 1 . We use a position argument (which passes to {ggplot2}, and so has the same syntax) to see overplotted points, and an add_eps to avoid zeros to relativise and plot all the data.\n\nsdl_fit_groups &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'flow_multiplier',\n                y_lab = 'Proportion met',\n                x_lab = 'Change in flow',\n                transx = 'log10',\n                transy = 'log10',\n                color_lab = 'Environmental\\ngroup',\n                colorset = 'env_group',\n                pal_list = list('scico::berlin'),\n                facet_row = 'SWSDLName',\n                facet_col = '.',\n                scene_pal = scene_pal,\n                sceneorder = sceneorder,\n                base_lev = 'base',\n                comp_fun = 'relative',\n                add_eps = min(obj_sdl_to_plot$ewr_achieved[obj_sdl_to_plot$ewr_achieved &gt; 0], \n                              na.rm = TRUE)/2,\n                group_cols = c('env_obj', 'polyID'),\n                smooth = TRUE,\n                position = position_jitter(width = 0.01, height = 0))\n\nsdl_fit_groups\n\n\n\n\nFigure 5: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Fits are loess smoothers. Colors indicate Environmental groups, which have single lines. Each point is an env_obj.\n\n\n\n\nAs we saw above, we can use method = 'lm' to plot a regression, though in general we do not expect these relationships to be linear, and mathematically characterising them will be a complex task that is not the purview of plotting (though is in the purview of the Comparer, and will be addressed once we have more complete outputs).\nA linear fit of the SDL units ( Figure 6 ) is one example of how this might work. It is useful to know here that deviations from a 1:1 line on logged axes as here means that the outcomes are responding disproportionately more (steeper) or less (shallower) than the underlying changes to flow.\n\nsdl_lm_sdl &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'flow_multiplier',\n                y_lab = 'Proportion met',\n                x_lab = 'Change in flow',\n                transx = 'log10',\n                transy = 'log10',\n                color_lab = 'SDL unit',\n                colorset = 'SWSDLName',\n                pal_list = list(\"ggsci::default_jama\"),\n                facet_wrapper = 'env_group',\n                scene_pal = scene_pal,\n                sceneorder = sceneorder,\n                base_lev = 'base',\n                comp_fun = 'relative',\n                add_eps = min(obj_sdl_to_plot$ewr_achieved[obj_sdl_to_plot$ewr_achieved &gt; 0], \n                              na.rm = TRUE)/2,\n                group_cols = c('env_obj', 'polyID'),\n                smooth = TRUE,\n                smooth_method = 'lm')\n\nsuppressWarnings(print(sdl_lm_sdl))\n\n\n\n\nFigure 6: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Fits are linear regressions. Colors indicate SDL unit, which have single lines. Each point is an env_obj."
  },
  {
    "objectID": "comparer/heatmap.html",
    "href": "comparer/heatmap.html",
    "title": "Heatmap",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "comparer/heatmap.html#demonstration-setup",
    "href": "comparer/heatmap.html#demonstration-setup",
    "title": "Heatmap",
    "section": "Demonstration setup",
    "text": "Demonstration setup\nAs usual, we need paths to the data.\n\nproject_dir &lt;- file.path('more_scenarios')\nhydro_dir = file.path(project_dir, 'hydrographs')\nagg_dir &lt;- file.path(project_dir, 'aggregator_output')"
  },
  {
    "objectID": "comparer/heatmap.html#scenario-information",
    "href": "comparer/heatmap.html#scenario-information",
    "title": "Heatmap",
    "section": "Scenario information",
    "text": "Scenario information\nThis should be auto-acquired from the dirs above. But while its format is still up in the air, I’m leaving it a bit more user-editable.\n\nscenarios &lt;- jsonlite::read_json(file.path(hydro_dir,\n                                           'scenario_metadata.json')) |&gt; \n  tibble::as_tibble() |&gt; \n  tidyr::unnest(cols = everything())"
  },
  {
    "objectID": "comparer/heatmap.html#subset-for-demo",
    "href": "comparer/heatmap.html#subset-for-demo",
    "title": "Heatmap",
    "section": "Subset for demo",
    "text": "Subset for demo\nWe have a lot of hydrographs, so for this demonstration, we will often use a subset.\n\ngauges_to_plot &lt;- c('412002', '419001', '422028', '421001')"
  },
  {
    "objectID": "comparer/causal_plots.html",
    "href": "comparer/causal_plots.html",
    "title": "Causal network outcomes",
    "section": "",
    "text": "As we showed in the theme aggregation example, we can color the nodes in a causal network by outcome values. That is perhaps the best place to see the use of causal networks for outcomes- using the interleaved example data is less clearly relevant, since spatial scale changes as well, though it can be done. Integrating these plots with the more general approach of plot_outcomes is high on the priority list.\nThere are many other causal network plots we can make but those in the theme aggregation example, target outcomes, rather than adjusting other aesthetics of the network for other reasons, such as altering color by level.\nWe should be able to change size as well. And we can color the edges, but that’s less clear what the meaning is."
  },
  {
    "objectID": "causal_networks/causal_plots.html",
    "href": "causal_networks/causal_plots.html",
    "title": "Causal Network Plotting",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(dplyr)"
  },
  {
    "objectID": "causal_networks/causal_plots.html#get-the-network-relationships",
    "href": "causal_networks/causal_plots.html#get-the-network-relationships",
    "title": "Causal Network Plotting",
    "section": "Get the network relationships",
    "text": "Get the network relationships\nFirst we need the relationships defining the network, which are extracted from tables in {werptoolkitr} and provided as the dataset werptoolkitr::causal_ewrs."
  },
  {
    "objectID": "causal_networks/causal_plots.html#process-data-for-network-plots",
    "href": "causal_networks/causal_plots.html#process-data-for-network-plots",
    "title": "Causal Network Plotting",
    "section": "Process data for network plots",
    "text": "Process data for network plots\nCausal networks need an edges dataframe specifying all pairwise connections, and a nodes dataframe specifying the nodes and their attributes. We make those here (and not in the data cleaning stage) for a couple reasons.\n\nThe full set of possible links is massively factorial, and so we want to choose only those useful for the needs of a given analysis.\n\nAnalyses may differ depending on network detail, spatial resolution, or use in the toolkit outside the network (e.g. theme aggregation)\n\nA key feature of the network isn’t just the existence of connections, but their directionality, and so we want to specify that explicitly.\nWe want to have the general ability to make edges and nodes available for other uses, e.g. aggregations\n\n{werptoolkitr} exports make_edges and make_nodes, along with some other network-manipulation functions\n\n\nTo build the nodes and edges for a specific plot or set of plots, we first build a dataframe of edges, and then extract nodes.\n\nConstruct edge dataframe\nThe edges dataframe contains all pairwise links between nodes in from and to columns. To get that, we need to pass it dataframes specifying links. There will usually be multiple datasets, reflecting the differing scales of the nodes (as in the causal_ewrs list provided by {werptoolkitr}). These dataframes may include many columns of potential nodes, e.g. they might provide the mapping for several steps in the network. Thus, we need to provide the node columns we actually want to map and their directionality- what are the ‘from-to’ pairings. We may want to filter to only some subset of nodes; for example we may only be interested in the environmental objectives related to waterbirds. Further, we will likely want to filter by geography, currently possible by either gauge or planning unit (deprecated).\nAs an example, we can make the relationships present at gauge 409025 linking EWRs to environmental objectives, environmental objectives to specific goals, Specific goals to Targets, and environmental objectives to 5-year targets. There are many more possible connections to include in the fromtos, which to include will depend on the questions being asked. I’ve just chosen these for a quick demo.\n\nedges &lt;- make_edges(dflist = causal_ewr, \n               fromtos = list(c('ewr_code', 'env_obj'), \n                              c('env_obj', 'Specific_goal'), \n                              c('Specific_goal', 'Target'), \n                              c('env_obj', 'target_5_year_2024')),\n               gaugefilter = '409025')\n\nedges\n\n\n\n  \n\n\n\nThere’s also the opportunity to filter the specific nodes to include with fromfilter and tofilter. This allows things like filtering the particular nodes within those node categories (e.g. env_objs related to waterbirds). However, it is typically better to use find_related_nodes after creation of the network, as that does network-aware filtering.\nAlthough we can specify defaults, this function is also reasonably generic and so can be used far beyond whatever defaults we set- it only depends on WERP-specific things in that the spatial filtering happens on gauge (and those are cross-referenced). For a particular set of analyses, we would typically set default fromtos lists, most relevantly in the Aggregator and Comparer. If we are producing plots for illustrating the network, there may be ad-hoc adjustments to that list.\n\n\nConstruct node dataframe\nThe node dataframe defines the ‘boxes’. The simplest way to make it is to extract it from the edges. Basically, we just grab all the nodes that are in either the from or to columns of the edges df. The make_nodes function does a bit more than just get unique node values from the edges df, it also attaches a column specifying the node order, reflecting their sequence in the causal network. There is a default sequence specified for WERP EWRs, but others can be specified with the typeorder argument. We expect that new default sequences will need to be created when new sorts of relationships come online.\n\nnodes &lt;- make_nodes(edges)\n# look at that for demo\nnodes\n\n\n\n  \n\n\n\nWe can now create a minimal plot before digging back in to demo some options under the hood."
  },
  {
    "objectID": "causal_networks/causal_plots.html#node-relevant-network",
    "href": "causal_networks/causal_plots.html#node-relevant-network",
    "title": "Causal Network Plotting",
    "section": "Node-relevant network",
    "text": "Node-relevant network\nOne thing we’re fairly likely to want to do is ask about the connections that relate to a node or a small set of nodes. To do that, we need to be able to traverse the network upstream and downstream, using the focalnodes argument, which calls the find_related_nodes function. This is a more complete network restructuring than just filtering a target level, as we did above, because it traces the full network and only returns nodes at any level that relate to the targets. Now we can include the 5-year targets again because we’ve reduce the nodes. Note that the focalnodes don’t have to be related to each other or at the same level- find_related_nodes prunes the network to all connections involving all the focalnodes.\n\nmake_causal_plot(nodes, edges, \n                 focalnodes = c('NF4', 'Sloanes froglet'), render = FALSE) %&gt;% \n  DiagrammeR::render_graph()"
  },
  {
    "objectID": "causal_networks/causal_plots.html#plot-setup--more-options-and-next-steps",
    "href": "causal_networks/causal_plots.html#plot-setup--more-options-and-next-steps",
    "title": "Causal Network Plotting",
    "section": "Plot setup- more options and next steps",
    "text": "Plot setup- more options and next steps\nThe above is running with defaults, but there’s quite a bit more capacity to change what is plotted and the look of the graphs. Primarily, I’ve focused on development related to how we’ll want to feed the outputs of the toolkit to the the network (e.g. shifts in the relationships or values of the nodes, such as fewer birds- see for example theme aggregation and overview presentation. That could be done with colour, node size, or edge penwidth.\nColour can be specified differently than is done by default above, such as coloring the nodes within the node groups by outcome, or assigning different node groups different color palettes, following the same idea as the generic colorgroups and colorset arguments in all the plotting functions. There’s an obvious step of making it interactive/dynamic, but that hasn’t been implemented yet.\n\nColour to indicate a value\n\nEdges\nEdges we want to be able to have colour in a column, as it would be if it came in as results from the toolkit. For example, we might want to colour the edges by change between scenarios, or strength of relationships. Down the track we could similarly alter penwidth as well.\nAs a demonstration, let’s add a value column to edges as a mock-up of the toolkit outputs and plot according to that. I’ll also use a continuous palette here rather than the default, since this is now a continuous variable. I’ll use a smaller network to make things visible. Note that above we removed the 5-year targets from the nodes df, and here we use edges since we’re already modifying it. Either approach can drop a set of nodes, though it’s generally easier to use the nodes since nodes can appear in either the from or to of the edges.\n\nedgewithvals &lt;- edges %&gt;% \n  filter(totype != 'target_5_year_2024') %&gt;% \n  mutate(value = rnorm(n()))\n\nmake_causal_plot(nodes,\n                 edgewithvals,\n                 focalnodes = c('NF4', 'Sloanes froglet'),\n                 edge_pal = list(value = 'viridis::plasma'),\n                 edge_colorset = 'value', render = FALSE) %&gt;% \n  DiagrammeR::render_graph()\n\n\n\n\n\n\n\nNodes (and single-colour edges)\nWe can also colour the nodes by results. Here I’ve also set the edges just to a single color - feeding edge_pal or node_pal a single character value specifying a colour or a character vector of length nrow of the relevant dataframe will just insert those values and bypass the palettes. Again, I start by dummying up some ‘toolkit results’ in a value column. Examples where these values do come out of EWR results is in the theme aggregation notebook and the overview presentation.\n\nnodewithvals &lt;- nodes %&gt;% \n  filter(NodeType != 'target_5_year_2024') %&gt;% \n  mutate(value = rnorm(n()))\n\nmake_causal_plot(nodewithvals,\n                 edges,\n                 focalnodes = c('NF4', 'Sloanes froglet'),\n                 edge_pal = 'black',\n                 node_pal = list(value = 'scico::oslo'),\n                 node_colorset = 'value', render = FALSE) %&gt;% \n  DiagrammeR::render_graph()\n\n\n\n\n\n\n\n\nColour within node groups\nWe might want to use different colour palettes within the different node groups, but colour the nodes themselves within them. To do that, we set the *_pal arguments as named lists of palettes, and also pass *_colorgroups arguments so it knows how to split the data into those palettes. This parallels the use in plot_outcomes for other plot types, where colorgroups is the groups that get the palette, while colorset are the individual units within each group that receive colors from the respective palette. Here, we demonstrate with nodes and plot the whole network so we can see what’s happening.\nFirst, set the list of palettes. This could be set by default for the project, along with other default colors.\n\nnode_list_c = list(ewr_code = 'viridis::mako', \n                   env_obj = 'viridis::plasma', \n                   Specific_goal = 'scico::oslo', \n                   Target = 'scico::hawaii', \n                   target_5_year_2024 = 'scico::lisbon')\n\nThen, make the network\n\nmake_causal_plot(nodes, edges, \n                 edge_pal = 'black',\n                 node_pal = node_list_c,\n                 node_colorgroups = 'NodeType',\n                 node_colorset = 'Name',render = FALSE) %&gt;% \n  DiagrammeR::render_graph()\n\n\n\n\n\n\n\nGroupings within node groups\nIt’s possible but gets rapidly bespoke to break those up into more discrete chunks. I’ve had a crude go at establishing a default though, where I grouped EWRs and environmental objectives by their main group, and lumped all the targets by year (though only including 5-year in this example). That default can be accessed by passing 'werp' to as the node_colorset. This is an end-run that builds a new column to use as colorset, defined according to some defaults. The same thing could be done externally by creating a new column to define the colorset outside the function and then making that column colorset.\nIn that case, we might want to use a different set of palettes,\n\nnode_list_g &lt;- list(ewr_code = 'viridis::mako', \n                    env_obj = 'ggthemes::excel_Green', \n                    Specific_goal = 'scico::oslo', \n                    Target = 'calecopal::superbloom3', \n                    target_5_year_2024 = 'calecopal::eschscholzia')\n\nThis lets us set the within-node_colorgroups palettes. The node_colorset = 'werp' just creates a new column in the data that gives rows values we want (e.g. the first two letters of the ewr_codes), which are then used to choose colors from that particular palette. It’s a way to have fewer colors (and more meaningful colors) within the colorgroups.\n\nmake_causal_plot(nodes, edges, \n                 edge_pal = 'black',\n                 node_pal = node_list_g,\n                 node_colorgroups = 'NodeType',\n                 node_colorset = 'werp', render = FALSE) %&gt;% \n  DiagrammeR::render_graph()"
  },
  {
    "objectID": "causal_networks/causal_plots.html#directions-from-here",
    "href": "causal_networks/causal_plots.html#directions-from-here",
    "title": "Causal Network Plotting",
    "section": "Directions from here",
    "text": "Directions from here\nThere’s clearly a lot more that could be done here. The nature of the output and the way I’ve set it up are really aimed at being interactive and usable to investigate the network.\nThe setup here is actually quite similar to the other plot outputs. In all cases, there is much opportunity to adjust the look of the plots to target different uses, but also the ability to establish a consistent default (and look). These plots lend themselves to both notebooks (as here), web, and interactive interfaces (Shiny, observablejs, etc) to investigate the networks themselves or use them to plot results. That gives flexibility to get at whatever the particular question is. Though causal networks are not a typical way to present outputs, here we see that they can be incredibly powerful for not only showing the relationships, but understanding how they change under different scenarios."
  },
  {
    "objectID": "causal_networks/causal_manipulation.html",
    "href": "causal_networks/causal_manipulation.html",
    "title": "Causal network functions",
    "section": "",
    "text": "Purpose\nThis document will cover the functions provided by {werptoolkitr} to interact with and manipulate causal networks.\nFor example\n\nmake_nodes\nmake_edges\nThe pruning function find_realted_nodes\nothers"
  },
  {
    "objectID": "aggregator/theme_agg.html",
    "href": "aggregator/theme_agg.html",
    "title": "Theme aggregation",
    "section": "",
    "text": "library(werptoolkitr)"
  },
  {
    "objectID": "aggregator/theme_agg.html#setting-aggregation-parameters",
    "href": "aggregator/theme_agg.html#setting-aggregation-parameters",
    "title": "Theme aggregation",
    "section": "Setting aggregation parameters",
    "text": "Setting aggregation parameters\nFor each step in the aggregation, we need to specify what levels we are aggregating from and to, the function to use to aggregate, and the mapping between the ‘from’ and ‘to’ levels.\nThe aggsequence list in multi_aggregate (and theme_aggregate) needs to be nested, e.g. links must be defined in causal_ewrs (or other causal mappings) from the ‘from’ and ‘to’ levels at each step. In other words, we can’t go backwards, and we can’t scale between levels with no defined relationship. However, this does not mean we have to always include every level. If a relationship exists, levels can be jumped (e.g. we could go straight from env_obj to target_20_year_2039), and indeed there may not be a defined ordering of some levels, and so it is perfectly reasonable to go from env_obj to both Objective and Target. For EWR outputs, the aggsequence list will typically need to start with ewr_code_timing and aggregate from there into ewr_code and env_obj as everything else flows from that.\nThe funsequence is a list instead of a simple vector because multiple functions can be used at each step. When multiple functions are passed, they are factorial (each function is calculated on the results of all previous aggregations). This keeps the history clean, and allows us to easily unpick the meaning of each value in the output.\nThese aggregation sequences and their associated functions are arguments to multi_aggregate. In practice, they will typically be set to a default value in a parameter file. There is a theme_aggregate function that performs a single-level of theme aggregation, much like spatial_aggregate does for space. I do not focus on that here, because single theme aggregations tend to be less complex than single spatial aggregations, and the capability to pass different sorts of arguments is discussed in space and the syntax notebook. Instead, here we focus on the sequential aggregation provided by multi_aggregate to understand theme aggregation, which is most interesting as a multi-step process along the causal network."
  },
  {
    "objectID": "aggregator/theme_agg.html#data-overview",
    "href": "aggregator/theme_agg.html#data-overview",
    "title": "Theme aggregation",
    "section": "Data overview",
    "text": "Data overview\nThe EWR results come in three main flavours- summary over the span of the run, annual, and all, a continuous set. At the time this was written, the all didn’t exist and annual was broken, so we focus here on the summary data, with updating to handle the others very high priority. We read them in with get_ewr_output also provides some cleanup. The wrapper function read_and_agg reads them in internally and then calls multi_aggregate to avoid having all objects for each scenario in memory. That’s what we’d do in production, typically, but here the goal is to see how the theme aggregation works. The get_ewr_output function has additional arguments to filter by gauge and scenario on read-in, allowing parallelisation without overloading memory.\nWe typically would use prep_ewr_agg (and this is wrapped by multi_aggregate) but that not only calls get_ewr_output, it then makes the data geographic. The geography doesn’t need to happen here since we’re just doing theme."
  },
  {
    "objectID": "aggregator/theme_agg.html#data",
    "href": "aggregator/theme_agg.html#data",
    "title": "Theme aggregation",
    "section": "Data",
    "text": "Data\nWe’ll pull in the summary data to use for demonstration so we can use multi_aggregate directly. If we want to feed a path instead of a dataframe, we would need to use read_and_agg.\n\nsumdat &lt;- get_ewr_output(ewr_results, type = 'summary')\n# Would make it geographic:\n# sumdat &lt;- prep_ewr_agg(ewr_results, type = 'summary', geopath = gpath)\nsumdat\n\n\n\n  \n\n\n\nThere’s an issue with outputType = 'annual' in the version of the EWR tool this was built with. Until I update and test the new EWR tool, skip the annual data.\n\nanndat &lt;- get_ewr_output(ewr_results, type = 'annual')\nanndat\n\n\nNote: I originally wrote get_ewr_output to automatically get both the annual and summary ewr output files, but I think given a typical workflow it will make more sense to just wrap it if we want to more than one type of output. Even if we want to use multiple output types (summary, annual, all, etc), they won’t talk to each other for any of the processing steps, so might as well run in parallel.\n\nWe’ll choose an example gauge to make it easier to visualise the data.\n\n# Dubbo is '421001', has 24 EWRs\n# Warren Weir is '421004', has 30 EWRs. \nexample_gauge &lt;- '421001'"
  },
  {
    "objectID": "aggregator/theme_agg.html#aggregation",
    "href": "aggregator/theme_agg.html#aggregation",
    "title": "Theme aggregation",
    "section": "Aggregation",
    "text": "Aggregation\nAs a demonstration, I’ve set a range of theme levels that hit all the theme relationship dataframes from the causal networks defined in causal_ewr, and set the aggregation functions fairly simply, but with two multi-aggregation steps to illustrate how that works. For more complexity in these aggregation functions, see the spatial notebook and aggregation syntax.\nI’m using CompensatingFactor as the aggregation function for the ewr_code_timing to ewr_code step here, assuming that passing either timing sub-code means the main code passes. A similar approach could be done if we want to lump the ewr_codes themselves, e.g. put EF4a,b,c,d into EF4. I use both ArithmeticMean and LimitingFactor for the 2nd and 3rd levels to demonstrate multiple aggregations and how the outputs from those steps get carried through subsequent steps.\n\naggseq &lt;- list(c('ewr_code_timing', 'ewr_code'),\n               c('ewr_code', \"env_obj\"), \n             c('env_obj', \"Specific_goal\"), \n             c('Specific_goal', 'Objective'), \n             c('Objective', 'target_5_year_2024'))\n\nfunseq &lt;- list(c('CompensatingFactor'),\n               c('ArithmeticMean', 'LimitingFactor'),\n             c('ArithmeticMean', 'LimitingFactor'),\n             c('ArithmeticMean'),\n             c('ArithmeticMean'))\n\nThe groupers and aggCols arguments can take a number of different formats- character vectors, bare column names and sometimes tidyselect, though this is more true in theme_aggregate and limited for multi_aggregate as discussed in the syntax documentation. This capability gives the user quite a few options for specifying the columns to use. The use of selectcreator makes it robust to nonexistent columns with failmissing = FALSE.\nTo create the aggregation, we provide the sequence lists created above, along with the causal links, defined by the causal_edges argument. Because the make_edges function also takes a sequence of node types, we can usually just call make_edges on the list of relationships and the desired set of theme levels. We can also just pass in causal_edges = causal_ewr (the list with all possible links), and theme_aggregate will auto-generate the edges it needs. That’s just a bit less efficient.\n\nsimpleThemeAgg &lt;- multi_aggregate(dat = sumdat,\n                         causal_edges = make_edges(causal_ewr, aggseq),\n                         groupers = c('scenario', 'gauge'),\n                         aggCols = 'ewr_achieved',\n                         aggsequence = aggseq,\n                         funsequence = funseq)\nsimpleThemeAgg\n\n\n\n  \n\n\n\nThat output has 4 columns of output values because aggregation steps are factorial in the number of aggregation functions applied. The second step found the ArithmeticMean and LimitingFactor for ewr_achieved into env_obj and then the third step found the ArithmeticMean and LimitingFactor for each of those outcomes into Specific_goal. Each subsequent step only found the ArithmeticMean for each, and so the number of output columns stopped growing.\n\nTracking aggregation steps\nTracking aggregation steps is critical for knowing the meaning of the numbers produced. We can do that in two different ways- in column headers (names) or in columns themselves.\nTracking history in column names is unweildy, but describes exactly what the numbers are and is smaller in memory. For example, the last column is\n\nnames(simpleThemeAgg)[ncol(simpleThemeAgg)]\n\n[1] \"target_5_year_2024_ArithmeticMean_Objective_ArithmeticMean_Specific_goal_LimitingFactor_env_obj_LimitingFactor_ewr_code_CompensatingFactor_ewr_achieved\"\n\n\nThis says the values in this column are the 5-year targets, calculated as the arithmetic mean of Objectives, which were the arithmetic mean of Specific goals, which were calculated from env_obj as limiting factors, which were obtained from the ewr_code as limiting factors and those were calculated from the ewr_code_timing as compensating factors.\nIt may be easier to think about the meaning of the names from the other direction- ewr_achieved were aggregated from ewr_code_timing into ewr_code as Compensating Factors, then into env_obj as limiting factors- for the env_obj to pass, all ewr_codes contributing to it must pass. Then the env_objs were aggregated into Specific_goal, again as limiting factors, so to meet a goal, all contributing env_obj must pass. Those Specific_goals were then aggregated into Objectives with the arithmetic mean, so the value for an Objective is then the average of the contributing Specific_goals. Since in this example the Specific_goals will be either 1 or 0, this average gives the proportion of Specific_goals that are met for each Objective. Similarly, the 5-year targets were obtained by averaging the Objectives contributing to them.\nA different way to track the aggregations is possible by including them in columns instead of the names. This takes more memory, but can be clearer and makes subsequent uses easier in many cases. For memory purposes, I currently parse the names into columns post-hoc. When we develop the ability to do different aggregations on different groups within a node type, we will need to make the columns as we go (and we will have to use the column method to track, since different things will happen to different values in a column). This is very high priority.\nIn the example above, we can feed the output data to agg_names_to_cols to put the history in columns instead of names.\n\nagg_names_to_cols(simpleThemeAgg, aggsequence = aggseq, funsequence = funseq, aggCols = 'ewr_achieved')\n\n\n\n  \n\n\n\nIn practice, what makes most sense is to use a switch (namehistory = FALSE) inside multi_aggregate to return this format.\n\nsimpleColHistory &lt;-  multi_aggregate(dat = sumdat,\n                         causal_edges = make_edges(causal_ewr, aggseq),\n                         groupers = c('scenario', 'gauge'),\n                         aggCols = 'ewr_achieved',\n                         aggsequence = aggseq,\n                         funsequence = funseq,\n                         namehistory = FALSE)\nsimpleColHistory"
  },
  {
    "objectID": "aggregator/theme_agg.html#arbitrary-input-data",
    "href": "aggregator/theme_agg.html#arbitrary-input-data",
    "title": "Theme aggregation",
    "section": "Arbitrary input data",
    "text": "Arbitrary input data\nThere are three main types of input EWR data (summary, annual, and all), but we expect there will be any number of input datasets once other modules exist. Provided the causal relationships are defined for the input sets, the specific input datasets and columns of data to aggregate are general. For example, we can feed it the annual data (type = 'annual') and aggregate two different columns (feed aggCols a vector of column names). In the case of the annual data, we might also want to group by year in addition to gauge and scenario, and so we add year to the groupers vector. We’ll keep the same sequences of aggregation and functions, noting that they now are calculated for both aggCols.\nTODO turn back on once update to new EWR without annual bug.\n\nannualColHistory &lt;-  multi_aggregate(dat = anndat,\n                                     causal_edges = make_edges(causal_ewr,\n                                                               aggseq),\n                                     groupers = c('scenario', 'gauge', 'year'),\n                                     aggCols = c('num_events', 'event_length'),\n                                     aggsequence = aggseq,\n                                     funsequence = funseq,\n                                     namehistory = FALSE)\nannualColHistory"
  },
  {
    "objectID": "aggregator/theme_agg.html#returning-every-stage",
    "href": "aggregator/theme_agg.html#returning-every-stage",
    "title": "Theme aggregation",
    "section": "Returning every stage",
    "text": "Returning every stage\nBy default, we return only the final aggregation after stepping up the full sequence set by aggsequence. But in some cases, we might want to return all of the intermediate aggregations (e.g. at each step of aggsequence). This full aggregation sequence can be useful for testing and checking, but probably more importantly, allows more complete understanding of the results. For visualization, this allows each step to be fed as colour or other attributes to the causal network. To save all steps in the aggregation sequence, we pass saveintermediate = TRUE to multi_aggregate, and it returns a list of tibbles named by the aggregation level instead of a single final tibble. We cannot just attach the stage results as columns to a flat dataframe because the aggregation is many-to-many, and so the rows do not match and are not strictly nested. Thus, each stage needs its own dataframe to avoid duplicating or deleting data. Moreover, using the additional flexibility of a list of dataframes is necessary for interleaved aggregations across theme, space, and time axes.\n\nallsteps  &lt;-  multi_aggregate(dat = sumdat,\n                         causal_edges = make_edges(causal_ewr, aggseq),\n                         groupers = c('scenario', 'gauge'),\n                         aggCols = 'ewr_achieved',\n                         aggsequence = aggseq,\n                         funsequence = funseq,\n                         saveintermediate = TRUE,\n                         namehistory = FALSE)\n\nnames(allsteps)\n\n[1] \"ewr_code_timing\"    \"ewr_code\"           \"env_obj\"           \n[4] \"Specific_goal\"      \"Objective\"          \"target_5_year_2024\"\n\n\n\nCausal plot\nBy returning values at each stage, we can map those to colour (and later size) in a causal network. In practice, this will happen in the Comparer (and the initial setup data arrangement will be made into a function there), but we can demonstrate it quickly here. Here, we map the values of the aggregation to node color. To do this, I’ll follow the usual causal_plots approach of making edges and nodes, and then use a join to attach the value to each node.\nTo keep this demonstration from becoming too unwieldy, we limit the edge creation to a single gauge, and so will filter the theme aggregations accordingly (or just rely on the join to drop). The ewr_node_timing outcomes are likely just confusing to include here, so we cut it off.\nThe first step is to generate the edges and nodes for the network we want to look at.\n\nedges &lt;- make_edges(causal_ewr, \n                    fromtos = aggseq[2:length(aggseq)],\n                    gaugefilter = example_gauge)\n\nnodes &lt;- make_nodes(edges)\n\nNow, extract the values we want from the aggregation and join them to the nodes.\nTODO this will all be done in a data prep function in the comparer that processes multi-step aggregation lists. This is high priority, but needs thought for how to handle interleaved aggregation along different axes.\n\n# need to grab the right set of aggregations if there are multiple at some stages\nwhichaggs &lt;- c('CompensatingFactor',\n               'ArithmeticMean',\n               'ArithmeticMean',\n               'ArithmeticMean',\n               'ArithmeticMean')\n\n# What is the column that defines the value?\nvalcol &lt;- 'ewr_achieved'\n\n# Get the values for each node\ntargetlevels &lt;- names(allsteps)\ntargetlevels[1] &lt;- 'ewr_code_timing'\naggvals &lt;- extract_vals_causal(allsteps, whichaggs, valcol, \n                               targetlevels = targetlevels)\n\n# Cut off the ewr_code_timing- should really have a drop_step argument to just not return it? In the extract_vals_causal\naggvals &lt;- aggvals %&gt;% dplyr::filter(NodeType != 'ewr_code_timing')\n# cut to relevant gauge, then remove- causes problems since node levels above env_obj aren't gauge-referenced\naggvals &lt;- aggvals %&gt;% dplyr::filter(gauge == example_gauge) %&gt;% \n  dplyr::select(-gauge)\n\n# join to the nodes\nnodes_with_vals &lt;- dplyr::left_join(nodes, aggvals)\n\nMake the causal network plot with the nodes we chose and colour by the values we’ve just attached to them from the aggregation. At present, it is easiest to make separate plots per scenario or other grouping ( Figure 1 , Figure 2 ). For example, in the increased watering scenario, we see more light colours, and so better performance across the range of outcomes. Further network outputs are provided in the Comparer.\n\naggNetwork &lt;- make_causal_plot(nodes = dplyr::filter(nodes_with_vals, \n                                        scenario == 'base'),\n                 edges = edges,\n                 edge_pal = 'black',\n                 node_pal = list(value = 'scico::tokyo'),\n                 node_colorset = 'ewr_achieved',\n                 render = FALSE)\n\nDiagrammeR::render_graph(aggNetwork)\n\n\n\n\n\nFigure 1: Causal network for baseline scenario at example gauge, coloured by proportion passing at each node, e.g. Arithmetic Means at every step. Light yellow is 1, dark purple is 0.\n\n\n\n\naggNetwork &lt;- make_causal_plot(nodes = dplyr::filter(nodes_with_vals, \n                                        scenario == 'up2'),\n                 edges = edges,\n                 edge_pal = 'black',\n                 node_pal = list(value = 'scico::tokyo'),\n                 node_colorset = 'ewr_achieved',\n                 render = FALSE)\n\nDiagrammeR::render_graph(aggNetwork)\n\n\n\n\n\nFigure 2: Causal network for 4x scenario at example gauge, coloured by proportion passing at each node, e.g. Arithmetic Means at every step. Light yellow is 1, dark purple is 0."
  },
  {
    "objectID": "aggregator/theme_agg.html#user-set-functions",
    "href": "aggregator/theme_agg.html#user-set-functions",
    "title": "Theme aggregation",
    "section": "User-set functions",
    "text": "User-set functions\nWe have established a simple set of default aggregation functions (ArithmeticMean, GeometricMean, LimitingFactor, and CompensatingFactor), available in default_agg_functions.R. I expect that list to grow, but it is also possible to supply user-defined functions to include in funsequence. Previously, we have used this sort of approach for things like threshold functions. For example, we might want to know the mean event length, but only for events longer than 2 days for the ewr to objective aggregation, and thereafter scale up with ArithmeticMean. We can do this by specifying a new function and including it in the list given to funsequence .\n\nevent2 &lt;- function(x) {\n  mean(ifelse(x &gt; 2, x, NA), na.rm = TRUE)\n}\n\nnewfuns &lt;- list(c('CompensatingFactor'),\n                  c('event2'),\n             c('event2'),\n             c('ArithmeticMean'),\n             c('ArithmeticMean'))\n\nThis is currently turned off until we update to the new EWR without a bug in the annual results.\n\nannualEv2 &lt;-  multi_aggregate(dat = anndat,\n                         causal_edges = make_edges(causal_ewr, aggseq),\n                         groupers = c('scenario', 'gauge', 'year'),\n                         aggCols = 'event_length',\n                         aggsequence = aggseq,\n                         funsequence = newfuns,\n                         namehistory = FALSE)\n# lots of NaN because many years and locations didn't have events &gt; 2, so for ease of viewing, filter\nannualEv2 %&gt;% dplyr::filter(!is.nan(event_length))"
  },
  {
    "objectID": "aggregator/theme_agg.html#gauge-and-scenario--filtering",
    "href": "aggregator/theme_agg.html#gauge-and-scenario--filtering",
    "title": "Theme aggregation",
    "section": "Gauge and scenario -filtering",
    "text": "Gauge and scenario -filtering\nReading in all of the EWR results across all gauges and scenarios could be massive, depending on the spatial scale and the number of scenarios, and so we might want to parallelise over gauges or scenarios. We also might only be interested in some subset for things like plotting. To address this, get_ewr_output has gaugefilter and scenariofilter arguments. This will is particularly useful once we have lots of data that doesn’t fit in memory or want to parallel process - if we have all the data in memory already, we can just pipe it in through a filter (or filter the first argument), but if we read in in parallel from a path, we can greatly speed up processing.\nTo only read-in the relevant data, we use the read_and_agg wrapper. The gaugefilter argument only works (currently) if there are separate files for each gauge. Once we settle on a data format, I will re-write the gaugefilter differently to only read the desired gauge from the file, though that won’t be possible with interleaved spatial aggregation.\n\nsmallreadagg  &lt;-  read_and_agg(datpath = ewr_results, type = 'summary',\n                               geopath = bom_basin_gauges,\n                               causalpath = causal_ewr,\n                               groupers = c('scenario', 'gauge'),\n                               aggCols = 'ewr_achieved',\n                               aggsequence = aggseq,\n                               funsequence = funseq,\n                               namehistory = FALSE, \n                               gaugefilter = NULL,\n                               scenariofilter = 'base')\n\ntable(smallreadagg$gauge, smallreadagg$scenario)\n\n        \n         base\n  412002  304\n  412004  304\n  412005  304\n  412011  304\n  412012  304\n  412016  208\n  412033  304\n  412038  304\n  412039  304\n  412046  208\n  412122   76\n  412124   76\n  412163  196\n  412188  200\n  412189  280\n  419001  280\n  419006  280\n  419007  280\n  419012  280\n  419015  280\n  419016  280\n  419020  280\n  419021  280\n  419022  280\n  419026  284\n  419027  280\n  419028  280\n  419032  280\n  419039  280\n  419045  280\n  419049  280\n  419091  284\n  420020    4\n  421001  192\n  421004  288\n  421011  280\n  421012  280\n  421019  208\n  421022  276\n  421023  276\n  421090  276\n  421146    4\n  422001  292\n  422028  292\n\n\nFor a one-off that fits in memory, this is slower than filtering the data after it’s in-memory, since the read-in happens first. The advantage comes when we don’t want (or can’t fit) all of the original data in memory, such as parallelisation over scenarios.\nThe read_and_agg function is also helpful if we just want to use paths as arguments instead of reading the data in and then calling multi_aggregate. In that case, we might not use any *filter arguments, in which case it works just like multi_aggregate, but takes paths instead of objects as arguments. For example, saving all intermediate and no filtering can be done with the path to data ewr_results.\n\nreadallsteps &lt;- read_and_agg(datpath = ewr_results, type = 'summary',\n                             geopath = bom_basin_gauges,\n                             causalpath = causal_ewr,\n                             groupers = c('scenario', 'gauge'),\n                             aggCols = 'ewr_achieved',\n                             aggsequence = aggseq,\n                             funsequence = funseq,\n                             saveintermediate = TRUE,\n                             namehistory = FALSE)\n\nnames(readallsteps)\n\n[1] \"ewr_code_timing\"    \"ewr_code\"           \"env_obj\"           \n[4] \"Specific_goal\"      \"Objective\"          \"target_5_year_2024\"\n\n\n\nParallelisation\nThe gauge and scenario filtering gives an easy way to parallelise. I haven’t written this into a function yet until we settle on how to use Azure batching with the toolkit. It will likely involve a wrapper around read_and_agg, but could be incorporated as parameters fed to read_and_agg itself. We demo how it works here. Parallelisation will not only speed up the processing, but because we can do the data reads inside the function, parallelisation over scenarios (and gauges, if not spatially-aggregating) avoids reading all the data in at once and so reduces unnecessary memory use.\nFor this demonstration, we are getting the gauge and scenario lists from previously-read data, but in typical use they would be available from scenario metadata.\n::: {#future-export .border: .2px .solid .gray; .color: .gray} Note: future is supposed to handle the .export from the calling environment, and seems to do just fine with everything except the aggregation functions. That can happen with nested foreach inside functions, but I think here it might be happening because of the way we’re using {{}} to pass an arbitrary set of functions. Easy enough to fix, by passing something to .export, but annoying. If we end up not using {future} for parallelisation on Azure, this will be moot. :::\nThe example below performs the same processing as above to produce output identical to simpleThemeAgg, but done in parallel. This is slower for this small simple demonstration because of overhead, but has the potential to be much faster for larger jobs.\nWe’re not parallelizing over gauges here because we’re unlikely to be able to do so with interleaved aggregation steps, but a nested loop would work if we are only aggregating in time or theme dimensions.\n\nlibrary(foreach)\nlibrary(doFuture)\nregisterDoFuture()\nplan(multisession)\n\nallgauges &lt;- 'all' # unique(simpleThemeAgg$gauge)\nallscenes &lt;- unique(simpleThemeAgg$scenario)\n\nparThemeAgg &lt;- foreach(s = allscenes, \n                       .combine = dplyr::bind_rows) %dopar% {\n  # If parallel over gauges\n  # foreach(g = allgauges, \n  #         .combine = dplyr::bind_rows) %dopar% {\n            \n    read_and_agg(datpath = ewr_results, type = 'summary',\n                 geopath = bom_basin_gauges,\n                 causalpath = causal_ewr,\n                 groupers = c('scenario', 'gauge'),\n                 aggCols = 'ewr_achieved',\n                 aggsequence = aggseq,\n                 funsequence = funseq,\n                 namehistory = TRUE, \n                 gaugefilter = NULL,\n                 scenariofilter = s)\n  }\n\nparThemeAgg\n\n\n\n  \n\n\n\nIf we’re doing something here that is too big to return the full output (likely in practice), it would also be straightforward for the parallel loop to save the iterations and not return anything. Then we could read the output in in pieces into the comparer."
  },
  {
    "objectID": "aggregator/aggregation_syntax.html",
    "href": "aggregator/aggregation_syntax.html",
    "title": "Aggregation syntax",
    "section": "",
    "text": "library(werptoolkitr)"
  },
  {
    "objectID": "aggregator/aggregation_syntax.html#argument-options-and-syntax",
    "href": "aggregator/aggregation_syntax.html#argument-options-and-syntax",
    "title": "Aggregation syntax",
    "section": "Argument options and syntax",
    "text": "Argument options and syntax\nThe aggregation functions have flexible syntax in some of their arguments, particulary for selecting grouping columns, the column(s) of values to aggregate, and the specification of aggregation functions. In general, these apply across the functions, though multi_aggregate has a bit less flexibility than the internal spatial_aggregate, theme_aggregate, and general_aggregate, largely as a consequence of passing through the call stack. Here, I use primarily the example of spatial_aggregate to illustrate the different arguments, their syntax, and why we might use it.\nI’ll create test data as in the spatial notebook.\n\nproject_dir &lt;- file.path('more_scenarios')\newr_results &lt;- file.path(project_dir, 'module_output', 'EWR')\nsumdat &lt;- prep_ewr_agg(ewr_results, type = 'summary', geopath = bom_basin_gauges)\n\nthemeseq &lt;- list(c('ewr_code_timing', 'ewr_code'),\n               c('ewr_code', \"env_obj\"))\n\nfunseq &lt;- list(c('CompensatingFactor'),\n               c('ArithmeticMean'))\n\nsimpleThemeAgg &lt;- multi_aggregate(dat = sumdat,\n                         causal_edges = make_edges(causal_ewr, themeseq),\n                         groupers = c('scenario', 'gauge'),\n                         aggCols = 'ewr_achieved',\n                         aggsequence = themeseq,\n                         funsequence = funseq)\n\nLinking to GEOS 3.11.2, GDAL 3.6.2, PROJ 9.2.0; sf_use_s2() is TRUE\n\n\nJoining with `by = join_by(gauge)`\nJoining with `by = join_by(gauge, ewr_code, ewr_code_timing, PlanningUnitID)`\nJoining with `by = join_by(gauge)`\nJoining with `by = join_by(gauge, ewr_code, PlanningUnitID)`\n\nsimpleThemeAgg\n\n\n\n  \n\n\n\n\nSelecting grouping and data columns\nBoth aggCols and groupers can be character vectors, bare data-variable names, or we might want to use tidyselect syntax. For example, maybe we want to use ends_with('ewr_achieved') as above to grab pre-aggregated columns with long name histories, as in simpleThemeAgg . This is handled under the hood by selectcreator and careful parsing in the function stack. Above, we had groupers as a character vector and aggCols as tidyselect, but now we flip, and groupers is a vector of tidyselect and bare names, while aggCols is a character.\n\nNote that multi_aggregate takes advantage of this tidyselect ability under the hood to deal with the ever-lengthening column names (and sometimes expanding number of value columns if we have multiple aggregation functions at a step). This means, though, that we cannot use tidyselect to specify aggCols in multi_aggregate- it would collide with the internal tidyselect, and so we are limited to characters in that case.\nThe other restriction in multi_aggregate is that it does not accept bare function names, as they get lost for the purposes of naming the history by the time they get used in the call stack.\n\n\nobj2poly2 &lt;- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = c(starts_with('sce'), env_obj),\n                             aggCols = \"env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved\",\n                             funlist = ArithmeticMean,\n                             keepAllPolys = TRUE)\n\nobj2poly2\n\n\n\n  \n\n\n\nWe can see we get the same result as obj2poly with different ways of specifying aggCols and groupers.\nThere are times when we might want to send a vector of names, but ignore those not in the data. Most likely would be something like a set of possible grouping variables but only using them if they exist, and ignoring if not. This is a bit sloppy, but is useful sometimes to send the same set of groupers to several datasets. It fails by default, but setting failmissing = FALSE allows it to pass. Here, the ‘extra_grouper’ column doesn’t exist in the data and so is ignored.\n\nobj2polyF &lt;- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = c('scenario', 'env_obj', 'extra_grouper'),\n                             aggCols = ends_with('ewr_achieved'),\n                             funlist = ArithmeticMean,\n                             keepAllPolys = TRUE,\n                             failmissing = FALSE)\nobj2polyF\n\n\n\n  \n\n\n\n\n\nFunctions\nWe can pass single bare aggregation function names, characters, or named lists defining functions with arguments. Above, we have been specifying the function to apply as just a single bare function name. Now, we explore some other possibilities and capabilities of the aggregator.\nMost simply, we can pass character names of functions instead of bare\n\ndoublesimplechar &lt;- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = c('scenario', 'env_obj'),\n                             aggCols = ends_with('ewr_achieved'),\n                             funlist = 'ArithmeticMean',\n                             keepAllPolys = TRUE,\n                             failmissing = FALSE)\ndoublesimplechar\n\n\n\n  \n\n\n\nIf we want to do two different aggregations on the same data, we can pass a vector of names.\n\nsimplefuns &lt;- c('ArithmeticMean', 'GeometricMean')\n\ndoublesimplec &lt;- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = c('scenario', 'env_obj'),\n                             aggCols = ends_with('ewr_achieved'),\n                             funlist = simplefuns,\n                             keepAllPolys = TRUE,\n                             failmissing = FALSE)\ndoublesimplec\n\n\n\n  \n\n\n\nNote- if passing multiple functions, it does not work to pass bare names. It just gets too complex to handle, and the bare names is really just a convenience shorthand.\n\nArguments to aggregation functions\nThere are three primary ways to specify function arguments- using …, writing a wrapper function with the arguments specified (e.g. see ArithmeticMean, which is just mean(x, na.rm = TRUE) ), or using anonymous functions with ~ syntax in a named list. The simplest version is to use …, but this really only works in simple cases, like passing na.rm = TRUE. It does work for multiple functions, but starts getting convoluted and unclear if they don’t share arguments or there are many arguments.\n\nsinglearg &lt;- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = 'scenario',\n                             aggCols = ends_with('ewr_achieved'),\n                 funlist = c(mean, sd),\n                 na.rm = TRUE,\n                 keepAllPolys = TRUE,\n                 failmissing = FALSE)\nsinglearg\n\n\n\n  \n\n\n\nWe can also pass arguments by sending a list of functions with their arguments. This is far more flexible than the … approach, as we can send any arguments to any functions this way. For clarity, we demonstrate it here for the same situation- passing the na.rm argument to mean and sd. This also lets us control the function names, because the list-names do not need to match the function names. The list-names are what get used in history-tracking (see the column names).\n\nsimplelamfuns &lt;- list(meanna = ~mean(., na.rm = TRUE), \n                     sdna = ~sd(., na.rm = TRUE))\n\ndoublelam &lt;- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = c('scenario', 'env_obj'),\n                             aggCols = ends_with('ewr_achieved'),\n                             funlist = simplelamfuns,\n                             keepAllPolys = TRUE,\n                             failmissing = FALSE)\ndoublelam\n\n\n\n  \n\n\n\nNote: if using anonymous functions in a list this way, they need to use rlang ~ syntax, not base \\(x) or function(x){}. That’s on the to-do list (and has now been implemented but only partially tested, so try but double check the output). It’s hopefully not much of a constraint, and anything complex can be written as a standard function and called that way.\nIt’s fairly common that we’ll have vector arguments, especially for the spatial aggregations. One primary example is weightings. The most flexible approach requires these vectors to be attached to the data before it enters the function (vs. creating them automatically in-function; though that is possible it gets fragile to handle arbitrary names and maintain the correct groupings). So, here we assume that the vectors will be columns in the dataset, and demonstrate with weighted means on dummy weights.\n::: {#dplyr 1.1 issues style=“color: gray”} As of {dplyr} 1.1, if we pass a function with a data-variable argument (e.g. the name of a column in the dataframe) we have to wrap the list in rlang::quo . Otherwise it looks for an object with that name instead of a column. We’re working on a cleaner way to handle this, but for now this is what we have to do. If we have several layers of aggregation, the inside level where the function is defined needs to be wrapped. E.g.\nfunlist &lt;- list(c('ArithmeticMean', 'LimitingFactor'),\n                rlang::quo(list(wm = ~weighted.mean(., area, na.rm = TRUE))),\n                rlang::quo(list(wm = ~weighted.mean(., area, na.rm = TRUE))))\n:::\n\nveclamfuns &lt;- rlang::quo(list(meanna = ~mean(., na.rm = TRUE), \n                     sdna = ~sd(., na.rm = TRUE),\n                     wmna = ~weighted.mean(., wt, na.rm = TRUE)))\n\n# Not really meaningful, but weight by the number of gauges.\nwtgauge &lt;- simpleThemeAgg %&gt;% \n  dplyr::group_by(scenario, gauge) %&gt;% \n  dplyr::mutate(wt = dplyr::n()) %&gt;% \n  dplyr::ungroup()\n\ntriplevec &lt;- spatial_aggregate(dat = wtgauge, \n                             to_geo = sdl_units,\n                             groupers = c('scenario', 'env_obj'),\n                             aggCols = ends_with('ewr_achieved'),\n                             funlist = veclamfuns,\n                             keepAllPolys = TRUE,\n                             failmissing = FALSE)\n\ntriplevec\n\n\n\n  \n\n\n\nIf we want to have custom functions with vector data arguments, we still need to use the tilde notation to point to those arguments. Making a dummy function that just adds two to the weighted mean, the wt argument doesn’t get seen if we just say funlist = wt2. Instead, we need to use a list.\n\nwt2 &lt;- function(x, wt) {\n  2+weighted.mean(x, w = wt, na.rm = TRUE)\n}\n\nwt2list &lt;- rlang::quo(list(wt2 = ~wt2(., wt)))\n\n\nvecnamedfun &lt;- spatial_aggregate(dat = wtgauge, \n                             to_geo = sdl_units,\n                             groupers = c('scenario', 'env_obj'),\n                             aggCols = ends_with('ewr_achieved'),\n                             funlist = wt2list,\n                             keepAllPolys = TRUE,\n                             failmissing = FALSE)\n\nvecnamedfun\n\n\n\n  \n\n\n\n\n\nThe ‘area’ exception\nThe only exception to attaching vector arguments are situations where the needed vector arguments depend on both sets of from and to data/polygons, and so can’t be pre-attached. The main way this comes up is with area-weighting, so spatial_joiner calculates areas so there is always an area column available for weighting.\nIn summary, we can pass single functions and their arguments in ellipses, complex lists of multiple functions using tilde-style anonymous functions, which can have vector arguments (as long as the vector is attached to the data), and lists of multiple function names. Note, however, that while we can use bare names in spatial_aggregate and theme_aggregate, we can’t use bare function names in multi_aggregate because they get lost for the namehistory creation. The only thing we can’t do currently is pass unattached vector args. I have to do such convoluted things for that to work with one function, and it’s so easy to just bind them on, I think that’s a tradeoff worth making. We can reassess if this becomes an issue later."
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#full-toolkit-run",
    "href": "Presentation/walkthrough_presentation.html#full-toolkit-run",
    "title": "Toolkit walkthrough",
    "section": "Full toolkit run",
    "text": "Full toolkit run\n\nlibrary(werptoolkitr)\nlibrary(sf)\nlibrary(dplyr)\n\n\nMany options we could set\nSee options at github pages site for WERP_toolkit_demo\nCan run with minimal arguments"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#paths",
    "href": "Presentation/walkthrough_presentation.html#paths",
    "title": "Toolkit walkthrough",
    "section": "Paths",
    "text": "Paths\n\n# Outer directory for scenario\nproject_dir = file.path('more_scenarios')\n\n# Hydrographs (expected to exist already)\nhydro_dir = file.path(project_dir, 'hydrographs')\n\n# Generated data\n# EWR outputs (will be created here in controller, read from here in aggregator)\newr_results &lt;- file.path(project_dir, 'module_output', 'EWR')\n\n# outputs of aggregator. There may be multiple modules\nagg_results &lt;- file.path(project_dir, 'aggregator_output')"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#ewr-controls",
    "href": "Presentation/walkthrough_presentation.html#ewr-controls",
    "title": "Toolkit walkthrough",
    "section": "EWR controls",
    "text": "EWR controls\n\nOther modules as they are available\n\n\noutputType &lt;- list('summary')\nreturnType &lt;- list('none') # list('summary', 'all')"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#aggregation-sequencing",
    "href": "Presentation/walkthrough_presentation.html#aggregation-sequencing",
    "title": "Toolkit walkthrough",
    "section": "Aggregation sequencing",
    "text": "Aggregation sequencing\n\nSequence of steps\nSequence of functions\n\n\naggseq &lt;- list(ewr_code = c('ewr_code_timing', 'ewr_code'),\n               env_obj =  c('ewr_code', \"env_obj\"),\n               sdl_units = sdl_units,\n               Specific_goal = c('env_obj', \"Specific_goal\"),\n               catchment = cewo_valleys,\n               Objective = c('Specific_goal', 'Objective'),\n               mdb = basin,\n               target_5_year_2024 = c('Objective', 'target_5_year_2024'))\n\n\nfunseq &lt;- list(c('CompensatingFactor'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               rlang::quo(list(wm = ~weighted.mean(., w = area, \n                                        na.rm = TRUE))),\n               c('ArithmeticMean'),\n               \n               rlang::quo(list(wm = ~weighted.mean(., w = area, \n                                    na.rm = TRUE))),\n               c('ArithmeticMean'))"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#module",
    "href": "Presentation/walkthrough_presentation.html#module",
    "title": "Toolkit walkthrough",
    "section": "Module",
    "text": "Module\n\nCurrently just EWR\nWhere are hydrographs\nWhere to save output\n\n\newr_out &lt;- prep_run_save_ewrs_R(scenario_dir = hydro_dir, \n                                  output_dir = project_dir, \n                                  outputType = outputType,\n                                  returnType = returnType)"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#aggregator",
    "href": "Presentation/walkthrough_presentation.html#aggregator",
    "title": "Toolkit walkthrough",
    "section": "Aggregator",
    "text": "Aggregator\nReturning instead of saving for presentation\n\nSequence (levels of each theme, space, time axis)\nAggregation functions at each step\n\n\nagged_data &lt;- read_and_agg(datpath = ewr_results, \n           type = 'summary',\n           geopath = bom_basin_gauges,\n           causalpath = causal_ewr,\n           groupers = 'scenario',\n           aggCols = 'ewr_achieved',\n           aggsequence = aggseq,\n           funsequence = funseq,\n           saveintermediate = TRUE,\n           namehistory = FALSE,\n           keepAllPolys = FALSE,\n           returnList = TRUE,\n           savepath = NULL)"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#input-hydrographs",
    "href": "Presentation/walkthrough_presentation.html#input-hydrographs",
    "title": "Toolkit walkthrough",
    "section": "Input hydrographs",
    "text": "Input hydrographs"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#maps-and-spatial-scaling",
    "href": "Presentation/walkthrough_presentation.html#maps-and-spatial-scaling",
    "title": "Toolkit walkthrough",
    "section": "Maps and spatial scaling",
    "text": "Maps and spatial scaling"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#bars--sdl-units-and-scenarios",
    "href": "Presentation/walkthrough_presentation.html#bars--sdl-units-and-scenarios",
    "title": "Toolkit walkthrough",
    "section": "Bars- SDL units and scenarios",
    "text": "Bars- SDL units and scenarios\nSDL unit differences in all environmental objectives"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#objective-and-scenario-comparisons",
    "href": "Presentation/walkthrough_presentation.html#objective-and-scenario-comparisons",
    "title": "Toolkit walkthrough",
    "section": "Objective and scenario comparisons",
    "text": "Objective and scenario comparisons\n\nBasinSDL units"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#lines-and-baseline",
    "href": "Presentation/walkthrough_presentation.html#lines-and-baseline",
    "title": "Toolkit walkthrough",
    "section": "Lines and baseline",
    "text": "Lines and baseline\nChange relative to baseline available to all plots\n\nDisproportionate response"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#fits",
    "href": "Presentation/walkthrough_presentation.html#fits",
    "title": "Toolkit walkthrough",
    "section": "Fits",
    "text": "Fits\nSmoothed fit of all environmental objectives in each group\n\nRelative to baseline"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#causal-networks",
    "href": "Presentation/walkthrough_presentation.html#causal-networks",
    "title": "Toolkit walkthrough",
    "section": "Causal networks",
    "text": "Causal networks\n\nDown 2BaselineUp 2"
  },
  {
    "objectID": "aggregator/aggregation_overview.html",
    "href": "aggregator/aggregation_overview.html",
    "title": "Aggregation overview",
    "section": "",
    "text": "Incoming data from modules is typically very granular in many dimensions (as it should be if the modules are modelling data near the scale of processes). However, this means that there are thousands of different outcomes across the basin and through time. To make that useful for anything other than targeted local planning, we need to aggregate. Here, we aggregate along three dimensions- space, time, and ‘Objective’, where ‘Objective’ is the axis along increasing organisational levels, for example flow requirements for fish spawning to fish spawning to fish populations to overall environmental success.\nWe want to be able to aggregate along each of these dimensions with any number of aggregation functions (e.g. mean, min, max, more complex) to reflect the processes being aggregated. These should be factorial- if we aggregate with min and max at step 2 (generating two columns of aggregated data), then each of those columns should be aggregated according to the step-3 aggregation, and so on.\nWe want to be able to interleave the dimensions, e.g. aggregate along the Objective dimension to some intermediate level, then aggregate in space, then time, then more Objective levels, then more time and more space.\nTo achieve this, we have developed a flexible set of aggregation functions that take a list of aggregation steps, each of which can be along any dimension, and a matching list of aggregation functions to apply to that step in the aggregation. It is possible to ask for multiple functions per step.\n\n\n\nAggregation can occur along three axes: space, time, and objective. The user can provide the sequence of aggregation steps and the aggregation function(s) to apply at each step."
  },
  {
    "objectID": "aggregator/aggregation_overview.html#goals",
    "href": "aggregator/aggregation_overview.html#goals",
    "title": "Aggregation overview",
    "section": "",
    "text": "Incoming data from modules is typically very granular in many dimensions (as it should be if the modules are modelling data near the scale of processes). However, this means that there are thousands of different outcomes across the basin and through time. To make that useful for anything other than targeted local planning, we need to aggregate. Here, we aggregate along three dimensions- space, time, and ‘Objective’, where ‘Objective’ is the axis along increasing organisational levels, for example flow requirements for fish spawning to fish spawning to fish populations to overall environmental success.\nWe want to be able to aggregate along each of these dimensions with any number of aggregation functions (e.g. mean, min, max, more complex) to reflect the processes being aggregated. These should be factorial- if we aggregate with min and max at step 2 (generating two columns of aggregated data), then each of those columns should be aggregated according to the step-3 aggregation, and so on.\nWe want to be able to interleave the dimensions, e.g. aggregate along the Objective dimension to some intermediate level, then aggregate in space, then time, then more Objective levels, then more time and more space.\nTo achieve this, we have developed a flexible set of aggregation functions that take a list of aggregation steps, each of which can be along any dimension, and a matching list of aggregation functions to apply to that step in the aggregation. It is possible to ask for multiple functions per step.\n\n\n\nAggregation can occur along three axes: space, time, and objective. The user can provide the sequence of aggregation steps and the aggregation function(s) to apply at each step."
  },
  {
    "objectID": "aggregator/aggregation_overview.html#tracking",
    "href": "aggregator/aggregation_overview.html#tracking",
    "title": "Aggregation overview",
    "section": "Tracking",
    "text": "Tracking\nIn general, aggregation over many steps can get quite complicated to track, particularly if some of the steps have multiple aggregation functions. Tracking the provenance of the final values is therefore critical to understand their meaning. By default, columns of values are named in a way that tracks their provenance, e.g. step_function_step_function_originalName. This is memory-friendly but ugly, and so we can also stack this information into columns (two for each step- one the step, the second the function) with the agg_names_to_cols function.\nFurther, in the case of a multi-step aggregation, we can either save only the final output (better for memory) or save the entire stepwise procedure, which can be very useful both for investigating results and visualisation, and it is often the case that we want to ask questions of several levels anyway."
  },
  {
    "objectID": "aggregator/aggregation_overview.html#user-inputs",
    "href": "aggregator/aggregation_overview.html#user-inputs",
    "title": "Aggregation overview",
    "section": "User inputs",
    "text": "User inputs\nThe user potentially has control over a number of decisions for the aggregation-\n\nSequencing in multiple dimensions\nAggregation function(s) to apply at each step\nData to aggregate (one or more columns)\nAny desired groupings to retain\nVarious others related primarily to data format (tracking format, saving each aggregation step, retention of NA spatial units, etc)\n\nMany of these can be specified in a number of ways. I have tried to demonstrate a number of these capabilities and options in the spatial and theme notebooks, while keeping to simpler, more typical uses in the interleaved example and the full toolkit. Some of the most useful, but also trickiest, capabilities revolve around being able to format the groupers and aggregation columns as bare names, character vectors, or tidyselect syntax, and the functions as single or multiple at each step, defined as bare names, characters, or lists with arguments. These approaches make the functions highly general and flexible to new datasets (and sequential aggregation) and custom aggregation functions.\nThe other powerful user input is the foundation of the multidimensional aggregator- the sequencing lists. These are demonstrated in all the notebooks, but are perhaps most interesting in the interleaved version. By default, only the final output is retained, but saveintermediate = TRUE saves a list of each step, which can be very useful.\nThe data itself and output data are also controlled by the user (by necessity). For inputs, multi_aggregate expects the incoming data to be in memory. There is also a wrapper read_and_agg that takes paths as arguments and does the read-in of the data internally and then runs multi_aggregate. This is often a useful approach, allowing parallelisation, better memory management, and it is far easier to use paths in a config file of arguments. Outputs from multi_aggregate are R objects- a dataframe or a list- and are returned back to the calling environment. The read_and_agg wrapper, however, can both return R objects to the session returnList = TRUE and save to a file if savepath = \"path/to/outfile\" ."
  },
  {
    "objectID": "aggregator/aggregation_overview.html#arguments-and-use-cases",
    "href": "aggregator/aggregation_overview.html#arguments-and-use-cases",
    "title": "Aggregation overview",
    "section": "Arguments and use-cases",
    "text": "Arguments and use-cases\nWe show the aggregation happening in multiple notebooks, spatial_agg.qmd, theme_agg.qmd, and theme_space_agg.qmd. These focus on different aspects, but there are valuable demonstrations of capacity in each. In addition to showing spatial aggregation, for example, the spatial aggregation notebook also works through a number of examples of how we can specify grouping, the columns to aggregate, and functions to use that are general across all the aggregation functions- we just use the spatial aggregation for an example. The different saveintermediate and namehistory options are demonstrated in each of those notebooks, showing how we can access the stepwise aggregations or only the final result, and ensure we know the exact meaning of each value in the output."
  },
  {
    "objectID": "aggregator/aggregation_overview.html#limitations",
    "href": "aggregator/aggregation_overview.html#limitations",
    "title": "Aggregation overview",
    "section": "Limitations",
    "text": "Limitations\n\nAt the time of development, there was no temporally resolved output data, and so temporal aggregation is not yet available. This is very high on the priority list, and the framework is in place to do it.\nDifferent aggregation functions for different rows in the data (e.g. mean of fish breeding, min of bird foraging) are not yet available\n\n\n-   Multiple functions are possible per step, and different functions are possible for different steps\n\n-   Different functions for different rows *within a step* requires mapping of what those functions should be, and this does not exist. Still, the capability to do this is very high on the priority list, and should not be too difficult."
  },
  {
    "objectID": "aggregator/aggregation_overview.html#development-notes",
    "href": "aggregator/aggregation_overview.html#development-notes",
    "title": "Aggregation overview",
    "section": "Development notes",
    "text": "Development notes\nI have assumed that we primarily have point data (gauges), rather than rasters, but handling rasters is a reasonably straightforward modification (and I have the code to do it elsewhere). Because of the current focus on gauges, I’m using sf primarily, but stars could be useful depending on where we get with input formats (netcdf etc) and higher-dimension data, or if we end up using rasters.\nTypically, aggregating (and some other operations) on `sf` dataframes with geometry is *much* slower than without. So I’ve put a heavy focus on safely stripping and re-adding geometry. This allows us to use dataframes that reference geometry without the geometry itself attached and only take the performance hit from geometry if it is needed.\nSpatial aggregation is inherently slow because of the geometric operations (specifically sf::st_intersection) but we’ve optimized the amount of intersecting and unpicked space where possible so nothing is slowed down by being spatial unless absolutely necessary (including the actual aggregation of the spatial data). We’re doing the absolute minimum spatially-aware processing, and doing that in a way that early spatial processing does not slow down later non-spatial processing."
  },
  {
    "objectID": "aggregator/spatial_agg.html",
    "href": "aggregator/spatial_agg.html",
    "title": "Spatial aggregation",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)"
  },
  {
    "objectID": "aggregator/spatial_agg.html#overview",
    "href": "aggregator/spatial_agg.html#overview",
    "title": "Spatial aggregation",
    "section": "Overview",
    "text": "Overview\nWe will often have spatial data that we want to aggregate into larger scales. We therefore want a set of functions that allow us to read in data, specify the larger units into which it gets aggregated and the functions to use to do that aggregation. Further, there is clear need to handle grouping, most obviously for scenarios, but we also need to keep theme groupings separated during spatial aggregation steps.\nThere is a standalone spatial aggregator spatial_aggregate, which I demonstrate here, along with multi_aggregate, which wraps both spatial_aggregate and theme_aggregate to allow interleaved aggregaton steps in a standardised format. This document focuses on spatial aggregation, while theme aggregation and interleaved spatial and theme aggregation are demonstrated in separate notebooks, allowing us to dig a little deeper into how each component works.\nWe often will want to only perform a single spatial aggregation (e.g. from gauges to sdl units), but there are instances where that isn’t true- perhaps we want to aggregate from sdl units to states or the basin. Thus, I demonstrate multi-step spatial aggregation, including the situation where aggregation units (polygons) are not nested, as would be the case for sdl units and states, for example. Even if some of the steps in this demonstration aren’t particularly interesting now, they allow us to develop the general process that can accept any set of polygons we want to aggregate into from any other spatial data.\nThis document delves fairly in-depth into capabilities, including things like argument types and how they relate to other functions and permit certain tricks. Not all of these will be used or needed to understand by most users- typically there will be a set of aggregation steps fed to multi_aggregate and that will be that. This sort of simpler setup is shown in the combined aggregation notebook and the full-toolkit runs. But it is helpful to document them for when they are needed. See the syntax notebook for a detailed look at argument construction for various purposes. Here, we use that syntax to demonstrate how the spatial aggregation works and the different ways it can be done."
  },
  {
    "objectID": "aggregator/spatial_agg.html#inputs",
    "href": "aggregator/spatial_agg.html#inputs",
    "title": "Spatial aggregation",
    "section": "Inputs",
    "text": "Inputs\nWe will need to be able to accept inputs at arbitrary aggregation levels (theme, spatial, or temporal). In other words, the spatial aggregation should aggregate any input spatial data into any set of spatial units, whatever that input data represents. The multi_aggregate function runs without spatial info until it reaches a step calling for spatial aggregation, at which point that data must be spatial. Beyond this requirement, multi_aggregate doesn’t care what theme or spatial scale that input data is- e.g. we could give it Objectives already at the Catchment scale, and then use it to move up.\n\nNote: in some cases, the definitions for outcomes along the ‘Objective’ axis are defined spatially; for example, the definition of Specific_objectives might vary between planning units. However, these are the scale at which definitions of outcomes change, not the scale at which those outcomes must be assessed. For example, just because Specific_objectives are differently defined between planning units, we can still scale them up in space from gauge to basin, with no reference to planning unit.\n\nFor this demonstration, we start with gauge-referenced data at the env_obj theme scale."
  },
  {
    "objectID": "aggregator/spatial_agg.html#demonstration-setup",
    "href": "aggregator/spatial_agg.html#demonstration-setup",
    "title": "Spatial aggregation",
    "section": "Demonstration setup",
    "text": "Demonstration setup\nFirst, we need to provide a set of paths to point to the input data, in this case the outputs from the EWR tool for the small demonstration, created by a controller notebook. Spatial units could be any arbitrary polygons, but we use those provided by {werptoolkitr} for consistency, which also provides the spatial locations of the gauges in bom_basin_gauges.\n\nproject_dir &lt;- file.path('more_scenarios')\newr_results &lt;- file.path(project_dir, 'module_output', 'EWR')\n\n\nTheme aggregated inputs\nThe multi_aggregate function can combine theme and spatial aggregation, but because I want this document to demonstrate spatial aggregation, I have split up the process to be clear what is happening. First, we do a theme aggregation to get to the desired Theme level to feed to spatial.\nBefore any aggregation, we need to read the data in and make it spatial (gauge2geo pairs gauge numbers with locations provided in the geopath argument inside prep_ewr_agg. Note that this prep step is wrapped in read_and_agg, reducing user input. I show it here so we can more clearly see what is happening.\n\nsumdat &lt;- prep_ewr_agg(ewr_results, type = 'summary', geopath = bom_basin_gauges)\n\nDefine simple theme aggregation lists to get to env_obj level, assuming that any pass on ewr_code_timing yields a pass for ewr_code and ewr_codes are averaged into env_obj. More complexity for the theme aggregations are shown in the theme notebook.\n\nthemeseq &lt;- list(c('ewr_code_timing', 'ewr_code'),\n               c('ewr_code', \"env_obj\"))\n\nfunseq &lt;- list(c('CompensatingFactor'),\n               c('ArithmeticMean'))\n\nPerform that simple theme aggregation so we have some test data. Since edges are only relevant for theme aggregation, make them in the call. This and everything that follows could be done with interleaved theme and spatial sequences starting with themeseq and funseq fed to multi_aggregate, but here I split them apart to better accentuate the spatial aggregation.\n\nsimpleThemeAgg &lt;- multi_aggregate(dat = sumdat,\n                         causal_edges = make_edges(causal_ewr, themeseq),\n                         groupers = c('scenario', 'gauge'),\n                         aggCols = 'ewr_achieved',\n                         aggsequence = themeseq,\n                         funsequence = funseq)\nsimpleThemeAgg\n\n\n\n  \n\n\n\nThis provides a spatially-referenced (to gauge) theme-aggregated tibble to use to demonstrate spatial aggregation. Note that this has the gauge (spatial unit), but also two groupings that we want to preserve when we spatially aggregate- scenario and the current level of theme grouping, env_obj.\n\n\nSpatial inputs (polygons)\nSpatial aggregation requires polygons to aggregate into, and we want the capability to do that several times. The user can read in any desired polygons with sf::read_sf(path/to/polygon.shp), but here we use those provided in the standard set with {werptoolkitr}. We’ll use SDL units, catchments (from cewo), and the basin to show how the aggregation can have multiple steps with polygons that may not be nested (though care should be taken when that is the case)."
  },
  {
    "objectID": "aggregator/spatial_agg.html#single-aggregation",
    "href": "aggregator/spatial_agg.html#single-aggregation",
    "title": "Spatial aggregation",
    "section": "Single aggregation",
    "text": "Single aggregation\nWe might just want to aggregate spatially once. We can do this simply by passing the input data (anything spatial, in this case simpleThemeAgg), a set of polygons, and providing a length-one funlist. In this simple case, we just use a bare function name, here the custom ArithmeticMean which is just a simple wrapper of mean with na.rm = TRUE. Any function can be passed this way, custom or in-built, provided it has a single argument. More complex situations are given below.\nNote that the aggCols argument is ends_with(original_name) to reference the original name of the column of values- it may have a long name tracking its aggregation history, so we give it the tidyselect ends_with to find the column. More generally, both aggCols and groupers can take any tidyselect syntax or bare names or characters.\n\nobj2poly &lt;- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = 'scenario',\n                             aggCols = ends_with('ewr_achieved'),\n                             funlist = ArithmeticMean,\n                             keepAllPolys = TRUE)\nobj2poly\n\n\n\n  \n\n\n\nNote that that has a horribly long name tracking the aggregation history, and has lost the theme levels- e.g. the different env_objs are no longer there and were all averaged together. The multi_aggregate function automatically handles this preservation, but spatial_aggregate is more general, and does not make any assumptions about the grouping structure of the data. Thus, to keep the env_obj groupings (as we should, otherwise we’re inadvertently theme-aggregating over all of them), we need to add env_obj to the groupers argument.\n\nobj2poly &lt;- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = c('scenario', 'env_obj'),\n                             aggCols = ends_with('ewr_achieved'),\n                             funlist = ArithmeticMean,\n                             keepAllPolys = TRUE)\nobj2poly\n\n\n\n  \n\n\n\nA quick plot shows what we’re dealing with. We’ll simplify the names and choose a subset of the environmental objectives.\nThere are many built-in plotting options in the toolkit, which we will use shortly. First, though a quick ggplot to see what those standardised plot functions start with.\n\n# The name is horrible, so change it.\nobj2poly %&gt;% \n  rename(ewr_achieved = spatial_ArithmeticMean_env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved) %&gt;% \n  filter(grepl('^EF', env_obj)) %&gt;% \nggplot(aes(fill = ewr_achieved)) +\n  geom_sf() + \n  facet_grid(scenario~env_obj) + \n  theme(legend.position = 'bottom')\n\n\n\n\nMoving forward, we’ll use the built-in plotting functions to keep consistent with the rest of the project.\n\nscene_pal &lt;- make_pal(unique(simpleThemeAgg$scenario), palette = 'ggsci::nrc_npg', refvals = 'base', refcols = 'black')\n\n\nobj2poly %&gt;% \n  rename(ewr_achieved = spatial_ArithmeticMean_env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved) %&gt;% \n  filter(grepl('^EF[1-3]', env_obj)) %&gt;% \nplot_outcomes(y_col = 'ewr_achieved',\n                  y_lab = 'Arithmetic Mean',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'env_obj',\n                          facet_row = 'scenario',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down2', 'base', 'up2'))"
  },
  {
    "objectID": "aggregator/spatial_agg.html#multiple-spatial-levels",
    "href": "aggregator/spatial_agg.html#multiple-spatial-levels",
    "title": "Spatial aggregation",
    "section": "Multiple spatial levels",
    "text": "Multiple spatial levels\nThere are a number of polygon layers we might want to aggregated into in addition to SDL units, e.g. resource plan areas, hydrological catchments, or the whole basin. We can aggregate directly into them just as we have here for SDL units. However, we might also want to have several levels of spatial aggregation, which may be nested or nearly so, e.g. from SDL units to the basin, or may be nonnested, e.g. from SDL units to catchments. Typically, this would happen with intervening theme aggregations, as in the interleaved example. The aggregation process for multiple spatial levels is similar whether or not the smaller levels nest into the larger, but more care should be taken (and more explanation is needed) in the nonnested case.\nNote that there is an exception to the ‘vector arguments must be attached to the data’ rule, in that an area column is always created, making it available for things like area-weighted means.\nAggregating from SDL units to cewo valleys requires addressing issues of overlaps among the various polygons. As such, it makes a good test case that catches issues with the intersection of polygons that might not happen with a simpler set of polygons Figure 1 .\n\noverlay_cewo_sdl &lt;- ggplot() +\n  geom_sf(data = sdl_units, aes(fill = SWSDLName), color = NA, alpha = 0.5) +\n  geom_sf(data = cewo_valleys, aes(color = ValleyName), fill = NA) +\n  theme(legend.position = 'none')\n\nvalleys &lt;- ggplot() +\n  geom_sf(data = cewo_valleys, aes(color = ValleyName), fill = 'white') + \n  theme(legend.position = 'none')\n\nsdls &lt;- ggplot() +\n  geom_sf(data = sdl_units, aes(fill = SWSDLName), alpha = 0.5) + \n  theme(legend.position = 'none')\n\nvalleys + sdls + overlay_cewo_sdl\n\n\n\n\nFigure 1: CEWO valleys (coloured lines) and SDL units (coloured fills) alone (a & b) and overlain (c), showing these are not nested, but instead are intersecting polygons.\n\n\n\n\nTo aggregate from one set of polygons into the other, we need to split them up and aggregate in a way that respects area and borders. In other words, if we have a polygon that lays across two of the next level up, we want to only include the bits that overlap into that next level up. Under the hood, we use sf::st_intersection, which splits the polygons to make a new set of nonoverlapping polygons. Then we can use these pieces to aggregate into the higher level. This aggregation should carefully consider area- things like means should be area-weighted, and things like sums, minima, and maxima should be thought about carefully- if the lower ‘from’ data are already sums, for example, an area weighting might make sense to get a proportion of the sum, but this is highly dependent on the particular sequence of aggregation. For other functions like minima and maxima, area-weighting may or may not be appropriate, and so careful attention should be paid to constructing the aggregation sequence and custom functions may be involved.\nThe intersection of sdl_units and cewo_valleys chops up sdl_units so there are unique polygons for each sdl unit - valley combination.\n\njoinpolys &lt;- st_intersection(sdl_units, cewo_valleys)\njoinpolys\n\n\n\n  \n\n\n\nTo better see the many-to-many chopping we get with this particular pair of intersecting shapefiles, we can isolate an SDL unit (Victorian Murray) and see that it contains bits of 8 catchments. Likewise, the Loddon catchment contains bits of 4 SDL units Figure 2 .\n\nnvorig &lt;- ggplot() +\n  geom_sf(data = dplyr::filter(sdl_units, SWSDLName == \"Victorian Murray\"))\n\nnvpostjoin &lt;- ggplot() +\n  geom_sf(data = dplyr::filter(joinpolys, \n                        SWSDLName == \"Victorian Murray\"), \n          aes(fill = ValleyName))\n\navorig &lt;- ggplot() +\n  geom_sf(data = dplyr::filter(cewo_valleys, ValleyName == 'Loddon'))\n\navpostjoin &lt;- ggplot() +\n  geom_sf(data = dplyr::filter(joinpolys, \n                        ValleyName == 'Loddon'), \n          aes(fill = SWSDLName))\n\n(nvorig + nvpostjoin)/(avorig + avpostjoin)\n\n\n\n\nFigure 2: SDL units intersected with CEWO valleys and split to allow aggregation\n\n\n\n\nAs a simple example, we could again do a one-off aggregation from a polygon to another polygon using spatial_aggregate. Here, we could use the obj2poly aggregation into SDL units created above as the starting point.\n\nsimplepolypoly &lt;- spatial_aggregate(dat = obj2poly, \n                 to_geo = cewo_valleys,\n                 groupers = c('scenario', 'env_obj'),\n                 aggCols = 'ewr_achieved',\n                 funlist = mean,\n                 na.rm = TRUE,\n                 keepAllPolys = TRUE,\n                 failmissing = FALSE)\nsimplepolypoly\n\n\n\n  \n\n\n\nAs with all sequential aggregations, this approach works but can be very ad-hoc and easy to forget theme-axis grouping. Instead, just like with interleaved theme and space, we can pass a list giving the aggregation sequence to multi_aggregate."
  },
  {
    "objectID": "aggregator/spatial_agg.html#passing-a-list",
    "href": "aggregator/spatial_agg.html#passing-a-list",
    "title": "Spatial aggregation",
    "section": "Passing a list",
    "text": "Passing a list\nWe can use multi_aggregate to aggregate through a list of polygon sets, here sdl units to cewo valleys to the basin.\nFirst, set up a list of the spatial aggregation steps, defined by the polygon sets and aggregation functions. As usual, multiple aggregation functions can happen at each stage, and they can be characters or named lists with arguments (the weighted mean needs to be a default). Now, we’re taking advantage of the auto-calculated area of each polygon chunk for the weighted mean.\nnote the rlang::quo to make this work with dplyr 1.1 and above as in syntax.\n\nglist &lt;- list(sdl_units = sdl_units, catchment = cewo_valleys, mdb = basin)\n\nfunlist &lt;- list(c('ArithmeticMean', 'LimitingFactor'),\n                rlang::quo(list(wm = ~weighted.mean(., area, na.rm = TRUE))),\n                rlang::quo(list(wm = ~weighted.mean(., area, na.rm = TRUE))))\n\n\nmultispat &lt;- multi_aggregate(dat = simpleThemeAgg,\n                         causal_edges = themeedges,\n                         groupers = c('scenario', 'env_obj'),\n                         aggCols = 'ewr_achieved',\n                         aggsequence = glist,\n                         funsequence = funlist)\n\nBy default, that returns only the final outcome, here the basin scale and with the aggregation history in column names so we are sure of what the values represent Figure 3 . That does not make a very readable legend, but we can rename it manually and tracking the meaning is very important.\n\nmultispat %&gt;% \n  filter(grepl('^NF[1-3]', env_obj)) %&gt;% \nggplot() +\n  geom_sf(aes(fill = mdb_wm_catchment_wm_sdl_units_ArithmeticMean_env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved)) +\n  facet_grid(env_obj~scenario) + theme(legend.position = 'bottom')\n\n\n\n\nFigure 3: Final basin-scale results of aggregating EWR achieved with arithmetic mean to sdl units, then weighted-mean aggregation of SDL units into CEWO valleys and weighted means of CEWO valleys to the Basin.\n\n\n\n\n\nSaveintermediate and namehistory\nWe might only want the final outcome as above, but we also might want all the steps (just as we did for the theme. Making the history in columns keeps the names easier to use, though it uses more memory. I am going to do something I shouldn’t here, and change the column ‘env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved’ to ‘ewr_achieved’. That’s for readability and because here we’re using namehistory = FALSE to track the history in columns. If we were truly doing this analysis, the theme-axis aggregations in simpleThemeAgg would be included in the aggsequence, and so those steps would also be handled with aggsequence columns.\n\nsimpleclean &lt;- simpleThemeAgg %&gt;% \n  rename(ewr_achieved = env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved)\n\nmultispatb &lt;- multi_aggregate(dat = simpleclean,\n                              causal_edges = themeedges,\n                              groupers = c('scenario', 'env_obj'),\n                              aggCols = 'ewr_achieved',\n                              aggsequence = glist,\n                              funsequence = funlist,\n                              saveintermediate = TRUE,\n                              namehistory = FALSE)\n\nThat final list item is scenario * aggfun1*aggfun2*aggfun3*env_obj long, e.g. it has a row for each scenario and env_obj by aggregation function, and the aggregation functions are factorial- both the aggregations at step 1 are subsequently aggregated at each of the following steps.\n\nlength(unique(multispatb$mdb$scenario)) * \nlength(unique(multispatb$mdb$aggfun_1)) * \nlength(unique(multispatb$mdb$aggfun_2)) * \nlength(unique(multispatb$mdb$aggfun_3)) *\n  length(unique(multispatb$mdb$env_obj))\n\n[1] 828\n\nnrow(multispatb$mdb)\n\n[1] 828\n\n\nNow we can more easily analyse the output, but it’s bigger. The three spatial levels plus the input can all be mapped. We’ll cut it down to a single env_obj for clarity and use the comparer standard plots, with a common set of limits. We have to choose one of the aggfun_1 values.\n\nl1 &lt;- multispatb$simpleclean %&gt;% \n  filter(grepl('^NF1', env_obj)) %&gt;% \n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'map',\n                colorgroups = NULL,\n                colorset = 'ewr_achieved',\n                pal_list = list('scico::berlin'),\n                facet_col = 'scenario',\n                facet_row = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down2', 'base', 'up2'),\n                underlay_list = 'basin',\n                setLimits = c(0,1))\n\nl2 &lt;- multispatb$sdl_units %&gt;% \n  filter(grepl('^NF1', env_obj) &\n           aggfun_1 == 'ArithmeticMean') %&gt;% \n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'map',\n                colorgroups = NULL,\n                colorset = 'ewr_achieved',\n                pal_list = list('scico::berlin'),\n                facet_col = 'scenario',\n                facet_row = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down2', 'base', 'up2'),\n                underlay_list = 'basin',\n                setLimits = c(0,1))\n\nl3 &lt;- multispatb$catchment %&gt;% \n  filter(grepl('^NF1', env_obj) &\n           aggfun_1 == 'ArithmeticMean') %&gt;% \n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'map',\n                colorgroups = NULL,\n                colorset = 'ewr_achieved',\n                pal_list = list('scico::berlin'),\n                facet_col = 'scenario',\n                facet_row = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down2', 'base', 'up2'),\n                underlay_list = 'basin',\n                setLimits = c(0,1))\n\nl4 &lt;- multispatb$mdb %&gt;% \n  filter(grepl('^NF1', env_obj) &\n           aggfun_1 == 'ArithmeticMean') %&gt;% \n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'map',\n                colorgroups = NULL,\n                colorset = 'ewr_achieved',\n                pal_list = list('scico::berlin'),\n                facet_col = 'scenario',\n                facet_row = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down2', 'base', 'up2'),\n                underlay_list = 'basin',\n                setLimits = c(0,1))\n\n\nl1 / l2 / l3 / l4 + plot_layout(guides = 'collect')\n\n\n\n\nFigure 4: Each step in spatial aggregation from gauge to SDL unit to catchment to basin, using arithmetic means of EWR achieved\n\n\n\n\n\n\nfailmissing and keepallpolys\nAs above, we might want to ignore some groupers or aggregation columns, and we might want to keep polygons that don’t have data so the maps look better.\n\nmultispatextra &lt;- multi_aggregate(dat = simpleclean,\n                                  causal_edges = themeedges,\n                                  groupers = c('scenario', 'env_obj',\n                                               'doesnotexist'),\n                                  aggCols = c('ewr_achieved', 'notindata'),\n                                  aggsequence = glist,\n                                  funsequence = funlist,\n                                  saveintermediate = TRUE,\n                                  namehistory = FALSE,\n                                  failmissing = FALSE,\n                                  keepAllPolys = TRUE)\n\nWe can see that the missing groupers and aggcols get ignored, while we have retained polygons without data. The default is keepAllPolys = FALSE, so the original multispatb only has relevant catchments while multispatextra has all of them. I’ve taken off the basin underlay from the plots above to make this clearer.\n\nkeepfalse &lt;- multispatb$catchment %&gt;% \n  filter(grepl('^NF1', env_obj) &\n           aggfun_1 == 'ArithmeticMean') %&gt;% \n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'map',\n                colorgroups = NULL,\n                colorset = 'ewr_achieved',\n                pal_list = list('scico::berlin'),\n                facet_col = 'scenario',\n                facet_row = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down2', 'base', 'up2'),\n                setLimits = c(0,1))\n\nkeeptrue &lt;- multispatextra$catchment %&gt;% \n  filter(grepl('^NF1', env_obj) &\n           aggfun_1 == 'ArithmeticMean') %&gt;% \n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'map',\n                colorgroups = NULL,\n                colorset = 'ewr_achieved',\n                pal_list = list('scico::berlin'),\n                facet_col = 'scenario',\n                facet_row = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down2', 'base', 'up2'),\n                setLimits = c(0,1))\n\n\nkeepfalse / keeptrue + plot_layout(guides = 'collect')\n\n\n\n\nIt would be possible to make the same plot by using the catchment polys themselves as underlay, and while the default (underlay_list = 'cewo_valleys' ) would yield white, we also have more control over colour too (not limited to the NA grey).\n\nkeepfalse_underlay &lt;- multispatb$catchment %&gt;% \n  filter(grepl('^NF1', env_obj) &\n           aggfun_1 == 'ArithmeticMean') %&gt;% \n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'map',\n                colorgroups = NULL,\n                colorset = 'ewr_achieved',\n                pal_list = list('scico::berlin'),\n                facet_col = 'scenario',\n                facet_row = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down2', 'base', 'up2'),\n                underlay_list = list(underlay = 'cewo_valleys', \n                                     underlay_pal = 'cornsilk'),\n                setLimits = c(0,1))\nkeepfalse_underlay"
  },
  {
    "objectID": "aggregator/spatial_agg.html#using-multi_aggregate-for-one-aggregation",
    "href": "aggregator/spatial_agg.html#using-multi_aggregate-for-one-aggregation",
    "title": "Spatial aggregation",
    "section": "Using multi_aggregate for one aggregation",
    "text": "Using multi_aggregate for one aggregation\nIf we’re only using one level of spatial aggregation and nothing else, there’s typically no need for the multi_aggregate wrapper. That wrapper does work even for single steps though, and becomes almost essential for multi-step. We do have a bit less flexibility with how we specify arguments- aggsequence and funsequence need to be lists or characters (funsequence cannot be bare function names). Perhaps the biggest issue is that tidyselect in aggCols runs into issues because it gets used again inside multi_aggregate, and so tidyselect in the outer call collides with that. That could all be sorted out, but seems low priority- easier to just enforce characters for aggCols and lists or characters for the sequences.\nTypically we could use namehistory = FALSE to avoid the horrible long name with all the transforms in it, but there’s no way for it to know the previous aggregation history when it’s been done in pieces (as we say in the example above where I dangerously adjusted the name of simpleThemeAgg to make multispatb. A parsing function could handle this, but it’s better to just do it all on one go anyway so this is low priority.\nAs a quick example, here is a single-step spatial aggregation using multi_aggregate, not that we have slightly more restrictive specifications for groupers, aggCols, aggsequence and funsequence.\n\nobj2polyM1 &lt;- multi_aggregate(simpleThemeAgg,\n                            causal_edges = themeedges,\n                            groupers = c('scenario', 'env_obj'), \n                         aggCols = 'ewr_achieved',\n                         aggsequence = list(sdl_units = sdl_units),\n                         funsequence = list(list(am = ~ArithmeticMean(.))),\n                         keepAllPolys = TRUE)\n\nobj2polyM1"
  },
  {
    "objectID": "aggregator/spatial_agg.html#next-steps",
    "href": "aggregator/spatial_agg.html#next-steps",
    "title": "Spatial aggregation",
    "section": "Next steps",
    "text": "Next steps\nThe examples here are designed to dig into capability of the spatial aggregator in fairly high detail. In typical use, we’d follow something more like the interleaved notebook, but this document hopefully provides valuable demonstrations of capability and potential for how each spatial step in that sequence might work and could be set up."
  },
  {
    "objectID": "aggregator/theme_space_agg.html",
    "href": "aggregator/theme_space_agg.html",
    "title": "Aggregate Theme Space",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "aggregator/theme_space_agg.html#overview",
    "href": "aggregator/theme_space_agg.html#overview",
    "title": "Aggregate Theme Space",
    "section": "Overview",
    "text": "Overview\nWe have theme aggregation and spatial aggregation shown separately for in-depth looks at their meaning and capability. Here, we focus on the typical use-case of interleaved aggregation along multiple dimensions. We do not get into all the different options and syntax as they are covered in those other documents.\nAll aggregation in {werptoolkitr} operate on the same core function and use similar principles- take a list of aggregation sequences, and aggregate each step according to a list of aggregation functions. Here, we show how multi_aggregate allos us to interleave the dimensions along which we aggregate, including auto-detecting which dimension we’re operating on (though that is fragile).\nFundamentally, multi_aggregate wraps theme_aggregate and spatial_aggregate with some data organisation and tracking of what the previous level of aggregation was to maintain proper grouping as they alternate. Both of those, in turn, wrap general_aggregate with some data arrangement specific to the dimension they aggregate along, such as stripping and re-adding geometry.\nFor inputs, multi_aggregate expects the incoming data to be in memory and geographic, and prefers (but does not require) the edges defining theme relationships to be already calculated. There is also a wrapper read_and_agg that takes paths as arguments and does the read-in of the data internally, finds the edges, and then runs multi_aggregate. This is often a useful approach, allowing parallelisation, better memory management, and it is far easier to use paths in a config file of arguments."
  },
  {
    "objectID": "aggregator/theme_space_agg.html#demonstration-setup",
    "href": "aggregator/theme_space_agg.html#demonstration-setup",
    "title": "Aggregate Theme Space",
    "section": "Demonstration setup",
    "text": "Demonstration setup\nFirst, we need to provide a set of paths to point to the input data, in this case the outputs from the EWR tool for the small demonstration, created by a controller notebook. Spatial units could be any arbitrary polygons, but we use those provided by {werptoolkitr} for consistency, which also provides the spatial locations of the gauges in bom_basin_gauges.\n\nproject_dir &lt;- file.path('more_scenarios')\newr_results &lt;- file.path(project_dir, 'module_output', 'EWR')\n\nWhere do we want the outputs to go? The multi_aggregate function we focus on here takes R objects as inputs and returns a dataframe or a list back to the session. But in practice, we will wrap that with read_and_agg, which takes paths as inputs and can both return R objects to the session returnList = TRUE and save to a file if savepath = \"path/to/outfile\" . The aggregator_output directory is not broken into module subdirectories, since it is possible we will want to aggregate across modules at the highest levels.\n\nout_path &lt;- file.path(project_dir, 'aggregator_output')\n\nScenario information\nThis will be attached to metadata, typically. For now, I’m just using it for diagnostic plots and the demonstration data is simple, so make it here.\n\nmultipliers &lt;- c(1.1, 1.5, 2, 3, 4)\n\nscenemults &lt;- c(1/rev(multipliers), 1, multipliers)\n\nscenenames &lt;- c(paste0('down', as.character(rev(multipliers))), \n                'base', \n                paste0('up', as.character(multipliers))) |&gt; \n  stringr::str_replace('\\\\.', '_')\n\n\nscenarios &lt;- tibble::tibble(scenario = scenenames, delta = scenemults)\n\nscene_pal &lt;- make_pal(unique(scenarios$scenario), palette = 'ggsci::nrc_npg', refvals = 'base', refcols = 'black')"
  },
  {
    "objectID": "aggregator/theme_space_agg.html#data-prep",
    "href": "aggregator/theme_space_agg.html#data-prep",
    "title": "Aggregate Theme Space",
    "section": "Data prep",
    "text": "Data prep\nTo make the actual multi-aggregate loop general and only do one thing, dataprep needs to happen first (e.g. we don’t want to do EWR-specific dataprep on econ data). That said, we can use read_and_agg, which takes paths and the aggregation lists and runs the dataprep and aggregation functions. At present, the EWR tool is the only module, so we read it in and prep the data, including making it geographic with gauge locations.\n\newrdata &lt;- prep_ewr_agg(ewr_results, type = 'summary', geopath = bom_basin_gauges)\n\nWe use causal_ewrs for causal relationships and spatial layers provided by {werptoolkitr} to define the aggregation units."
  },
  {
    "objectID": "aggregator/theme_space_agg.html#setup",
    "href": "aggregator/theme_space_agg.html#setup",
    "title": "Aggregate Theme Space",
    "section": "Setup",
    "text": "Setup\nFirst, we specify a simple interleaved aggregation sequence with only one aggregation function applied per step for simplicity. Note that theme-axis aggregation steps are specified with a character vector c('level_from', 'level_to'), while spatial aggregation steps are specified with an sf object (polygons). Allowing specification of spatial steps by character instead of object is high on the priority list. Here, we specify the aggregation function sequence with a mixture of character names for functions and list-defined anonymous functions, as discussed in the syntax notebook.\nNaming the list by target makes tracking and interpretation much easier, and is highly recommended.\nSpatial aggregation should almost always be area-weighted after the data is in polygons (see spatial notebook), though there are some aggregation functions where it doesn’t matter (e.g. max). All polygon data has an area column calculated automatically for this reason. The first aggregation into polygons doesn’t need to be area-weighted, because the thing being aggregated (typically at the gauge scale) doesn’t have area. After that, all data is in polygons and so has area. It is likely safest to always use weighted functions like weighted.mean though, since they default to even weights if none are given.\nnote the rlang::quo to make this work with dplyr 1.1 and above as in syntax.\n\naggseq &lt;- list(ewr_code = c('ewr_code_timing', 'ewr_code'),\n               env_obj =  c('ewr_code', \"env_obj\"),\n               sdl_units = sdl_units,\n               Specific_goal = c('env_obj', \"Specific_goal\"),\n               catchment = cewo_valleys,\n               Objective = c('Specific_goal', 'Objective'),\n               mdb = basin,\n               target_5_year_2024 = c('Objective', 'target_5_year_2024'))\n\n\nfunseq &lt;- list(c('CompensatingFactor'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               rlang::quo(list(wm = ~weighted.mean(., w = area, \n                                        na.rm = TRUE))),\n               c('ArithmeticMean'),\n               \n               rlang::quo(list(wm = ~weighted.mean(., w = area, \n                                    na.rm = TRUE))),\n               c('ArithmeticMean'))\n\nThe multi_aggregate function needs the edges, so calculate them just for the theme sequence (dropping the spatial steps). This can also happen automatically in multi_aggregate and, most importantly, read_and_agg, allowing us to run the code only by specifying parameters and without this sort of intermediate processing.\n\nthemeseq &lt;- aggseq[purrr::map_lgl(aggseq, is.character)]\newr_edges &lt;- make_edges(dflist = causal_ewr, \n                         fromtos = themeseq)"
  },
  {
    "objectID": "aggregator/theme_space_agg.html#aggregate",
    "href": "aggregator/theme_space_agg.html#aggregate",
    "title": "Aggregate Theme Space",
    "section": "Aggregate",
    "text": "Aggregate\nNow we do the aggregation. Note that we have been very aggressive in handling spatial processing and so while spatial processing is slow, we minimize it as much as possible internally.\nReturn only final\n\ntsagg &lt;- multi_aggregate(dat = ewrdata,\n                         causal_edges = ewr_edges,\n                         groupers = 'scenario',\n                         aggCols = 'ewr_achieved',\n                         aggsequence = aggseq,\n                         funsequence = funseq)\n\nThat saves only the final outcome, which is far cheaper for memory, but doesn’t say how we got that answer.\n\ntsagg\n\n\n\n  \n\n\n\nReturn all steps\nMost often, we’ll want to save the list of outcomes at each step- it allows us to see how we got the final outcome, and it’s likely we’re interested in the outcomes at more than one step anyway. As in the theme notebook, we do this with saveintermediate = TRUE. We also set namehistory = FALSE to put the aggregation tracking in columns instead of names for ease of handling the output.\n\nallagg &lt;- multi_aggregate(dat = ewrdata,\n                         causal_edges = ewr_edges,\n                         groupers = 'scenario',\n                         aggCols = 'ewr_achieved',\n                         aggsequence = aggseq,\n                         funsequence = funseq,\n                         saveintermediate = TRUE,\n                         namehistory = FALSE,\n                         keepAllPolys = FALSE)\n\nNow, we’ll inspect each step, both as dataframes and maps. There are many other ways of plotting the outcome data available in the comparer. The goal here is simply to visualize what happens at each step along the way, so we make some quick maps.\nSheet 1- raw data from ewr\nThis is just the input data, so we don’t bother plotting it.\n\nallagg$ewr_code_timing\n\n\n\n  \n\n\n\nSheet 2- ewr_code\nThe first aggregated level is in sheet 2, and has the code_timings aggregated to ewr_code.\n\nallagg$ewr_code\n\n\n\n  \n\n\n\nThere are many EWR codes, so just pick three haphazardly (LF1, BF1, and CF) and plot to see that this data is at the gauge scale.\n\nallagg$ewr_code |&gt;\n  dplyr::filter(ewr_code %in% c('LF1', 'BF1', 'CF')) %&gt;%\n  dplyr::left_join(scenarios) %&gt;% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'ewr_code',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down2', 'base', 'up2'),\n                          underlay_list = list(underlay = sdl_units, \n                                               underlay_pal = 'cornsilk'))\n\n\n\n\nSheet 3- env_obj\nSheet three has now been aggregated to the env_obj on the theme scale, still gauges spatially.\n\nallagg$env_obj\n\n\n\n  \n\n\n\nAgain choosing three of the first codes, we see this is still gauged.\n\nallagg$env_obj |&gt;\n  dplyr::filter(env_obj %in% c('EF1', 'WB1', 'NF1')) %&gt;%\n  dplyr::left_join(scenarios) %&gt;% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'env_obj',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down2', 'base', 'up2'),\n                          underlay_list = list(underlay = sdl_units, \n                                               underlay_pal = 'cornsilk'))\n\n\n\n\nSheet 4- sdl_units\nThe fourth step is a spatial aggregation of env_obj theme-level data into sdl_units. This stays at the env_obj theme scale but aggregates the gauges into sdl units.\n\nallagg$sdl_units\n\n\n\n  \n\n\n\nNow we have aggregated the data above into sdl polygons.\n\nallagg$sdl_units |&gt;\n  dplyr::filter(env_obj %in% c('EF1', 'WB1', 'NF1')) %&gt;%\n  dplyr::left_join(scenarios) %&gt;% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'env_obj',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down2', 'base', 'up2'),\n                          underlay_list = list(underlay = sdl_units, \n                                               underlay_pal = 'grey90'))\n\n\n\n\nSheet 5- Specific goal\nSheet 5 is back to the theme axis, aggregating env_obj to Specific goal, remaining in SDL units.\n\nallagg$Specific_goal\n\n\n\n  \n\n\n\nUsing fct_reorder. this is where info about the scenarios would come in handy as reorder cols.\n\nallagg$Specific_goal |&gt;\n  dplyr::filter(Specific_goal %in% c('All recorded fish species', \n                                     \"Spoonbills\", \n                                     \"Decompsition\"))  %&gt;% \n  dplyr::left_join(scenarios) %&gt;% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'Specific_goal',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down2', 'base', 'up2'),\n                          underlay_list = list(underlay = sdl_units, \n                                               underlay_pal = 'grey90'))\n\n\n\n\nSheet 6- Catchment\nSheet 6 (aggregation step 5) remains at the Specific goal theme scale, and aggregates spatially from SDL unit into catchment (cewo_valleys). This is a bit of a contrived aggregation, since these are at similar spatial scales but represent different spatial groupings, but it is a good test of spatial aggregation of nonnested spatial units. This is explored in more detail in the spatial notebook.\n\nallagg$catchment\n\n\n\n  \n\n\n\nNow we can see that the aggregation has occurred into a different set of polygons. In practice, this we would likely aggregate gauge-scale data into either sdl_units or cewo_valleys, depending on the target, but this demonstrates the capability and flexibility of the multistage aggregations.\n\nallagg$catchment |&gt;\n  dplyr::filter(Specific_goal %in% c('All recorded fish species', \n                                     \"Spoonbills\", \n                                     \"Decompsition\"))  %&gt;% \n  dplyr::left_join(scenarios) %&gt;% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'Specific_goal',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down2', 'base', 'up2'),\n                          underlay_list = list(underlay = cewo_valleys, \n                                               underlay_pal = 'grey90'))\n\n\n\n\nSheet 7- Objective\nWe are now back to aggregation along the theme axis (from Specific goal to Objective), remaining in cewo_valleys.\n\nallagg$Objective\n\n\n\n  \n\n\n\nWe see that these are still in the catchments, but now the values are different Objectives.\n\nallagg$Objective |&gt;\n  dplyr::filter(Objective %in% c('No loss of native fish species',\n                                 \"Increase total waterbird abundance across all functional groups\", \n                                     \"Support instream & floodplain productivity\"))  %&gt;% \n  dplyr::left_join(scenarios) %&gt;% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'Objective',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down2', 'base', 'up2'),\n                          underlay_list = list(underlay = sdl_units, \n                                               underlay_pal = 'grey90'))\n\n\n\n\nSheet 8- Basin\nThis step is a spatial aggregation to the basin scale, with theme remaining at the Objective level. The scaling to the basin is area-weighted, so larger catchments count more toward the basin-scale outcome. Recognize that for this situation with data in only a subset of the basin, aggregation to the whole basin is fraught and is likely biased by missing data.\n\nallagg$mdb\n\n\n\n  \n\n\n\nWe drop the underlay on the plots since we’re now plotting the whole basin\n\nallagg$mdb |&gt;\n  dplyr::filter(Objective %in% c('No loss of native fish species',\n                                 \"Increase total waterbird abundance across all functional groups\", \n                                     \"Support instream & floodplain productivity\"))  %&gt;% \n  dplyr::left_join(scenarios) %&gt;% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'Objective',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down2', 'base', 'up2'))\n\n\n\n\nSheet 9- 5-year targets\nFinally, we aggregate along the theme axis to 5-year targets, remaining at the basin-scale spatialy\n\nallagg$target_5_year_2024\n\n\n\n  \n\n\n\nAnd we’re still at the basin, just plotting different outcomes.\n\nallagg$target_5_year_2024 |&gt;\n  dplyr::filter(target_5_year_2024 %in% c('All known species detected annually', \n                                          \"Establish baseline data on the number and distribution of wetlands with breeding activity of flow-dependant frog species\",\n                                          \"Rates of fall does not exceed the 5th percentile of modelled natural rates during regulated water deliveries\"))  %&gt;% \n  dplyr::left_join(scenarios) %&gt;% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'target_5_year_2024',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down2', 'base', 'up2'))"
  },
  {
    "objectID": "aggregator/theme_space_agg.html#simple-inputs-and-saving",
    "href": "aggregator/theme_space_agg.html#simple-inputs-and-saving",
    "title": "Aggregate Theme Space",
    "section": "Simple inputs and saving",
    "text": "Simple inputs and saving\nIn practice, we often won’t call multi_aggregate directly, but will use read_and_agg to run multi_aggregate, since it automates data read-in and processing and saving. To do the same analysis as above but using read_and_agg, we give it the path to the data instead of the data itself. Note also that the geopath and causalpath arguments can be objects or paths; we use objects here because they are provided with the {werptoolkitr} package. We use returnList to return the output to the active session, and savepath to save an .rds file to out_path (but only if we’re rebuilding data).\nNote- to readRDS sf objects produced here, we need to have sf loaded in the reading script.\n\nif (params$REBUILD_DATA) {savep &lt;- file.path(out_path)} else {savep &lt;- NULL}\n\nts_from_raa &lt;- read_and_agg(datpath = ewr_results, \n                            type = 'summary',\n                 geopath = bom_basin_gauges,\n                 causalpath = causal_ewr,\n                 groupers = c('scenario'),\n                 aggCols = 'ewr_achieved',\n                 aggsequence = aggseq,\n                 funsequence = funseq,\n                 namehistory = FALSE,\n                 saveintermediate = TRUE,\n                 returnList = TRUE,\n                 savepath = savep)\n\nWe can see that that produces the same list as tsagg\n\nnames(tsagg)\n\n [1] \"scenario\"                                                                                                                                                                                            \n [2] \"polyID\"                                                                                                                                                                                              \n [3] \"target_5_year_2024\"                                                                                                                                                                                  \n [4] \"target_5_year_2024_ArithmeticMean_mdb_wm_Objective_ArithmeticMean_catchment_wm_Specific_goal_ArithmeticMean_sdl_units_ArithmeticMean_env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved\"\n [5] \"OBJECTID\"                                                                                                                                                                                            \n [6] \"DDIV_NAME\"                                                                                                                                                                                           \n [7] \"AREA_HA\"                                                                                                                                                                                             \n [8] \"SHAPE_AREA\"                                                                                                                                                                                          \n [9] \"SHAPE_LEN\"                                                                                                                                                                                           \n[10] \"geometry\"                                                                                                                                                                                            \n\nnames(ts_from_raa)\n\n[1] \"ewr_code_timing\"    \"ewr_code\"           \"env_obj\"           \n[4] \"sdl_units\"          \"Specific_goal\"      \"catchment\"         \n[7] \"Objective\"          \"mdb\"                \"target_5_year_2024\""
  },
  {
    "objectID": "aggregator/theme_space_agg.html#parallelization",
    "href": "aggregator/theme_space_agg.html#parallelization",
    "title": "Aggregate Theme Space",
    "section": "Parallelization",
    "text": "Parallelization\nThe theme notebook, demonstrates potential parallelisation over gauges and scenarios from read-in onwards, which will likely be very useful once we’re dealing with real scenarios. Once spatial aggregation occurs, parallelisation over gauges doesn’t work, since their outcomes need to be aggregated together. That means in general, we are likely to run parallelisation over just scenarios, although there is certainly scope for clever chunking to allow parallelisation over space and time as well if that becomes necessary.\n\nNote: if we want to saveintermediate = TRUE, which we often do, we can’t .combine = bind_rows, but would need to save a list of lists and then bind_rows at each list-level post-hoc with purrr. I have not established that as a function yet, but it is high priority as we settle on data formats and batching workflows.\n\n\nlibrary(foreach)\nlibrary(doFuture)\n\nregisterDoFuture()\nplan(multisession)\n# plan(sequential) # debug\n\n# get these from elsewhere, the whole point is to not read everything in. Should be able to extract from the paths (or the scenario metadata - better)\nallscenes &lt;- list.files(ewr_results, recursive = TRUE) %&gt;% \n  dirname() %&gt;% \n  dirname() %&gt;% \n  unique()\n\n# I think no longer needed now we have a package\n# passfuns &lt;- unique(unlist(funseq))\n# passfuns &lt;- unlist(passfuns[purrr::map_lgl(passfuns, is.character)])\n\nparAgg &lt;- foreach(s = allscenes,\n                       .combine = dplyr::bind_rows) %dopar% {\n    \n    read_and_agg(datpath = ewr_results, type = 'summary',\n                 geopath = bom_basin_gauges,\n                 causalpath = causal_ewr,\n                 groupers = c('scenario'),\n                 aggCols = 'ewr_achieved',\n                 aggsequence = aggseq,\n                 funsequence = funseq,\n                 namehistory = TRUE,\n                 saveintermediate = FALSE,\n                 scenariofilter = s)\n  }\n\nparAgg\n\n\n\n  \n\n\n\nThat output is the same as tsagg, but now it’s been read-in and processed in parallel over scenarios. As in the theme situation, this toy example is slower, but should yield large speedups for larger jobs."
  },
  {
    "objectID": "aggregator/theme_space_agg.html#next-steps",
    "href": "aggregator/theme_space_agg.html#next-steps",
    "title": "Aggregate Theme Space",
    "section": "Next steps",
    "text": "Next steps\nWe can now proceed to the comparer. We could use the tsagg list directly if we want to do all this processing in a single session, but what is more likely is that we’ll read the data produced by read_and_agg and saved at out_path to read in to the comparer, so we do not have to re-run the aggregator every time we use the comparer."
  },
  {
    "objectID": "causal_networks/causal_overview.html",
    "href": "causal_networks/causal_overview.html",
    "title": "Causal networks",
    "section": "",
    "text": "Causal networks are models that describe the relationships between climate, adaptation options and outcomes for environmental, cultural, social, and economic values and assets (that is, the elements of the quadruple bottom line) (adapted from (Peeters et al. 2022)). They include the links which form the basis of the Driver-indicator-response models (e.g. EWR tool: hydrology to indicators), in addition to the links that connect indicators to objectives that are defined for key values.\nCausal networks can show many relationships and outcomes. To illustrate here, we show the links between EWRs (hydrologic indicators), proximate environmental objectives, larger-scale objectives, and finally broad-based ecological groupings.\n\n\n\n\n\n\n\n\n\nThe causal networks for environmental, cultural, social, or economic values are derived from specific documents detailing the relationships between indicators and objectives for those bottom-line elements. For the present example, we focus on environmental values in two catchments of the Murray-Darling Basin and so we draw the causal relationships from the Long Term Water Plan (LTWP). The LTWP reports the environmental water requirements (EWRs) for spatially explicit objectives to be achieved. These objectives are aimed to support the completion of all elements of a lifecycle of an organism or group of organisms (taxonomic or spatial) (LTWP doc). Objectives are described for five target groups and are associated with long-term targets (5, 10, and 20 year) of the LTWP's management strategies. The links from EWRs to environmental objectives to long-term targets are captured in the causal networks to enable assessment of outcomes in direct equivalence to the LTWP's management strategies. \n\n\n\nThe causal networks enables 1) visual representation of the complex inter-relationships between scenario inputs (hydrographs) and river-related outcomes and 2) assessment of outcomes aggregated along the thematic dimension. The former, aids transparency, elucidating the targets and causal relationships behind the Driver-indicator-response models and is a useful device for communication about the toolkit and its outputs. The latter, allows outcomes to be assessed for individual (or sets of) environmental objectives, target groups, long-term targets, or at the level of the quadruple bottom line to identify synergies and trade-offs among values. \n\n\n\nThe toolkit provides various functions for creation and manipulation of causal networks, depending on what needs to be plotted or investigated, and the causal plots page provides some examples of various ways they can be visualised with the toolkit."
  },
  {
    "objectID": "causal_networks/causal_overview.html#overview",
    "href": "causal_networks/causal_overview.html#overview",
    "title": "Causal networks",
    "section": "",
    "text": "Causal networks are models that describe the relationships between climate, adaptation options and outcomes for environmental, cultural, social, and economic values and assets (that is, the elements of the quadruple bottom line) (adapted from (Peeters et al. 2022)). They include the links which form the basis of the Driver-indicator-response models (e.g. EWR tool: hydrology to indicators), in addition to the links that connect indicators to objectives that are defined for key values.\nCausal networks can show many relationships and outcomes. To illustrate here, we show the links between EWRs (hydrologic indicators), proximate environmental objectives, larger-scale objectives, and finally broad-based ecological groupings.\n\n\n\n\n\n\n\n\n\nThe causal networks for environmental, cultural, social, or economic values are derived from specific documents detailing the relationships between indicators and objectives for those bottom-line elements. For the present example, we focus on environmental values in two catchments of the Murray-Darling Basin and so we draw the causal relationships from the Long Term Water Plan (LTWP). The LTWP reports the environmental water requirements (EWRs) for spatially explicit objectives to be achieved. These objectives are aimed to support the completion of all elements of a lifecycle of an organism or group of organisms (taxonomic or spatial) (LTWP doc). Objectives are described for five target groups and are associated with long-term targets (5, 10, and 20 year) of the LTWP's management strategies. The links from EWRs to environmental objectives to long-term targets are captured in the causal networks to enable assessment of outcomes in direct equivalence to the LTWP's management strategies. \n\n\n\nThe causal networks enables 1) visual representation of the complex inter-relationships between scenario inputs (hydrographs) and river-related outcomes and 2) assessment of outcomes aggregated along the thematic dimension. The former, aids transparency, elucidating the targets and causal relationships behind the Driver-indicator-response models and is a useful device for communication about the toolkit and its outputs. The latter, allows outcomes to be assessed for individual (or sets of) environmental objectives, target groups, long-term targets, or at the level of the quadruple bottom line to identify synergies and trade-offs among values. \n\n\n\nThe toolkit provides various functions for creation and manipulation of causal networks, depending on what needs to be plotted or investigated, and the causal plots page provides some examples of various ways they can be visualised with the toolkit."
  },
  {
    "objectID": "comparer/bar_plots.html",
    "href": "comparer/bar_plots.html",
    "title": "Bar plots",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)"
  },
  {
    "objectID": "comparer/bar_plots.html#scenario-information",
    "href": "comparer/bar_plots.html#scenario-information",
    "title": "Bar plots",
    "section": "Scenario information",
    "text": "Scenario information\nThis should be auto-acquired from the dirs above. But while its format is still up in the air, I’m leaving it a bit more user-editable.\n\nscenarios &lt;- jsonlite::read_json(file.path(hydro_dir, \n                                           'scenario_metadata.json')) |&gt; \n  tibble::as_tibble() |&gt; \n  tidyr::unnest(cols = everything())"
  },
  {
    "objectID": "comparer/bar_plots.html#subset-for-demo",
    "href": "comparer/bar_plots.html#subset-for-demo",
    "title": "Bar plots",
    "section": "Subset for demo",
    "text": "Subset for demo\nWe have a lot of hydrographs, so for this demonstration, we will often use a subset.\n\ngauges_to_plot &lt;- c('412002', '419001', '422028', '421001')"
  },
  {
    "objectID": "comparer/bar_plots.html#choosing-example-data",
    "href": "comparer/bar_plots.html#choosing-example-data",
    "title": "Bar plots",
    "section": "Choosing example data",
    "text": "Choosing example data\nFirst, we read in the aggregated data and make a simple demonstration bar plot. There is example data provided by the toolkit (agg_theme_space and agg_theme_space_colsequence), but to continue with the demonstration, we will use the aggregations created here in the interleaved aggregation notebook.\nNote- to readRDS sf objects, we need to have sf loaded.\n\nagged_data &lt;- readRDS(file.path(agg_dir, 'summary_aggregated.rds'))\n\nThat has all the steps in the aggregation, so we’ll choose one (the Objective theme scale at the basin spatial scale, agged_data$mdb) for the first set of plots and another (agged_data$sdl_units at the SDL unit scale and env_obj theme scale for the second set of plots. This finer scale lets us look at complicating factors like multiple spatial units and grouping outcomes.\nTo make these examples more easily, we create some slightly simpler dataframes here for those examples, but this isn’t really necessary- small data manipulations are easily piped in to plot_outcomes. The basin-scale needs a bit of cleanup because Objectives (and many of the other categories other than codes, e.g. yearly targets) are really long. We could fold them in the facet labels with ggplot2::label_wrap_gen(), but they’re so long it blocks out the plots. Ideally, we would use descriptive short names for each, but that’s a large manual job to assign them. For this demonstration, I’ll just cut them off and make them unique, but we need a better solution. The SDL units data is given a grouping column that puts the many env_obj variables in groups defined by their first two letters, e.g. EF for Ecosystem Function.\nIf we had used multiple aggregation functions at any step, we should filter down to the one we want here, but we only used one for this example.\n\n# make the super long names shorter but less useful.\nbasin_to_plot &lt;- agged_data$mdb %&gt;% \n  dplyr::filter(!is.na(Objective)) %&gt;% \n  dplyr::mutate(Objective = stringr::str_trunc(Objective, 15)) %&gt;% \n  dplyr::group_by(scenario, Objective) %&gt;% \n  dplyr::mutate(id = as.character(row_number())) %&gt;% \n  dplyr::ungroup() %&gt;% \n  dplyr::mutate(Objective = stringr::str_c(Objective, '_', id)) %&gt;% \n  dplyr::select(-id)\n\n# Create a grouping variable\nobj_sdl_to_plot &lt;- agged_data$sdl_units |&gt;\n  dplyr::mutate(env_group = stringr::str_extract(env_obj, '^[A-Z]+')) |&gt;\n  dplyr::filter(!is.na(env_group)) |&gt;\n  dplyr::arrange(env_group, env_obj)"
  },
  {
    "objectID": "comparer/bar_plots.html#scenario-fills",
    "href": "comparer/bar_plots.html#scenario-fills",
    "title": "Bar plots",
    "section": "Scenario fills",
    "text": "Scenario fills\n\nBasin scale\nWe can make plots looking at how scenarios differ for each of the outcome categories. This uses facet_wrapper to just wrap the single facet axis, looking at the basin scale and Objectives first. If we had more than one spatial unit, we would need to either filter to a target unit or facet by them. As with {ggplot2} itself, we tend to use facet_wrap for single-variable facetting and facet_row and facet_grid for specifying rows and columns, though here we use facet_row and facet_col to feed facet_grid. Plots at the basin scale are the simplest because we don’t have to worry about different bars for different spatial units.\nThe colorset argument is the column that determines color, while the pal_list defines those colors, here as a named colors object, but as we see below it can also be palette names. The sceneorder argument lets us explicitly set the order of the scenarios. This is typically easiest to have a Factor object with the scenarios and their orders, as here. But we can also just use a character vector (demonstrated later).\n\nplot_outcomes(basin_to_plot, \n                 y_col = 'ewr_achieved', \n                 facet_wrapper = 'Objective', \n                 colorset = 'scenario',\n                 pal_list = scene_pal,\n                 sceneorder = sceneorder) +\n  theme(axis.text.x = element_blank())\n\n\n\n\nWe retain the axis names as-is from the incoming dataframe, as they provide the true meaning of each value. But we can change them, either inside the plot_outcomes function or post-hoc. We can also set the sceneorder with a character vector if that’s easier than setting up a Factor.\n\nplot_outcomes(basin_to_plot, \n                 y_col = 'ewr_achieved', \n              y_lab = 'Proportion Objectives\\nAchieved',\n              x_lab = 'Scenario',\n              color_lab = 'Scenario',\n                 facet_wrapper = 'Objective', \n                 colorset = 'scenario',\n                 pal_list = scene_pal,\n                 sceneorder = c('down2', 'down1_5', 'down1_25', 'down1_1', 'base', 'up1_1', 'up1_25', 'up1_5', 'up2'))+\n  theme(axis.text.x = element_blank())\n\n\n\n\nBecause these are just ggplot objects, we can also change the labels outside the function, which can be very useful for checking that each axis is in fact what we think it is before giving it clean labels.\n\nscenebar &lt;- plot_outcomes(basin_to_plot, \n                 y_col = 'ewr_achieved',\n                 facet_wrapper = 'Objective', \n                 colorset = 'scenario',\n                 pal_list = scene_pal,\n                 sceneorder = sceneorder)\n\nscenebar + \n  labs(x = NULL, y = 'Proportion\\nObjectives Achieved') +\n  theme(axis.text.x = element_blank())\n\n\n\n\nAnother approach is to put the outcomes on the x-axis, and color by scenario. This requires using the special scene_pal argument currently instead of pal_list. This is a bit of a historical holdover and will be made more general.\n\nplot_outcomes(basin_to_plot, \n                 y_col = 'ewr_achieved',\n                 x_col = 'Objective',\n                 colorset = 'scenario',\n                 scene_pal = scene_pal,\n                 sceneorder = sceneorder)\n\n\n\n\nUsing dodged bars can allow clearer comparisons, particularly accentuating the variation in sensitivity of the different outcomes to the scenarios, though there are a lot of bars to try to read here.\n\nplot_outcomes(basin_to_plot, \n                 y_col = 'ewr_achieved',\n                 x_col = 'Objective',\n                 colorset = 'scenario',\n                 scene_pal = scene_pal,\n                 sceneorder = sceneorder,\n              position = 'dodge')\n\n\n\n\n\n\nSDL units\nWe can use the aggregation step of env_obj and SDL units to demonstrate plotting that not only addresses the outcomes for scenarios, but how they differ across space. I’ll often use subsets of the env_obj codes here to keep the number of plots visible.\nFirst, we look at how the different scenarios perform for the Ecosystem Function objectives in each SDL unit\n\nobj_sdl_to_plot %&gt;% \n  filter(grepl('^EF', env_obj)) %&gt;% \n  plot_outcomes(y_col = 'ewr_achieved', \n                facet_col = 'env_obj',\n                facet_row = \"SWSDLName\",\n                colorset = 'scenario',\n                pal_list = scene_pal,\n                sceneorder = sceneorder)+\n  theme(axis.text.x = element_blank())\n\n\n\n\nWe address a few ways to handle groups of outcome variables, one of the simplest is to simply facet these plots by those groups, with all the outcomes in the group getting their own bars. This puts the objectives on x and colors by scenario, with the groups accentuated by facets. These can be stacked (position = 'stack'- the default) or dodged (demonstrated here).\n\ndodgefacet &lt;- obj_sdl_to_plot |&gt;\n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'env_obj',\n                          colorset = 'scenario',\n                          facet_row = 'SWSDLName',\n                          facet_col = 'env_group',\n                          scales = 'free_x',\n                          scene_pal = scene_pal,\n                          sceneorder = sceneorder,\n                          position = 'dodge')\n\ndodgefacet + theme(legend.position = 'bottom') + \n  labs(x = 'Environmental Objective')"
  },
  {
    "objectID": "comparer/bar_plots.html#grouped-colors",
    "href": "comparer/bar_plots.html#grouped-colors",
    "title": "Bar plots",
    "section": "Grouped colors",
    "text": "Grouped colors\nWe have the ability to assign different color palettes to different sets of outcomes, yielding what is essentially another axis on which we can plot information. We use this same ability across a number of plot types, particularly causal networks. For example, we might categorize the env_obj outcomes into the larger scale groups (e.g. ‘NF’, ‘EF’, etc). We can then assign each of these a separate palette, and so the individual env_objs get different colors chosen from different palettes.\nAchieving this requires specifying two columns- the colorset, as above, is the column that determines color. The colorgroups column specifies the groupings of those colorset values, and so what palette to use. Thus, the pal_list needs to be either length 1 (everything gets the same palette) or length(unique(data$colorgroups)). Note also that the colorset values must be unique to colorgroups- this cannot be a one-to-many mapping because each colorset value must get a color from a single palette defined by the colorgroup it is in.\nWe demonstrate with env_obj variables mapped to larger environmental groups, making it easier to see at a glance the sorts of environmental objectives that are more or less affected, while also allowing views of the individual environmental objectives. Here we use facet_col and facet_row to ensure the SDL units don’t wrap around. We made the env_groups column when we chose the data initially.\n\n# Create a palette list\nenv_pals = list(EF = 'grDevices::Purp',\n                NF = 'grDevices::Mint',\n                NV = 'grDevices::Burg',\n                OS = 'grDevices::Blues',\n                WB = 'grDevices::Peach')\n\n# need to facet by space sdl unit and give it the colorgroup argument to take multiple palettes\nsdl_stack &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(y_col = 'ewr_achieved',\n                colorgroups = 'env_group',\n                colorset = 'env_obj',\n                pal_list = env_pals,\n                facet_col = 'SWSDLName',\n                facet_row = '.',\n                sceneorder = sceneorder)\nsdl_stack +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\n\n\nAdding facetting by those groups can make that easier to read if the goal is to focus on changes within groups, but more plots.\n\nobj_sdl_to_plot |&gt;\n  plot_outcomes(y_col = 'ewr_achieved',\n                colorgroups = 'env_group',\n                colorset = 'env_obj',\n                pal_list = env_pals,\n                facet_col = 'SWSDLName',\n                facet_row = 'env_group',\n                sceneorder = sceneorder)+\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\n\n\nWe could also split those bars sideways instead of stack them, but that likely makes more sense if there are fewer categories than here. We again use position = 'dodge', but now we don’t need to sum because we’re stacking each row already. I’ve flipped the facetting and taken advantage of the fact that these are just ggplot objects to remove the legend, making it very slightly easier to read (but harder to interpret). This gets very crowded with the full set of scenarios, so we can use the scenariofilter argument to cut it to just a few (here, base and multiply and divide by 1.5).\n\nobj_sdl_to_plot |&gt;\n  plot_outcomes(y_col = 'ewr_achieved',\n                colorgroups = 'env_group',\n                colorset = 'env_obj',\n                pal_list = env_pals,\n                facet_col = 'env_group',\n                facet_row = 'SWSDLName',\n                sceneorder = sceneorder,\n                scenariofilter = c('down1_5', 'base', 'up1_5'),\n                position = 'dodge') +\n  theme(legend.position = 'none')+\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\n\n\nAnother approach to groups of outcomes without the colors explicitly grouped is to not use colorgroup, but instead just facet by the group and give every colorset value a color from the same palette. Depending on the palette chosen and the breaks, this can be quicker, but will not accentuate groups as well.\n\nobj_sdl_to_plot |&gt;\n  plot_outcomes(y_col = 'ewr_achieved',\n                colorgroups = NULL,\n                colorset = 'env_obj',\n                pal_list = list('scico::berlin'),\n                facet_row = 'SWSDLName',\n                facet_col = 'env_group',\n                scales = 'free_x',\n                scene_x = FALSE,\n                scene_pal = scene_pal,\n                sceneorder = sceneorder,\n                scenariofilter = c('down1_5', 'base', 'up1_5')) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\n\n\nThese plots are interesting, but in typical use, the plots above using facets for the groups or coloring by the groups themselves are likely to be easier to read, unless we really are interested in this level of granularity. Whatever approach we choose for a given plot, accentuating the differences between outcome groups can be a powerful interpretation tool."
  },
  {
    "objectID": "comparer/bar_plots.html#manual-color-definition",
    "href": "comparer/bar_plots.html#manual-color-definition",
    "title": "Bar plots",
    "section": "Manual color definition",
    "text": "Manual color definition\nThough the above examples using {paletteer} palettes are the easiest way to specify coloring, we don’t have to let the palettes auto-choose colors, and can instead pass colors objects, just as we do for scenarios. This can be particularly useful with small numbers of groups (defining too many colors is cumbersome- that’s what palettes are for) when we want to control which is which. Just as with scenarios, we use make_pal . Here, we will use ‘scico::berlin’ as the base, but define several ‘reference’ values manually. This demonstration uses includeRef = TRUE so we replace the palette values with the refs, rather than choose them from the set of values with refs removed. This tends to yield better spread of colors (and lets us sometimes ref colors and sometimes not if we also used returnUnref). For example, maybe we want to sometimes really accentuate ecosystem function and native vegetation, but not in all plots.\nFirst, we create the palettes with and without the (garish) ref values.\n\nobj_pal &lt;- make_pal(levels = unique(obj_sdl_to_plot$env_group),\n                      palette = 'scico::lisbon',\n                    refvals = c('EF', 'NV'), refcols = c('purple', 'orange'), includeRef = TRUE, returnUnref = TRUE)\n\nThen we can create an accentuated plot sometimes, if, perhaps, we want to highlight how EF performed.\n\nplot_outcomes(obj_sdl_to_plot,\n              y_col = 'ewr_achieved',\n              colorset = 'env_group',\n              pal_list = obj_pal$refcols,\n                facet_col = 'SWSDLName',\n              facet_row = '.',\n              sceneorder = sceneorder) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\n\n\nBut for other plots maybe we don’t want that accentuation and we can use the unrefcols to retain the standard coloring- note that ‘NF’, ‘OS’, and ‘WB’ colors are unchanged.\n\nplot_outcomes(obj_sdl_to_plot,\n              y_col = 'ewr_achieved',\n              colorset = 'env_group',\n              pal_list = obj_pal$unrefcols,\n                facet_col = 'SWSDLName',\n              facet_row = '.',\n              sceneorder = sceneorder) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))"
  },
  {
    "objectID": "comparer/comparer_overview.html",
    "href": "comparer/comparer_overview.html",
    "title": "Comparer overview",
    "section": "",
    "text": "The Comparer really has two components- underlying functions and structure to perform comparisons and other analyses, and plotting (and other presentation types in future) capabilities to produce some standardized plots that capture important data visualisation.\nThere is quite a lot of flexibility built into all of the comparer, because different uses and different questions will require different outputs to assess, whether that means different scales of analysis, different types of plots, or different numerical comparisons. It is also likely that the desired outputs will change as we work through options.\nWhile this is called the ‘Comparer’, it also contains other functionality related to analysis generally, and can produce plots that do not include comparisons, e.g. hydrograps to simply illustrate historical flows.\nNearly all plots of outcomes (e.g not hydrographs) are made with plot_outcomes, including bars, lines, and maps. This is because at their foundation, they area all plotting a quantitative outcome with grouping of some sort. The data preparation is the same across all of them, as well as many of the arguments to ggplot. We may, though, eventually separate out the data preparation from the plots to make this function cleaner.\nNearly all plots (with the current exception of the causal networks) are made in ggplot2 and return ggplot2 objects, which can easily be further modified. The plot functions here just wrap the ggplot2 to standardise appearance and data preparation. Though it can be annoying to not use ggplot() directly to make the plots, one MAJOR advantage of the plotting function here is that any data changes that clean it for a given plot aren’t preserved, and so it’s far easier to keep the data clean, know what the data is, and avoid accidental overwriting or mislabeling of data. If we consistently do the same changes with slight modification, we can write a function, e.g. plot_prep , avoiding lots of copy-paste and its attendant errors.\nThere is clear opportunity for reactivity with nearly all plots, allowing a user to select plot types, any filtering (espcially for networks, spatial units, etc), and produces the plot."
  },
  {
    "objectID": "comparer/comparer_overview.html#theme",
    "href": "comparer/comparer_overview.html#theme",
    "title": "Comparer overview",
    "section": "Theme",
    "text": "Theme\nI have developed a theme_werp_toolkit() ggplot theme that we use to get a consistent look. We can build on this and change it as we go, as it is fairly simple at present. Additional theme arguments can be passed to it, if we want to change any of the other arguments in ggplot2::theme() on a per-plot basis. By default, theme_werp_toolkit is applied when making the plots inside plot_outcomes and plot_hydrographs, though it can be applied to any ggplot object."
  },
  {
    "objectID": "comparer/comparer_overview.html#colour",
    "href": "comparer/comparer_overview.html#colour",
    "title": "Comparer overview",
    "section": "Colour",
    "text": "Colour\nAt present, I do not enforce a standard set of colors- they’ll change between scenarios/projects and there are too many possibilities of what we might plot. I do provide default palettes for the plotting functions, but we will likely want to change them depending on what we plot. We could enforce palettes within projects, however, once the plots firm up. In general, colors can either be specified manually (usually with the help of make_pal) or with {paletteer} palettes because of the wide range of options with standard interface and ability to choose based on names. A good reference for the available palettes is here, and demonstrations of both ways of specifying colors are throughout the examples, but specifically bar plots.\nThough we do not enforce standard colors, we have established the infrastructure to set consistent colors within a project by using named color objects. This is particularly useful for scenarios, but also can be used for other qualitative categories. Quantitative outcomes (e.g. matching different palettes to outcome x vs y) is not handled automatically at present, but is left to the user. I expect that as a project proceeds and settles on desired outcomes, we will standardize color palettes for different outcome variables, scenarios, etc.\nThere is some interesting ability to set colors within a single column based on different palettes, which can be a useful way to indicate grouping variables. This is available everywhere, but is best demonstrated in the bar plots and causal plots."
  },
  {
    "objectID": "comparer/comparer_overview.html#internal-calculations-and-structure",
    "href": "comparer/comparer_overview.html#internal-calculations-and-structure",
    "title": "Comparer overview",
    "section": "Internal calculations and structure",
    "text": "Internal calculations and structure\nWhile the plots are the typical outputs of the Comparer, it has a set of useful functions for preparing data, including calculating values relative to a baseline (baseline_compare) using either default functions difference and relative, or with any user-supplied function.\nThere is an internal function plot_prep that does all the data prep, including applying baseline_compare, finding colors, and setting established scenario orders. This keeps plots and the data processing consistent, and dramatically reduces the error-prone copy-pasting of data processing with minor changes for different plots. Instead, we can almost always feed the plotting functions the same set of clean data straight out of the aggregator, and just change the arguments to the plot functions.\nBaselining is available as a standalone function (baseline_compare) and can be done automatically in the plot_* functions. This capacity is demonstrated in all the plot examples, but in most detail in the hydrographs notebook.\nOne critical issue, particularly with complex data, is being unaware of overplotting values. The plot_* functions have internal checks that the number of rows of data matches the number of axes on which the data is plotted (including facets, colors, linetype, etc). This prevents things like plotting a map of env_obj data facetted only by scenario, and so each fill represents outcomes for all env_obj, which is meaningless but very easy to do. The exception is that points are allowed to overplot, though we can use the position = 'position_jitter' to avoid even that."
  },
  {
    "objectID": "comparer/hydrographs.html",
    "href": "comparer/hydrographs.html",
    "title": "Hydrographs",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "comparer/hydrographs.html#demonstration-setup",
    "href": "comparer/hydrographs.html#demonstration-setup",
    "title": "Hydrographs",
    "section": "Demonstration setup",
    "text": "Demonstration setup\nAs usual, we need paths to the data, in this case the hydrographs.\n\nproject_dir &lt;- file.path('more_scenarios')\nhydro_dir = file.path(project_dir, 'hydrographs')"
  },
  {
    "objectID": "comparer/hydrographs.html#scenario-information",
    "href": "comparer/hydrographs.html#scenario-information",
    "title": "Hydrographs",
    "section": "Scenario information",
    "text": "Scenario information\nGet scenario metadata. This will be auto-found later, but leaving here until it firms up.\n\nscenarios &lt;- jsonlite::read_json(file.path(hydro_dir, \n                                           'scenario_metadata.json')) |&gt; \n  tibble::as_tibble() |&gt; \n  tidyr::unnest(cols = everything())"
  },
  {
    "objectID": "comparer/hydrographs.html#subset-for-demo",
    "href": "comparer/hydrographs.html#subset-for-demo",
    "title": "Hydrographs",
    "section": "Subset for demo",
    "text": "Subset for demo\nWe have a lot of hydrographs, so for this demonstration, we will often use a subset.\n\ngauges_to_plot &lt;- c('412002', '419001', '422028', '421001')"
  },
  {
    "objectID": "comparer/maps.html",
    "href": "comparer/maps.html",
    "title": "Maps",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)"
  },
  {
    "objectID": "comparer/maps.html#scenario-information",
    "href": "comparer/maps.html#scenario-information",
    "title": "Maps",
    "section": "Scenario information",
    "text": "Scenario information\nGet scenario metadata. This will be auto-found later, but leaving here until it firms up.\n\nscenarios &lt;- jsonlite::read_json(file.path(hydro_dir, \n                                           'scenario_metadata.json')) |&gt; \n  tibble::as_tibble() |&gt; \n  tidyr::unnest(cols = everything())\n\n\nSubset for easier demonstration\nIn many cases there are too many scenarios for clear examples, so we use the scenariofilter argument to reduce them to a subset. For this demonstration, we will use\n\nscenario_subset &lt;- c('down2', 'down1_25', 'base', 'up1_25', 'up2')"
  },
  {
    "objectID": "comparer/maps.html#choosing-example-data",
    "href": "comparer/maps.html#choosing-example-data",
    "title": "Maps",
    "section": "Choosing example data",
    "text": "Choosing example data\nFirst, we read in the aggregated data. There is example data provided by the toolkit (agged_data and agged_data_colsequence), but to continue with the demonstration, we will use the aggregations created here in the interleaved aggregation notebook.\nNote- to readRDS sf objects, we need to have sf loaded.\n\nagged_data &lt;- readRDS(file.path(agg_dir, 'summary_aggregated.rds'))\n\nAs with line plots, we’ll make a grouping variable in the SDL-scale env_obj data for demonstrating grouped palettes, but will leave the other aggregation levels alone, and do any minor modifications there while piping into plot_outcomes.\n\n# Create a grouping variable\nobj_sdl_to_plot &lt;- agged_data$sdl_units |&gt;\n  left_join(scenarios, by = c('scenario' = 'scenario_name')) |&gt;\n  dplyr::mutate(env_group = stringr::str_extract(env_obj, '^[A-Z]+')) |&gt;\n  dplyr::filter(!is.na(env_group)) |&gt;\n  dplyr::arrange(env_group, env_obj)"
  },
  {
    "objectID": "controller/controller_ewr_wrapped.html",
    "href": "controller/controller_ewr_wrapped.html",
    "title": "Scenario controller",
    "section": "",
    "text": "Load the package\nlibrary(werptoolkitr)\nThe controller primarily sets the paths to scenarios, calls the modules, and saves the output and metadata. In normal use, we set the directory and any other needed parameters (e.g. point at a config file), and the controller functions auto-generate the folder structure, run the ewr, and output the results. This can be taken up a level to the the whole toolkit, where the controller and subsequent steps are all run at once. A detailed stepthrough of what happens in the controller is also available, useful to see what is happening under the hood."
  },
  {
    "objectID": "controller/controller_ewr_wrapped.html#set-paths",
    "href": "controller/controller_ewr_wrapped.html#set-paths",
    "title": "Scenario controller",
    "section": "Set paths",
    "text": "Set paths\nWe need to set the path to this demonstration. This should all be in a single outer directory project_dir, and there should be an inner directory with the input data /hydrographs. These would typically point to external shared directories. For this simple example though, we put the data inside the repo to make it self contained. The saved data goes to project_dir/module_output automatically. The /hydrographs subdirectory could be made automatic as well, but I’m waiting for the input data format to firm up.\n\nproject_dir = file.path('more_scenarios')\nhydro_dir = file.path(project_dir, 'hydrographs')"
  },
  {
    "objectID": "controller/controller_ewr_wrapped.html#control-output-and-return",
    "href": "controller/controller_ewr_wrapped.html#control-output-and-return",
    "title": "Scenario controller",
    "section": "Control output and return",
    "text": "Control output and return\nTo determine what to save and what to return to the active session, use outputType and returnType, respectively. Each of them can take a list of any of 'none', 'summary', 'annual', 'all', with more I need to add to reflect new EWR functionality (e.g. returnType = list('summary', 'all') in R or returnType = ['summary', 'all] in python). These have to be lists to work right- To make a list in python, need to have unnamed lists in R.\nThere’s an issue with 'annual' in py-ewr- I’m getting an error inside the EWR tool. Until I updated the EWR version, skip that.\n\nreturnType &lt;- list('summary', 'all')\n\n# We use outputtype to save, so only save outputs if params$REBUILD_DATA is TRUE\n# To make a list in python, need to have unnamed lists in R\nif (!params$REBUILD_DATA) {\n  outputType &lt;- list('none')\n}\nif (params$REBUILD_DATA) {\n  outputType &lt;- list('summary', 'all')\n}"
  },
  {
    "objectID": "controller/controller_ewr_wrapped.html#run-and-save",
    "href": "controller/controller_ewr_wrapped.html#run-and-save",
    "title": "Scenario controller",
    "section": "Run and save",
    "text": "Run and save\nThe above is all user parameters. All the formatting, running, and saving is then handled with the wrapper function prep_run_save_ewrs. See stepthrough for an expanded version used to run test data and expand each step to make testing/changes more transparent. This is not actually run here for speed- the same thing is done in a notebook for the full toolkit.\n\newr_out &lt;- prep_run_save_ewrs_R(scenario_dir = hydro_dir, \n                                output_dir = project_dir, \n                                outputType = outputType, \n                                returnType = returnType)\n\nNow we have a summary and all. Because the chunk above is not run (#| eval: false), these dataframes are not available here, but would be.\n\newr_out$summary\n\n\newr_out$all"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_in_memory.html",
    "href": "full_toolkit/WERP_toolkit_in_memory.html",
    "title": "Run full toolkit in memory",
    "section": "",
    "text": "This document provides a template for running through the toolkit in a single document, retaining everything in-memory (no intermediate saving). Intermediate saving is a very simple flip of a switch, demoed in its own doc.\nLoad the package\nlibrary(werptoolkitr)\nlibrary(sf)"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_in_memory.html#structure",
    "href": "full_toolkit/WERP_toolkit_in_memory.html#structure",
    "title": "Run full toolkit in memory",
    "section": "Structure",
    "text": "Structure\nTo run the toolkit, we need to provide paths to directories for input data and output data, as well as arguments for the aggregation.\nOne option is to do that in a parameters file, and then treat this as a parameterised notebook.\nThe other option is to have this be the parameterising file, so we can have a bit more text around the parameterisations. Not sure which makes more sense, but they’re not mutually exclusive, and the answer likely depends on whether we’re working interactively or want to fire off 1,000 runs."
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_in_memory.html#directories",
    "href": "full_toolkit/WERP_toolkit_in_memory.html#directories",
    "title": "Run full toolkit in memory",
    "section": "Directories",
    "text": "Directories\n\nInput and output directories\nUse the scenario_example/ directory created to capture a very simple demonstration case of 46 gauges in three catchments for 10 years.\nNormally scenario_dir should point somewhere external (though keeping it inside or alongside the hydrograph data is a good idea.). But here, I’m generating test data, so I’m keeping it in the repo. I will probably change that when I move to Azure.\n\n# Outer directory for scenario\nproject_dir = file.path('more_scenarios')\n\n# Preexisting data\n# Hydrographs (expected to exist already)\nhydro_dir = file.path(project_dir, 'hydrographs')\n\n# Generated data\n# EWR outputs (will be created here in controller, read from here in aggregator)\newr_results &lt;- file.path(project_dir, 'module_output', 'EWR')\n\n# outputs of aggregator. There may be multiple modules\nagg_results &lt;- file.path(project_dir, 'aggregator_output')"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_in_memory.html#controller",
    "href": "full_toolkit/WERP_toolkit_in_memory.html#controller",
    "title": "Run full toolkit in memory",
    "section": "Controller",
    "text": "Controller\nWe use the default IQQM model format and climate categorisations, though those could be passed here as well (see controller).\n\nControl output and return\nTo determine what to save and what to return to the active session, use outputType and returnType, respectively. Each of them can take a list of any of 'none', 'summary', 'annual', 'all'. For this demonstration I’ll not save anything and return summary to the active session.\n\noutputType &lt;- list('summary')\nreturnType &lt;- list('summary') # list('summary', 'all')"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_in_memory.html#aggregator",
    "href": "full_toolkit/WERP_toolkit_in_memory.html#aggregator",
    "title": "Run full toolkit in memory",
    "section": "Aggregator",
    "text": "Aggregator\nTo keep this simple, we use one aggregation list and the read_and_agg wrapper to only have to pass paths. See the more detailed documents for the different ways to specify those aggregation lists.\n\nWhat to aggregate\nThe aggregator needs to know which set of EWR outputs to use (to navigate the directory or list structure). It should accept multiple types, but that’s not well tested, so for now just use one.\n\naggType &lt;- 'summary'\n\nWe need to tell it the variable to aggregate, and any grouping variables other than the themes and spatial groups. Typically, scenario will be a grouper, but there may be others.\n\nagg_groups &lt;- 'scenario'\nagg_var &lt;- 'ewr_achieved'\n\nDo we want it to return to the active session? For this demo, I’m keeping everything in the session, so set to TRUE.\n\naggReturn &lt;- TRUE\n\n\n\nHow to aggregate\nFundamentally, the aggregator needs paths and two lists\n\nsequence of aggregations\nsequence of aggregation functions (can be multiple per step)\n\nHere, I’m using an interleaved list of theme and spatial aggregations (see the detailed docs for more explanation), and applying only a single aggregation function at each step for simplicity. Those steps are specified a range of different ways to give a small taste of the flexibility here, but see the spatial and theme docs for more examples.\n\naggseq &lt;- list(ewr_code = c('ewr_code_timing', 'ewr_code'),\n               env_obj =  c('ewr_code', \"env_obj\"),\n               resource_plan = resource_plan_areas,\n               Specific_goal = c('env_obj', \"Specific_goal\"),\n               catchment = cewo_valleys,\n               Objective = c('Specific_goal', 'Objective'),\n               mdb = basin,\n               target_5_year_2024 = c('Objective', 'target_5_year_2024'))\n\n\nfunseq &lt;- list(c('CompensatingFactor'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               rlang::quo(list(wm = ~weighted.mean(., w = area, \n                                        na.rm = TRUE))),\n               c('ArithmeticMean'),\n               \n               rlang::quo(list(wm = ~weighted.mean(., w = area, \n                                    na.rm = TRUE))),\n               c('ArithmeticMean'))"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_in_memory.html#controller-1",
    "href": "full_toolkit/WERP_toolkit_in_memory.html#controller-1",
    "title": "Run full toolkit in memory",
    "section": "Controller",
    "text": "Controller\nThis is not actually run here for speed- the same thing is done in a notebook for the full toolkit saving steps.\n\n  ewr_out &lt;- prep_run_save_ewrs_R(scenario_dir = hydro_dir, \n                                  output_dir = project_dir, \n                                  outputType = outputType,\n                                  returnType = returnType)"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_in_memory.html#aggregator-1",
    "href": "full_toolkit/WERP_toolkit_in_memory.html#aggregator-1",
    "title": "Run full toolkit in memory",
    "section": "Aggregator",
    "text": "Aggregator\nBecause the chunk above is not run, the needed EWR outputs are not available, but would be if it were run.\n\naggout &lt;- read_and_agg(datpath = ewr_results, \n             type = aggType,\n             geopath = bom_basin_gauges,\n             causalpath = causal_ewr,\n             groupers = agg_groups,\n             aggCols = agg_var,\n             aggsequence = aggseq,\n             funsequence = funseq,\n             saveintermediate = TRUE,\n             namehistory = FALSE,\n             keepAllPolys = TRUE,\n             returnList = aggReturn,\n             savepath = agg_results)"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_save_steps.html",
    "href": "full_toolkit/WERP_toolkit_save_steps.html",
    "title": "Run full toolkit (saving)",
    "section": "",
    "text": "This document provides a template for running through the toolkit, saving the output of each step along the way in a single document (e.g. not running the Controller, Aggregator, and Comparer as separate notebooks). Retaining everything in-memory is a very simple flip of a switch, demoed in its own doc.\nlibrary(werptoolkitr)\nlibrary(sf)"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_save_steps.html#structure",
    "href": "full_toolkit/WERP_toolkit_save_steps.html#structure",
    "title": "Run full toolkit (saving)",
    "section": "Structure",
    "text": "Structure\nTo run the toolkit, we need to provide paths to directories for input data and output data, as well as arguments for the aggregation.\nOne option is to do that in a parameters file, and then treat this as a parameterised notebook.\nThe other option is to have this be the parameterising file, so we can have a bit more text around the parameterisations. These are not mutually exclusive options, just different interfaces to the code. The answer for any particular analysis likely depends on whether we’re working interactively or want to fire off a large number of parallel runs."
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_save_steps.html#directories",
    "href": "full_toolkit/WERP_toolkit_save_steps.html#directories",
    "title": "Run full toolkit (saving)",
    "section": "Directories",
    "text": "Directories\n\nInput and output directories\nUse the scenario_example/ directory created to capture a very simple demonstration case of 46 gauges in three catchments for 10 years.\nNormally project_dir should point somewhere external (though keeping it outside or alongside the hydrograph data is a good idea.). But here, I’m generating test data, so I’m keeping it in the repo.The flow scaling example takes the more typical approach of pointing to data external to the repo.\n\n# Outer directory for scenario\nproject_dir = file.path('more_scenarios')\n\n# Preexisting data\n# Hydrographs (expected to exist already)\nhydro_dir = file.path(project_dir, 'hydrographs')\n\n# Generated data\n# EWR outputs (will be created here in controller, read from here in aggregator)\newr_results &lt;- file.path(project_dir, 'module_output', 'EWR')\n\n# outputs of aggregator. There may be multiple modules\nagg_results &lt;- file.path(project_dir, 'aggregator_output')"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_save_steps.html#controller",
    "href": "full_toolkit/WERP_toolkit_save_steps.html#controller",
    "title": "Run full toolkit (saving)",
    "section": "Controller",
    "text": "Controller\nWe use the default IQQM model format and climate categorisations, though those could be passed here as well (see controller).\n\nControl output and return\nTo determine what to save and what to return to the active session, use outputType and returnType, respectively. Each of them can take a list of any of 'none', 'summary', 'annual', 'all'. For this demonstration I’ll just use summary and not return anything to memory.\n\noutputType &lt;- list('summary')\nreturnType &lt;- list('none') # list('summary', 'all')"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_save_steps.html#aggregator",
    "href": "full_toolkit/WERP_toolkit_save_steps.html#aggregator",
    "title": "Run full toolkit (saving)",
    "section": "Aggregator",
    "text": "Aggregator\nTo keep this simple, we use one aggregation list and the read_and_agg wrapper to only have to pass paths. See the more detailed documents for the different ways to specify those aggregation lists.\n\nWhat to aggregate\nThe aggregator needs to know which set of EWR outputs to use (to navigate the directory or list structure). It should accept multiple types, but that’s not well tested, so for now just use one.\n\naggType &lt;- 'summary'\n\nWe need to tell it the variable to aggregate, and any grouping variables other than the themes and spatial groups. Typically, scenario will be a grouper, but there may be others.\n\nagg_groups &lt;- 'scenario'\nagg_var &lt;- 'ewr_achieved'\n\nDo we want it to return to the active session? For this demo, nothing should return here- we’re saving outputs, not returning them to the session.\n\naggReturn &lt;- FALSE\n\n\n\nHow to aggregate\nFundamentally, the aggregator needs paths and two lists\n\nsequence of aggregations\nsequence of aggregation functions (can be multiple per step)\n\nHere, I’m using an interleaved list of theme and spatial aggregations (see the detailed docs for more explanation), and applying only a single aggregation function at each step for simplicity. Those steps are specified a range of different ways to give a small taste of the flexibility here, but see the spatial and theme docs for more examples.\n\naggseq &lt;- list(ewr_code = c('ewr_code_timing', 'ewr_code'),\n               env_obj =  c('ewr_code', \"env_obj\"),\n               sdl_units = sdl_units,\n               Specific_goal = c('env_obj', \"Specific_goal\"),\n               catchment = cewo_valleys,\n               Objective = c('Specific_goal', 'Objective'),\n               mdb = basin,\n               target_5_year_2024 = c('Objective', 'target_5_year_2024'))\n\n\nfunseq &lt;- list(c('CompensatingFactor'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               rlang::quo(list(wm = ~weighted.mean(., w = area, \n                                        na.rm = TRUE))),\n               c('ArithmeticMean'),\n               \n               rlang::quo(list(wm = ~weighted.mean(., w = area, \n                                    na.rm = TRUE))),\n               c('ArithmeticMean'))"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_save_steps.html#controller-1",
    "href": "full_toolkit/WERP_toolkit_save_steps.html#controller-1",
    "title": "Run full toolkit (saving)",
    "section": "Controller",
    "text": "Controller\n\nif (params$REBUILD_DATA) {\n  ewr_out &lt;- prep_run_save_ewrs_R(scenario_dir = hydro_dir, \n                                  output_dir = project_dir, \n                                  outputType = outputType,\n                                  returnType = returnType)\n}"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_save_steps.html#aggregator-1",
    "href": "full_toolkit/WERP_toolkit_save_steps.html#aggregator-1",
    "title": "Run full toolkit (saving)",
    "section": "Aggregator",
    "text": "Aggregator\n\nif (params$REBUILD_DATA) {\n  aggout &lt;- read_and_agg(datpath = ewr_results, \n             type = aggType,\n             geopath = bom_basin_gauges,\n             causalpath = causal_ewr,\n             groupers = agg_groups,\n             aggCols = agg_var,\n             aggsequence = aggseq,\n             funsequence = funseq,\n             saveintermediate = TRUE,\n             namehistory = FALSE,\n             keepAllPolys = FALSE,\n             returnList = aggReturn,\n             savepath = agg_results)\n}\n\nJoining with `by = join_by(gauge)`\nJoining with `by = join_by(gauge, ewr_code, ewr_code_timing, PlanningUnitID)`\nJoining with `by = join_by(gauge)`\nJoining with `by = join_by(gauge, ewr_code, PlanningUnitID)`\nJoining with `by = join_by(env_obj)`\nJoining with `by = join_by(Specific_goal)`\nJoining with `by = join_by(Objective)`\n\n\nIt would be straightforward here to run the comparer as well, but as discussed above, there is not much reason until we settle on a couple canonical outputs."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "WERP toolkit demonstration",
    "section": "",
    "text": "This repo provides a templates and documentation for using the toolkit in various ways. In use, we would expect a user to consider the various capabilities demonstrated here, and then develop a streamlined flow with just those parameters (at least through the aggregator step), similar to what is shown in the full toolkit pages and implement that in an analysis repo for that particular set of analyses/scenarios.\nThe full_toolkit directory provides notebooks for running the full toolkit in single files, either in memory or saving each step and reading back in.\nWithin this repo there are also documents providing templates for each step separately (in controller and aggregation directories). These files allow exploring more of the options for calling, particularly for aggregation. There are also a few templates or examples of notebooks that are not in the toolkit flow per se, but are necessary for it (scenario_creation and causal_networks).\nToolkit flow\nNecessary pieces but not part of the data flow in the toolkit\nIn use, the toolkit expects that scenario hydrographs will be available and the causal networks are defined."
  },
  {
    "objectID": "index.html#installing-werptoolkitr",
    "href": "index.html#installing-werptoolkitr",
    "title": "WERP toolkit demonstration",
    "section": "Installing werptoolkitr",
    "text": "Installing werptoolkitr\nThe {werptoolkitr} package needs to be installed to provide all functions used here. It also provides some necessary data for the causal network relationships, and (at least for now) canonical shapefiles that have been prepped.\nTypically install it from github over SSH while the package is private.\n\n# GITHUB INSTALL\n# SSH- preferred\n\ndevtools::install_git(\"git@github.com:MDBAuth/WERP_toolkit.git\", ref = 'master', force = TRUE, upgrade = 'ask')\n\nThe package allowing SSH install is broken in R 4.3, so in that case and for rapid development, clone the repo and use devtools::install_local (or even just to load_all), but these rely on paths that aren’t portable and are more manual to keep up with updates.\n\n# LOCAL INSTALL- easier for quick iterations, but need a path.\ndevtools::install_local(\"C:/path/to/WERP_toolkit\", force = TRUE)\n\n# And for very fast iteration (no building, but exposes too much, often)\ndevtools::load_all(\"C:/path/to/WERP_toolkit\")\n\nLoad the package\n\nlibrary(werptoolkitr)"
  },
  {
    "objectID": "index.html#github-and-development",
    "href": "index.html#github-and-development",
    "title": "WERP toolkit demonstration",
    "section": "Github and development",
    "text": "Github and development\nSee the repo readme for overall structure of the repo and additional dev info for package installation issues and development, and the {werptoolkitr} repo for the package itself."
  },
  {
    "objectID": "index.html#using-notebooks",
    "href": "index.html#using-notebooks",
    "title": "WERP toolkit demonstration",
    "section": "Using notebooks",
    "text": "Using notebooks\nMost notebooks that generate outputs have a REBUILD_DATA parameter in the yaml header that is set to FALSE to avoid overwriting data. To rebuild the data, we can manually change them to TRUE, or on a case-by-case basis at the terminal quarto render path/to/file.qmd -P REBUILD_DATA:TRUE, and for all files in the project (dangerous!), quarto render -P REBUILD_DATA:TRUE.\nIf there is unexpected behaviour, e.g. changes not reflected in the output, check the _cache files and probably throw them out. Caching speeds up by not re-running code and is supposed to notice changes and re-evaluate, but sometimes hangs on when it shouldn’t."
  },
  {
    "objectID": "scenario_creation/scenario_creation_more.html",
    "href": "scenario_creation/scenario_creation_more.html",
    "title": "Creating simple scenarios",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(reticulate) # Not strictly necessary but allows easier referencing of objects"
  },
  {
    "objectID": "scenario_creation/scenario_creation_more.html#toolkit-relevance",
    "href": "scenario_creation/scenario_creation_more.html#toolkit-relevance",
    "title": "Creating simple scenarios",
    "section": "Toolkit relevance",
    "text": "Toolkit relevance\nThe creation of flow scenarios is not part of the toolkit proper. Instead, the toolkit expects to ingest hydrographs and then handles the ongoing response models, aggregation, and analyses. Thus, hydrographs are an essential input to the toolkit. The point of this code is to generate those hydrographs.\nThis notebook generates 9 simple scenarios by multiplying and dividing hydrographs. This lets us look at a range of changes. The primary needs are multiple guages in multiple catchments (or other spatial units), and scenarios defined by different hydrographs for the same gauge."
  },
  {
    "objectID": "scenario_creation/scenario_creation_more.html#process",
    "href": "scenario_creation/scenario_creation_more.html#process",
    "title": "Creating simple scenarios",
    "section": "Process",
    "text": "Process\nWe pull a limited set of gauges for a limited time period to keep this dataset small. Primarily, we identify a set of gauges in two catchments, pull them for a short time period, and adjust them to create two simple modified scenarios, with the original data serving as the baseline scenario. Along the way, we examine the data in various ways to visualise what we’re doing and where.\nA larger and more complex set of scenarios is created in the flow scaling demonstration, without as much visualisation."
  },
  {
    "objectID": "scenario_creation/scenario_creation_more.html#paths-and-other-data",
    "href": "scenario_creation/scenario_creation_more.html#paths-and-other-data",
    "title": "Creating simple scenarios",
    "section": "Paths and other data",
    "text": "Paths and other data\nThe shapefiles used to see what we’re doing and do the selecting were produced with within the WERP_toolkit package to keep consistency. It’s possible we’ll add more shapefile creation and move all the canonical versions and their creation to their own data package or repo.\nSet the data directory to make that easy to change. These should usually point to external shared directories. For this simple example though, we put the data inside the repo to make it self contained. The flow-scaling analyses sends them externally, which would be more typical.\n\nscenario_dir &lt;- 'more_scenarios'\nhydro_dir &lt;- file.path(scenario_dir, 'hydrographs')"
  },
  {
    "objectID": "scenario_creation/scenario_creation_more.html#language-note",
    "href": "scenario_creation/scenario_creation_more.html#language-note",
    "title": "Creating simple scenarios",
    "section": "Language note",
    "text": "Language note\nThis notebook was originally built using only python, and there is an unmaintained python-only version available by contacting the authors. Using Python makes a lot of sense because the underlying data here uses python packages. I’ve moved the active version of this notebook to R, however, when the toolkit became an R package and the flow scaling analyses ended up using R gauge pullers. There is still some remaining python in here (pulling gauges and some minor EWR functions). This notebook provides an example of how to mix R and python code chunks, which we do fairly frequently.\nWe can access python objects in R with py$objectnameand access R objects in python with r.objectname .\nIt takes -forever- to do a type translation on the DATETIME column in the gauge data. It’s unclear why (can’t replicate it with any other datetime py object). We work around that by changing it to something simple while still in python, and change it back to datetime in R."
  },
  {
    "objectID": "scenario_creation/scenario_creation_more.html#spatial-datasets",
    "href": "scenario_creation/scenario_creation_more.html#spatial-datasets",
    "title": "Creating simple scenarios",
    "section": "Spatial datasets",
    "text": "Spatial datasets\nWe use spatial datasets provided by {werptoolkitr}, which creates a standard set in data_creation/spatial_data_creation.qmd. These are visualised in a separate notebook. Relevant to this scenario creation, we are interested in the gauges, (werptoolkitr::bom_basin_gauges) since this is what were contained in the EWR tool. We use the sdl_units dataset to obtain a subset of gauges for these simple scenarios. Relevant to the case study- the original polygon used was the Macquarie-Castlereagh in the resource_plan_areas, though we seem to use sdl units elsewhere, so I’ll use them here."
  },
  {
    "objectID": "scenario_creation/scenario_creation_more.html#get-relevant-gauges",
    "href": "scenario_creation/scenario_creation_more.html#get-relevant-gauges",
    "title": "Creating simple scenarios",
    "section": "Get relevant gauges",
    "text": "Get relevant gauges\nCut the bom_basin_gauges from the whole country to just those four catchments\n\ndemo_gauges &lt;- st_intersection(bom_basin_gauges, catch_demo)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nHow many are there?\n\ndemo_gauges %&gt;% nrow()\n\n[1] 295\n\n\nThat’s a fair number, but they won’t all be in the EWR.\n\nExtract their names\nTo feed to the gauge puller, we need their gauge numbers.\n\ngaugenums &lt;- demo_gauges$gauge\n\n\n\nFind those relevant to toolkit\nWe have the list of gauges, but now we need to cut the list down to those in the EWR tool. There’s not any point in pulling gauges that do not appear later in the toolkit.\nWhich gauges are actually in the EWR tool? The EWR tool has a function, so use that.\n\nTODO THIS FAILS AS OF 1.0.4. I have rolled back to ewr version 1.0.1, since the necessary file just doesn’t exist in 1.0.4 (and in about half the branches on github). This needs to be updated and tested.\nError messages:\nFileNotFoundError: [Errno 2] No such file or directory: 'py_ewr/parameter_metadata/NSWEWR.csv'\n\nError in py_get_attr_impl(x, name, silent) : \n  AttributeError: module '__main__' has no attribute 'ewrs'\n\nA simple {python} chunk works fine in Rstudio and for quarto render from command line. But in vscode, the default (and unchangeable at present) is to start a whole new engine that doesn’t talk to R, rather than using reticulate::repl_python as is done in Rstudio and by quarto itself.\n```{python}\nfrom py_ewr.data_inputs import get_EWR_table\newrs, badewrs = get_EWR_table()\n```\n\npdi &lt;- import(\"py_ewr.data_inputs\")\n\nWarning in normalizePath(path.expand(path), winslash, mustWork):\npath[1]=\"C:/Users/galen/Documents/code/WERP/demo_pkg_updates/.venv/Scripts\":\nThe system cannot find the path specified\n\newrs_in_pyewr &lt;- pdi$get_EWR_table() \nnames(ewrs_in_pyewr) &lt;- c('ewrs', 'badewrs')\n\nWhat are those gauges, and which are in both the ewr and the desired catchments?\nThe way that works everywhere but vscode- this seems universal though\n\newrgauges &lt;- ewrs_in_pyewr$ewr$Gauge\newr_demo_gauges &lt;- gaugenums[gaugenums %in% ewrgauges]\nlength(ewr_demo_gauges)\n\n[1] 47\n\n\n47 isn’t too many.\n\n\nGet all the gauge data\nNow we have a list of gauges, we need their hydrographs. We need a reasonable time span to account for temporal variation, but not too long- this is a simple case. Let’s choose 10 years.\n\nstarttime = lubridate::ymd(20100101)\nendtime = lubridate::ymd(20191231)\n\nPull the gauges with mdba_gauge_getter. The type-translation that happens in here is because translating from python time to R time is extremely slow for this particular case (though not in general).\nThis again needs a vscode translation\n\n#| message: false\nimport mdba_gauge_getter as gg\ndemo_levs = gg.gauge_pull(r.ewr_demo_gauges, start_time_user = r.starttime, end_time_user = r.endtime)\ndemo_ids = demo_levs.SITEID.unique()\nlen(demo_ids)\n\n# I think this will work, the above is running\ndemo_levs['Date'] = demo_levs['DATETIME'].astype(str)\n\ngg &lt;- import('mdba_gauge_getter')\ndemo_levs &lt;- gg$gauge_pull(ewr_demo_gauges, start_time_user = starttime, end_time_user = endtime)\ndemo_ids &lt;- unique(demo_levs$SITEID)\nlength(demo_ids)\n\n[1] 46\n\n\nDo a bit of cleanup- for some reason demo_levs['VALUE'] is an object and not numeric, and 'DATETIME' needs to be named Date for the EWR tool to read it. I copy the py object to R for this manipulation and visualisation, but we could just proceed in python if we wanted.\nAgain, we need to change things to run on R\n\ndemo_levs &lt;- py$demo_levs\n\ndemo_levs$VALUE = as.numeric(demo_levs$VALUE)\n\n# # In python, we just need to change the name of the date column. Here, we need to change the python datetime.date objects to R dates\n# \n# # Really slow\n# # MUCH faster to just make the dates characters in python, and back to dates here.\n# rdates &lt;- purrr::map(demodates, py_to_r) %&gt;% \n#   tibble(.name_repair = ~'Date') %&gt;%  \n#   unnest(cols = Date)\n# \n# demo_levs &lt;- bind_cols(rdates, demo_levs)\ndemo_levs &lt;- dplyr::select(demo_levs, -DATETIME) %&gt;% \n  dplyr::mutate(Date = lubridate::ymd(Date))\nThe new version has to use the reticulate::py_to_r, because when we’re not in the repl, we have an R object to work with for every line of python. Should probably just change to {vicwater}, like we do for flow scaling.\n\ndemo_levs &lt;- demo_levs |&gt; \n  dplyr::mutate(VALUE = as.numeric(VALUE))\n\n# 47 seconds- doesn't matter if preallocated\nsystem.time(\n  Date &lt;- purrr::map(demo_levs$DATETIME, reticulate::py_to_r)\n)\n\n   user  system elapsed \n  10.87    0.05   25.66 \n\n# # 50 seconds\n# system.time(\n#   Date &lt;- lapply(demo_levs$DATETIME,FUN = reticulate::py_to_r)\n# )\n\nDate &lt;- tibble::tibble(Date) |&gt;\n  tidyr::unnest(cols = Date)\n\ndemo_levs &lt;- demo_levs |&gt;\n  dplyr::select(-DATETIME) |&gt; \n  dplyr::bind_cols(Date) \n\n\n\nMap the gauges\n\ndemo_geo = bom_basin_gauges %&gt;% dplyr::filter(gauge %in% demo_ids)\n\nLooks reasonable. Probably overkill for testing, but can do a cut down version too.\nThe azure boxes have old GDAL, which can’t read WKT2. Need to fix, but in the meantime, force with the crs number.\n\nif (grepl('npd-dat', Sys.info()['nodename'])) {\n  st_crs(basin) &lt;- 4283\n  st_crs(catch_demo) &lt;- 4283\n  st_crs(demo_geo) &lt;- 4283\n}\n\n\n(ggplot() + \ngeom_sf(data = basin, fill = 'lightsteelblue') +\ngeom_sf(data = catch_demo, mapping = aes(fill = SWSDLID)) +\ngeom_sf(data = demo_geo, color = 'black') +\nscale_fill_brewer(type = 'qual', palette = 8))"
  }
]