[
  {
    "objectID": "aggregator/using_multi_aggregate.html",
    "href": "aggregator/using_multi_aggregate.html",
    "title": "Aggregating over multiple dimensions",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(dplyr)\nlibrary(ggplot2)",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#overview",
    "href": "aggregator/using_multi_aggregate.html#overview",
    "title": "Aggregating over multiple dimensions",
    "section": "Overview",
    "text": "Overview\nWe have theme aggregation, temporal_aggregation, and spatial aggregation shown separately for in-depth looks at their meaning and capability. Here, we focus on the typical use-case of interleaved aggregation along multiple dimensions. We do not get into all the different options and syntax as they are covered in those other documents. In use, multi_aggregate() is typically not called directly, instead, [read_and_agg](read_and_agg.qmd) while providing data read-in to allow just passing paths, allows for parallelisation, and maintains data provenance and metadata. Because multi_aggregate() provides the core aggregation functionality, we discuss it here.\nAll multi-step aggregation in {HydroBOT} operates on the same core function and use similar principles- take a list of aggregation sequences, and aggregate each step according to a list of aggregation functions. Here, we show how multi_aggregate allows us to interleave the dimensions along which we aggregate, including auto-detecting which dimension we’re operating on.\nFundamentally, multi_aggregate wraps theme_aggregate(), temporal_aggregate(), and spatial_aggregate() with data organisation and tracking of what the previous level of aggregation was to maintain proper grouping as they alternate. This provides critical functionality to prevent accidental collapse along multiple dimensions simultaneously, and also allows grouping through part of the aggregation sequence (see here). These internal functions wrap general_aggregate with some data arrangement specific to the dimension they aggregate along, such as stripping and re-adding geometry.\nThe multi_aggregate() function expects the incoming data to be in memory and geographic, and prefers (but does not require) the edges defining theme relationships to already be calculated (though in practice they typically are calculated on the fly by read_and_agg().\n\n\n\n\n\n\nNote\n\n\n\nAny step in the aggregation sequence can have multiple aggregations (e.g. calculating min and max). These single-step multiples are factorial with the steps- if we aggregate with min and max at step 2 (generating two columns of aggregated data), then each of those columns is aggregated according to the step-3 aggregation, and so on. This can be quite useful, but can also yield complex outputs with many unneeded sequences. It is worth considering running separate aggregation sequences if there are many places with multiple aggregations, particularly if they occur early in the sequence.",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#demonstration-setup",
    "href": "aggregator/using_multi_aggregate.html#demonstration-setup",
    "title": "Aggregating over multiple dimensions",
    "section": "Demonstration setup",
    "text": "Demonstration setup\nFirst, we need to provide a set of paths to point to the input data, in this case the outputs from the EWR tool for the small demonstration, created by a controller notebook. Spatial units could be any arbitrary polygons, but we use those provided by {HydroBOT} for consistency, which also provides the spatial locations of the gauges in bom_basin_gauges.\n\nproject_dir &lt;- \"hydrobot_scenarios\"\nhydro_dir &lt;- file.path(project_dir, 'hydrographs')\newr_results &lt;- file.path(project_dir, \"module_output\", \"EWR\")\n\nScenario information\nThis will be attached to metadata, typically. For this demonstration, we just use it for plot clarity and the data is simple.\n\nmultipliers &lt;- c(1.1, 1.5, 2, 3, 4)\n\nscenemults &lt;- c(1 / rev(multipliers), 1, multipliers)\n\nscenenames &lt;- c(\n  paste0(\"down\", as.character(rev(multipliers))),\n  \"base\",\n  paste0(\"up\", as.character(multipliers))\n) |&gt;\n  stringr::str_replace(\"\\\\.\", \"_\")\n\n\nscenarios &lt;- tibble::tibble(scenario = scenenames, delta = scenemults)\n\nscene_pal &lt;- make_pal(unique(scenarios$scenario), palette = \"ggsci::nrc_npg\", refvals = \"base\", refcols = \"black\")",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#data-prep",
    "href": "aggregator/using_multi_aggregate.html#data-prep",
    "title": "Aggregating over multiple dimensions",
    "section": "Data prep",
    "text": "Data prep\nTo make the actual multi-aggregate loop general, dataprep needs to happen first (e.g. we don’t want to do EWR-specific dataprep on econ data). That said, we can use read_and_agg, which takes paths and the aggregation lists and runs the dataprep and aggregation functions. At present, the EWR tool is the only module, so we run it and do minor data prep that would usually be handled by read_and_agg(), e.g. calculating EWR achievement and making it geographic with gauge locations.\n\newr_out &lt;- prep_run_save_ewrs(\n  hydro_dir = hydro_dir,\n  output_parent_dir = project_dir,\n  outputType = list('none'),\n  returnType = list('yearly')\n)\n\n\n# This is just a simple prep step that is usually done internally to put the geographic coordinates on input data\newrdata &lt;- prep_ewr_output(ewr_out$yearly, type = 'achievement', add_max = FALSE)\n\nInitially, we use causal_ewrs for causal relationships and spatial layers provided by {HydroBOT} to define the aggregation units, and defined functions for the aggregations, though each of these can be manually specified (see Section 1.3, Section 1.4, Section 1.5).",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#setup",
    "href": "aggregator/using_multi_aggregate.html#setup",
    "title": "Aggregating over multiple dimensions",
    "section": "Setup",
    "text": "Setup\nFirst, we specify a simple interleaved aggregation sequence with only one aggregation function applied per step for simplicity. Note that theme-axis aggregation steps are specified with a character vector c('level_from', 'level_to'), while spatial aggregation steps are specified with an sf object (polygons) or the name of the sf object, e.g. sdl_units or \"sdl_units\". Allowing specification of spatial steps by character instead of object is a bit more fragile because it relies on get(\"name_of_object\"), but allows the list to be wholly specified with characters.\nHere, we specify the aggregation function sequence with character names for functions, though other specifications are possible as discussed in the syntax notebook.\n\n\n\n\n\n\nTip\n\n\n\nNaming the aggsequence and funsequence lists by the level aggregated into makes tracking and interpretation much easier, and is highly recommended.\n\n\nSpatial aggregation should almost always be area-weighted after the data is in polygons (see spatial notebook and aggregation syntax), though there are some aggregation functions where it doesn’t matter (e.g. max). All polygon data has an area column calculated automatically for this reason. The first aggregation into polygons typically is not area-weighted, because the thing being aggregated (typically at the gauge scale) usually doesn’t have area. After that, all data is in polygons and so has area.\nWe consider a set of aggregations covering all three dimensions. It begins temporal (all_time), then has two theme aggregations (ewr_code and env_obj), then spatial to sdl_units, two more theme-dimension (Specific_goal, Objective), a spatially-weighted aggregation to the basin, and finally to the theme level of 5-year management targets.\n\naggseq &lt;- list(\n  all_time = 'all_time',\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  env_obj = c(\"ewr_code\", \"env_obj\"),\n  sdl_units = sdl_units,\n  Specific_goal = c(\"env_obj\", \"Specific_goal\"),\n  Objective = c(\"Specific_goal\", \"Objective\"),\n  basin = basin,\n  target_5_year_2024 = c(\"Objective\", \"target_5_year_2024\")\n)\n\nfunseq &lt;- list(\n  all_time = 'ArithmeticMean',\n  ewr_code = \"CompensatingFactor\",\n  env_obj = \"ArithmeticMean\",\n  sdl_units = \"ArithmeticMean\",\n  Specific_goal = \"ArithmeticMean\",\n  Objective = \"ArithmeticMean\",\n  basin = 'SpatialWeightedMean',\n  target_5_year_2024 = \"ArithmeticMean\"\n  )",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#aggregate",
    "href": "aggregator/using_multi_aggregate.html#aggregate",
    "title": "Aggregating over multiple dimensions",
    "section": "Aggregate",
    "text": "Aggregate\nNow we do the aggregation. Note that we have been very aggressive in handling spatial processing and so while spatial processing is slow, we minimize it as much as possible internally.\nBecause we’re aggregating EWR outputs, multi_aggregate() will catch some common pitfalls. The best solution is to use pseudo_spatial and group_until, but here for simplicity we use auto_ewr_PU = TRUE , which does that automatically.\n\n\n\n\n\n\nTip\n\n\n\nBy default the data column has a very long name, which provides a record of its full provenance. We can turn this into columns, which is usually clearer. The simplest way to do this is to let multi_aggregate() do it internally with namehistory = FALSE. For more detail, see Section 1.2.\n\n\nReturn only final\nThe default option is to return only the final outcome, which is far cheaper for memory, but doesn’t say how we got that answer.\n\ntsagg &lt;- multi_aggregate(\n  dat = ewrdata,\n  causal_edges = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  aggsequence = aggseq,\n  funsequence = funseq,\n  auto_ewr_PU = TRUE,\n  namehistory = FALSE\n)\n\nThat saves only the final outcome, which is far cheaper for memory, but doesn’t let us step through the intermediate steps.\n\ntsagg\n\n\n  \n\n\n\nWe can do the exact same thing with characters instead of objects for the spatial units:\n\naggseq_c &lt;- list(\n  all_time = 'all_time',\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  env_obj = c(\"ewr_code\", \"env_obj\"),\n  sdl_units = \"sdl_units\",\n  Specific_goal = c(\"env_obj\", \"Specific_goal\"),\n  Objective = c(\"Specific_goal\", \"Objective\"),\n  basin = \"basin\",\n  target_5_year_2024 = c(\"Objective\", \"target_5_year_2024\")\n)\n\ntsagg_c &lt;- multi_aggregate(\n  dat = ewrdata,\n  causal_edges = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  aggsequence = aggseq_c,\n  funsequence = funseq,\n  auto_ewr_PU = TRUE,\n  namehistory = FALSE\n)\n\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`.\n\n\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`\n.\n! Unmatched links in causal network\n• 29 from ewr_code_timing to ewr_code\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`\n.\nℹ EWR gauges joined to larger units pseudo-spatially.\n• Done automatically because `auto_ewr_PU = TRUE`\n• Non-spatial join needed because gauges may inform areas they are not within\n• Best to explicitly use `pseudo_spatial = 'sdl_units'` in `multi_aggregate()` or `read_and_agg()`.\n\n! Unmatched links in causal network\n• 10 from env_obj to Specific_goal\n! Unmatched links in causal network\n• 7 from Objective to target_5_year_2024\n\ntsagg_c\n\n\n  \n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe ability to pass characters is only available in multi_aggregate() and read_and_agg(), not the internal spatial_aggregate() function.\n\n\nAll steps\nMost often, we’ll want to save the list of outcomes at each step- it allows us to see how we got the final outcome, and it’s likely we’re interested in the outcomes at more than one step anyway. As in the theme notebook, we do this with saveintermediate = TRUE. We also set namehistory = FALSE to put the aggregation tracking in columns instead of names for ease of handling the output.\n\nallagg &lt;- multi_aggregate(\n  dat = ewrdata,\n  causal_edges = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  aggsequence = aggseq,\n  funsequence = funseq,\n  auto_ewr_PU = TRUE,\n  namehistory = FALSE,\n  saveintermediate = TRUE\n)\n\nNow, we’ll inspect each step, both as dataframes and maps. There are many other ways of plotting the outcome data available in the comparer. The goal here is simply to visualize what happens at each step along the way, so we make some quick maps.\nSheet 1- raw data from ewr\nThis is just the input data, so we don’t bother plotting it.\n\nallagg$agg_input\n\n\n  \n\n\n\nSheet 2- aggregated over the time period\nThe first aggregated level is in sheet 2, where the yearly data has been aggregated to the full time period. It is still quite complex, so we again don’t plot it.\n\nallagg$all_time\n\n\n  \n\n\n\nSheet 3- ewr_code\nSheet 3 has the ewr_code_timings aggregated to ewr_code.\n\nallagg$ewr_code\n\n\n  \n\n\n\nThere are many EWR codes, so just pick three haphazardly (LF1, BF1, and CF) and plot to see that this data is at the gauge scale.\n\nallagg$ewr_code |&gt;\n  dplyr::filter(ewr_code %in% c(\"LF1\", \"BF1\", \"CF\")) |&gt;\n  dplyr::left_join(scenarios) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"ewr_code\",\n    sceneorder = c(\"down4\", \"base\", \"up4\"),\n    underlay_list = list(\n      underlay = sdl_units,\n      underlay_pal = \"cornsilk\"\n    )\n  )\n\n\n\n\n\n\n\nSheet 4- env_obj\nSheet 4 has now been aggregated to the env_obj on the theme scale, still gauges spatially.\n\nallagg$env_obj\n\n\n  \n\n\n\nAgain choosing three of the first codes, we see this is still gauged.\n\nallagg$env_obj |&gt;\n  dplyr::filter(env_obj %in% c(\"EF1\", \"WB1\", \"NF1\")) |&gt;\n  dplyr::left_join(scenarios) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    sceneorder = c(\"down4\", \"base\", \"up4\"),\n    underlay_list = list(\n      underlay = sdl_units,\n      underlay_pal = \"cornsilk\"\n    )\n  )\n\n\n\n\n\n\n\nSheet 5- sdl_units\nThe fifth step is a spatial aggregation of env_obj theme-level data into sdl_units. This stays at the env_obj theme scale but aggregates the gauges into sdl units.\n\nallagg$sdl_units\n\n\n  \n\n\n\nNow we have aggregated the data above into sdl polygons.\n\nallagg$sdl_units |&gt;\n  dplyr::filter(env_obj %in% c(\"EF1\", \"WB1\", \"NF1\")) |&gt;\n  dplyr::left_join(scenarios) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    sceneorder = c(\"down4\", \"base\", \"up4\"),\n    underlay_list = list(\n      underlay = sdl_units,\n      underlay_pal = \"grey90\"\n    )\n  )\n\n\n\n\n\n\n\nSheet 6- Specific goal\nSheet 6 is back to the theme axis, aggregating env_obj to Specific goal, remaining in SDL units.\n\nallagg$Specific_goal\n\n\n  \n\n\n\nUsing fct_reorder. this is where info about the scenarios would come in handy as reorder cols.\n\nallagg$Specific_goal |&gt;\n  dplyr::filter(Specific_goal %in% c(\n    \"All recorded fish species\",\n    \"Spoonbills\",\n    \"Decompsition\"\n  )) |&gt;\n  dplyr::left_join(scenarios) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"Specific_goal\",\n    sceneorder = c(\"down4\", \"base\", \"up4\"),\n    underlay_list = list(\n      underlay = sdl_units,\n      underlay_pal = \"grey90\"\n    )\n  )\n\n\n\n\n\n\n\nSheet 7- Objective\nWe are now back to aggregation along the theme axis (from Specific goal to Objective), remaining in cewo_valleys.\n\nallagg$Objective\n\n\n  \n\n\n\nWe see that these are still in the catchments, but now the values are different Objectives.\n\nallagg$Objective |&gt;\n  dplyr::filter(Objective %in% c(\n    \"No loss of native fish species\",\n    \"Increase total waterbird abundance across all functional groups\",\n    \"Support instream & floodplain productivity\"\n  )) |&gt;\n  dplyr::left_join(scenarios) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"Objective\",\n    sceneorder = c(\"down4\", \"base\", \"up4\"),\n    underlay_list = list(\n      underlay = sdl_units,\n      underlay_pal = \"grey90\"\n    )\n  )\n\n\n\n\n\n\n\nSheet 8- Basin\nThis step is a spatial aggregation to the basin scale, with theme remaining at the Objective level. The scaling to the basin is area-weighted, so larger catchments count more toward the basin-scale outcome. Recognize that for this situation with data in only a subset of the basin, aggregation to the whole basin is fraught and is likely biased by missing data.\n\nallagg$basin\n\n\n  \n\n\n\nWe drop the underlay on the plots since we’re now plotting the whole basin\n\nallagg$basin |&gt;\n  dplyr::filter(Objective %in% c(\n    \"No loss of native fish species\",\n    \"Increase total waterbird abundance across all functional groups\",\n    \"Support instream & floodplain productivity\"\n  )) |&gt;\n  dplyr::left_join(scenarios) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"Objective\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n  )\n\n\n\n\n\n\n\nSheet 9- 5-year targets\nFinally, we aggregate along the theme axis to 5-year targets, remaining at the basin-scale spatialy\n\nallagg$target_5_year_2024\n\n\n  \n\n\n\nAnd we’re still at the basin, just plotting different outcomes.\n\nallagg$target_5_year_2024 |&gt;\n  dplyr::filter(target_5_year_2024 %in% c(\n    \"All known species detected annually\",\n    \"Establish baseline data on the number and distribution of wetlands with breeding activity of flow-dependant frog species\",\n    \"Rates of fall does not exceed the 5th percentile of modelled natural rates during regulated water deliveries\"\n  )) |&gt;\n  dplyr::left_join(scenarios) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"target_5_year_2024\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n  )",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#multiple-aggregation-functions",
    "href": "aggregator/using_multi_aggregate.html#multiple-aggregation-functions",
    "title": "Aggregating over multiple dimensions",
    "section": "Multiple aggregation functions",
    "text": "Multiple aggregation functions\nFor EWR outputs, the aggsequence list will typically need to start with a time aggregation, followed by ewr_code_timing and aggregate from there into ewr_code and env_obj as everything else flows from that.\nThe funsequence is a list (instead of a simple vector) because multiple functions can be used at each step. When multiple functions are passed, they are factorial (each function is calculated on the results of all previous aggregations). This keeps the history clean, and allows us to easily unpick the meaning of each value in the output.\nAs an example, we set a range of theme levels that hit all the theme relationship dataframes from the causal networks defined in HydroBOT::causal_ewr, and set the aggregation functions fairly simply, but with two multi-aggregation steps to illustrate how that works. For more complexity in these aggregation functions, see the spatial notebook and aggregation syntax.\nWe use CompensatingFactor as the aggregation function for the ewr_code_timing to ewr_code step here, assuming that passing either timing sub-code means the main code passes. A similar approach could be done if we want to lump the ewr_codes themselves, e.g. put EF4a,b,c,d into EF4. To demonstrate multiple aggregations, we use both ArithmeticMean and LimitingFactor for the 2nd and 3rd levels, showing also how the outputs from those steps get carried through subsequent steps.\n\naggseq_multi &lt;- list(\n  all_time = \"all_time\",\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  env_obj = c(\"ewr_code\", \"env_obj\"),\n  Specific_goal = c(\"env_obj\", \"Specific_goal\"),\n  Objective = c(\"Specific_goal\", \"Objective\"),\n  target_5_year_2024 = c(\"Objective\", \"target_5_year_2024\")\n)\n\nfunseq_multi &lt;- list(\n  all_time = \"ArithmeticMean\",\n  ewr_code = c(\"CompensatingFactor\"),\n  env_obj = c(\"ArithmeticMean\", \"LimitingFactor\"),\n  Specific_goal = c(\"ArithmeticMean\", \"LimitingFactor\"),\n  Objective = c(\"ArithmeticMean\"),\n  target_5_year_2024 = c(\"ArithmeticMean\")\n)\n\n\nmulti_functions &lt;- multi_aggregate(   \n  dat = ewrdata,      \n  causal_edges = causal_ewr,   \n  groupers = \"scenario\",   \n  aggCols = \"ewr_achieved\",\n  aggsequence = aggseq_multi,   \n  funsequence = funseq_multi,\n  auto_ewr_PU = TRUE) \n\nmulti_functions\n\n\n  \n\n\n\nThat output has 4 columns of output values because aggregation steps are factorial in the number of aggregation functions applied. The second step found the ArithmeticMean and LimitingFactor for ewr_achieved into env_obj and then the third step found the ArithmeticMean and LimitingFactor for each of those outcomes into Specific_goal. Each subsequent step only found the ArithmeticMean for each, and so the number of output columns stopped growing.",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#sec-namehistory",
    "href": "aggregator/using_multi_aggregate.html#sec-namehistory",
    "title": "Aggregating over multiple dimensions",
    "section": "Tracking aggregation steps",
    "text": "Tracking aggregation steps\nTracking aggregation steps is critical for knowing the meaning of the numbers produced. We can do that in two different ways- in column headers (names) or in columns themselves.\nTracking history in column names is unweildy, but describes exactly what the numbers are and is smaller in memory. For example, the last data column is\n\nnames(multi_functions)[ncol(multi_functions)-1]\n\n[1] \"target_5_year_2024_ArithmeticMean_Objective_ArithmeticMean_Specific_goal_LimitingFactor_env_obj_LimitingFactor_ewr_code_CompensatingFactor_all_time_ArithmeticMean_ewr_achieved\"\n\n\nThis says the values in this column are the 5-year targets, calculated as the arithmetic mean of Objectives, which were the arithmetic mean of Specific goals, which were calculated from env_obj as limiting factors, which were obtained from the ewr_code as limiting factors, with those were calculated from the ewr_code_timing as compensating factors, which were calculated from the time-average of the original data.\nIt may be easier to think about the meaning of the names from the other direction- ewr_achieved were aggregated from the yearly to the full timeseries as an average, and then ewr_code_timing into ewr_code as Compensating Factors, then into env_obj as limiting factors- for the env_obj to pass, all ewr_codes contributing to it must pass. Then the env_objs were aggregated into Specific_goal, again as limiting factors, so to meet a goal, all contributing env_obj must pass. Those Specific_goals were then aggregated into Objectives with the arithmetic mean, so the value for an Objective is then the average of the contributing Specific_goals. Similarly, the 5-year targets were obtained by averaging the Objectives contributing to them.\nA different way to track the aggregations is possible by including them in columns instead of the names. This takes more memory, but can be clearer and makes subsequent uses easier in many cases. In the example above, we can feed the output data to agg_names_to_cols to put the history in columns instead of names.\n\nagg_names_to_cols(multi_functions, \n                  aggsequence = aggseq_multi, \n                  funsequence = funseq_multi, \n                  aggCols = \"ewr_achieved\")\n\n\n  \n\n\n\nIn practice, what makes most sense is to use a switch (namehistory = FALSE) inside multi_aggregate to return this format.\n\nmulti_functions_history &lt;- multi_aggregate(   \n  dat = ewrdata,      \n  causal_edges = causal_ewr,   \n  groupers = \"scenario\",   \n  aggCols = \"ewr_achieved\",\n  aggsequence = aggseq_multi,   \n  funsequence = funseq_multi,\n  auto_ewr_PU = TRUE,\n  namehistory = FALSE) \n\nmulti_functions_history",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#sec-manual-causal",
    "href": "aggregator/using_multi_aggregate.html#sec-manual-causal",
    "title": "Aggregating over multiple dimensions",
    "section": "User-passed causal networks",
    "text": "User-passed causal networks\nIn most of the demonstrations, we have used the HydroBOT::causal_ewr causal networks. However, the causal network is an argument to multi_aggregate() and read_and_agg(), and so it is possible for the user to pass arbitrary networks. One use that is likely to be useful is to extract the (sometimes newer, but less tested) causal networks from the EWR tool with get_causal_ewr().\n\nnew_ewr_causal &lt;- multi_aggregate(   \n  dat = ewrdata,      \n  causal_edges = get_causal_ewr(),   \n  groupers = \"scenario\",   \n  aggCols = \"ewr_achieved\",\n  aggsequence = aggseq,   \n  funsequence = funseq,\n  auto_ewr_PU = TRUE,\n  namehistory = FALSE) \n\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`.\n\n\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`\n.\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`\n.\nℹ EWR gauges joined to larger units pseudo-spatially.\n• Done automatically because `auto_ewr_PU = TRUE`\n• Non-spatial join needed because gauges may inform areas they are not within\n• Best to explicitly use `pseudo_spatial = 'sdl_units'` in `multi_aggregate()` or `read_and_agg()`.\n\n! Unmatched links in causal network\n• 11 from env_obj to Specific_goal\n! Unmatched links in causal network\n• 7 from Objective to target_5_year_2024\n\nnew_ewr_causal\n\n\n  \n\n\n\nIt is also possible to use any arbitrary network with the needed links (columns). Here, we make up a very simple one. See causal_ewr for needed structure; the main key is it needs to be a list of dataframe(s).\n\nfakegroups &lt;- c('a', 'b', 'c')\nfake_causal &lt;- tibble::tibble(ewr_code_timing = unique(ewrdata$ewr_code_timing),\n                              fake_group = sample(fakegroups,\n                                                  length(unique(ewrdata$ewr_code_timing)), \n                                                  replace = TRUE))\n\n\naggseq_fakecausal &lt;- list(\n  all_time = 'all_time',\n  fake_group = c(\"ewr_code_timing\", \"fake_group\"),\n  sdl_units = sdl_units\n)\n\nfunseq_fakecausal &lt;- list(\n  all_time = 'ArithmeticMean',\n  fake_group = \"CompensatingFactor\",\n  sdl_units = \"ArithmeticMean\"\n  )\n\nfake_causal_agg &lt;- multi_aggregate(   \n  dat = ewrdata,      \n  causal_edges = list(fake_causal),   \n  groupers = \"scenario\",   \n  aggCols = \"ewr_achieved\",\n  aggsequence = aggseq_fakecausal,   \n  funsequence = funseq_fakecausal,\n  auto_ewr_PU = TRUE,\n  namehistory = FALSE) \n\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`.\n\n\nWarning in filtergroups(thisdf, fromcol = p[1], tocol = p[2], fromfilter =\nfromfilter, : Unable to cross-check gauges and planning units, trusting the\nuser they work together\n\nfake_causal_agg\n\n\n  \n\n\n\nAnd a quick plot of the random groupings implied there.\n\nfake_causal_agg |&gt; \n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"fake_group\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n  )",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#sec-manual-spatial",
    "href": "aggregator/using_multi_aggregate.html#sec-manual-spatial",
    "title": "Aggregating over multiple dimensions",
    "section": "User-passed spatial data",
    "text": "User-passed spatial data\nLike the causal networks, users can pass arbitrary spatial units to multi_aggregate() and read_and_agg(), and are not limited to those provided by HydroBOT. The only requirement is that they are sf objects with a geometry column. To demonstrate, we’ll download Australian states and aggregate into those (and the fake causal network above).\n\naustates &lt;- rnaturalearth::ne_states(country = 'australia') |&gt; \n  dplyr::select(state = name, geometry)\n\n\naggseq_states &lt;- list(\n  all_time = 'all_time',\n  fake_group = c(\"ewr_code_timing\", \"fake_group\"),\n  state = austates\n)\n\nstate_agg &lt;- multi_aggregate(   \n  dat = ewrdata,      \n  causal_edges = list(fake_causal),   \n  groupers = \"scenario\",   \n  aggCols = \"ewr_achieved\",\n  aggsequence = aggseq_states,   \n  funsequence = funseq_fakecausal,\n  namehistory = FALSE) \n\nWarning: ! EWR outputs detected without `group_until`!\nℹ EWR outputs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\nℹ Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`.\nℹ Lower-level processing should include as `grouper` in `temporal_aggregate()`\n\n\nWarning in filtergroups(thisdf, fromcol = p[1], tocol = p[2], fromfilter =\nfromfilter, : Unable to cross-check gauges and planning units, trusting the\nuser they work together\n\nstate_agg\n\n\n  \n\n\n\nAnd a quick plot of that to see the different spatial units (though this example is all in New South Wales).\n\nstate_agg |&gt; \n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"fake_group\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n  )",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#sec-manual-functions",
    "href": "aggregator/using_multi_aggregate.html#sec-manual-functions",
    "title": "Aggregating over multiple dimensions",
    "section": "User-passed functions",
    "text": "User-passed functions\nSee the aggregation syntax for a full treatment of the way functions can be specified. The most stable and best practice for specifying aggregation functions is to specify them as a named function, and supply that to the funsequence. Copying what’s at that page,\nWe demonstrate here with a threshold function and a median. For example, we might want to know the mean of all values greater than 0 for some stages and use the median at others.\n\nmean_given_occurred &lt;- function(x) {\n  mean(ifelse(x &gt; 0, x, NA), na.rm = TRUE)\n}\n\nmedna &lt;- function(x) {\n  median(x, na.rm = TRUE)\n}\n\naggseq_funs &lt;- list(\n  all_time = 'all_time',\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  env_obj = c(\"ewr_code\", \"env_obj\"),\n  sdl_units = sdl_units,\n  Target = c(\"env_obj\", \"Target\")\n  )\n\n\nfunseq_funs &lt;- list(\n  all_time = 'ArithmeticMean',\n  ewr_code = 'ArithmeticMean',\n  env_obj = 'mean_given_occurred',\n  sdl_units = 'medna',\n  Target = 'mean_given_occurred'\n)\n\nThose changes are then reflected in the aggregation history and determine the aggregated values.\n\nagged_custom_funs &lt;- multi_aggregate(\n  dat = ewrdata,\n  causal_edges = causal_ewr,\n  groupers = c(\"scenario\"),\n  aggCols = \"ewr_achieved\",\n  aggsequence = aggseq_funs,\n  funsequence = funseq_funs,\n  namehistory = FALSE\n)\n\nWe can plot them to double check it worked.\n\nagged_custom_funs |&gt; \n  dplyr::filter(scenario != 'MAX')  |&gt;  \n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    y_lab = \"Arithmetic Mean\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    facet_col = \"Target\",\n    facet_row = \"scenario\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n)",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#dimensional-information",
    "href": "aggregator/using_multi_aggregate.html#dimensional-information",
    "title": "Aggregating over multiple dimensions",
    "section": "Dimensional information",
    "text": "Dimensional information\nThe multi_aggregate function handles dimensional information about space, time, and theme. It does this dependent on knowing theme levels from a causal network, having a time column, and having geographic information. Thus, it is general, and will run fine without this information, e..g a dataframe without a geometry column will not trigger spatial checks, and one without a column in a time format will not trigger temporal checks. That raises some issues, however, in that if a dataframe should have those special columns, the dimensional safety will be lost. Thus, multi_aggregate doesn’t care what theme, temporal, or spatial scale its input data is- e.g. we could give it Objectives already at the Catchment scale, and then use it to move up.",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#syntax-for-arguments",
    "href": "aggregator/using_multi_aggregate.html#syntax-for-arguments",
    "title": "Aggregating over multiple dimensions",
    "section": "Syntax for arguments",
    "text": "Syntax for arguments\nThe groupers and aggCols arguments can take a number of different formats- character vectors, bare column names and sometimes tidyselect, though this is more limited for multi_aggregate than the dimension-specific aggregations. Moreover, functions can be passed as characters, bare names, and anonymous functions in lists. For more detail, see the syntax documentation.",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#using-multi_aggregate-for-one-aggregation",
    "href": "aggregator/using_multi_aggregate.html#using-multi_aggregate-for-one-aggregation",
    "title": "Aggregating over multiple dimensions",
    "section": "Using multi_aggregate for one aggregation",
    "text": "Using multi_aggregate for one aggregation\nIf we’re only using one level of aggregation and nothing else, there may not be need for the multi_aggregate() wrapper, though in a formal analysis the safety provided by multi_aggregate() and even read_and_agg() are likely worth it. These wrappers do work even for single steps though. We do have a bit less flexibility with how we specify arguments, see syntax documentation.\nTypically we could use namehistory = FALSE to avoid the horrible long name with all the transforms in it, but there’s no way for it to know the previous aggregation history when it’s been done in pieces (as we do in each of the theme, spatial, and temporal examples. As we do in those examples, it is possible to use agg_names_to_cols() with the full sequence including the earlier steps to extract them.",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#value-to-aggregate",
    "href": "aggregator/using_multi_aggregate.html#value-to-aggregate",
    "title": "Aggregating over multiple dimensions",
    "section": "Value to aggregate",
    "text": "Value to aggregate\nWe have demonstrated everything here by aggregating EWR data with the ewr_achieved column. However, any numeric column can be aggregated, as we show in Section 1.10 for made up module outputs. Here, we also show aggregation on a different EWR metric, the achivement of interevent requirements:\n\nallagg_interevent &lt;- multi_aggregate(\n  dat = ewrdata,\n  causal_edges = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"interevent_achieved\",\n  aggsequence = aggseq,\n  funsequence = funseq,\n  auto_ewr_PU = TRUE,\n  namehistory = FALSE,\n  saveintermediate = TRUE\n)\n\nWe’ll show that worked for just one sheet (the sdl units).\n\nallagg_interevent$sdl_units |&gt;\n  dplyr::filter(env_obj %in% c(\"EF1\", \"WB1\", \"NF1\")) |&gt;\n  dplyr::left_join(scenarios) |&gt;\n  plot_outcomes(\n    outcome_col = \"interevent_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"interevent_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    sceneorder = c(\"down4\", \"base\", \"up4\"),\n    underlay_list = list(\n      underlay = sdl_units,\n      underlay_pal = \"grey90\"\n    )\n  )\n\nJoining with `by = join_by(scenario)`",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "aggregator/using_multi_aggregate.html#sec-dummy-module",
    "href": "aggregator/using_multi_aggregate.html#sec-dummy-module",
    "title": "Aggregating over multiple dimensions",
    "section": "Using un-integrated response models",
    "text": "Using un-integrated response models\nIn some cases, response models may not yet be integrated into HydroBOT, or may not be able to be integrated (e.g. if they are proprietary or unscriptable). In these cases, we can still use multi_aggregate() for processing with the Aggregator and then Comparer. All we need is a dataframe. For aggregation along the theme, space, and time dimensions, it needs to have information about those dimensions (i.e. a time column, a geometry column, and a column matching something in its relevant causal network). It will typically have a ‘scenario’ column as well.\nSetting up non-module data and relationships\nTo demonstrate, we will create some dummy data in the Australian states.\nWe need some values in different years.\n\n  # add a date column\n  state_inputs &lt;- austates |&gt;\n    dplyr::mutate(date = lubridate::ymd('20000101'))\n\n  # add some values\n  withr::with_seed(17,\n                   state_inputs &lt;- state_inputs |&gt;\n                     dplyr::mutate(value = runif(nrow(state_inputs)))\n  )\n\n  withr::with_seed(17,\n                   # add some more days, each with different values\n                   state_inputs &lt;- purrr::map(0:10,\n                                              \\(x) dplyr::mutate(state_inputs,\n                                                                 date = date + x,\n                                                                 value = value * rnorm(nrow(state_inputs),\n                                                                                       mean = x, sd = x/2))) |&gt;\n                     dplyr::bind_rows()\n                   )\n\nWe need some scenarios too.\n\n  # add a scenario column, each with different values\n  state_inputs &lt;- purrr::imap(letters[1:4],\n                              \\(x,y) dplyr::mutate(state_inputs,\n                                            scenario = x,\n                                            value = value + y)) |&gt;\n    dplyr::bind_rows()\n\nWe need some dummy ‘theme’ groupings and a network.\n\n  # add a theme-relevant column, each with different values\n  state_inputs &lt;- purrr::imap(c(\"E\", \"F\", \"G\", \"H\", \"I\", \"J\"),\n                              \\(x,y) dplyr::mutate(state_inputs,\n                                            theme1 = x,\n                                            value = value + y)) |&gt;\n    dplyr::bind_rows()\n\n# make a simple 'causal' network\n  state_theme &lt;- tibble::tibble(theme1 = c(\"E\", \"F\", \"G\", \"H\", \"I\", \"J\"),\n                                theme2 = c(\"vowel\", \"consonant\", \"consonant\",\n                                           \"consonant\", \"vowel\", \"consonant\")) |&gt; \n    list()\n\nAnd finally, we will provide a larger spatial unit to aggregate into, all of Australia (though this loses the islands).\n\nall_aus &lt;- rnaturalearth::ne_countries(country = 'australia') |&gt; \n  dplyr::select(geounit)\n\nUsing the Aggregator\nFirst, we set up some aggregation steps. Will just use means throughout.\n\n# This will aggregate into weeks, then to type, and then to the country.\n  ausseq &lt;- list(\n    week = 'week',\n    theme2 = c('theme1', 'theme2'),\n    all_aus = all_aus\n  )\n\n  # just use mean, since there are no NA in the data.\n  ausfuns &lt;- list(\n    week = 'mean',\n    type = 'mean',\n    all_aus = 'mean'\n  )\n\nDo the aggregation\n\n  # Do the aggregation\n  ausagg &lt;-  multi_aggregate(\n    dat = state_inputs,\n    causal_edges = state_theme,\n    groupers = \"scenario\",\n    aggCols = \"value\",\n    aggsequence = ausseq,\n    funsequence = ausfuns,\n    namehistory = FALSE,\n    saveintermediate = TRUE\n  )\n\nWarning in filtergroups(thisdf, fromcol = p[1], tocol = p[2], fromfilter =\nfromfilter, : Unable to cross-check gauges and planning units, trusting the\nuser they work together\n\n\nQuick plots of a couple levels\n\nausagg$week |&gt; \n  filter(theme1 == 'E') |&gt; \n    plot_outcomes(\n    outcome_col = \"value\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"value\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"date\"\n  )\n\n\n\n\n\n\n\n\nausagg$theme2 |&gt; \n  filter(date == lubridate::ymd(\"2000-01-03\")) |&gt; \n    plot_outcomes(\n    outcome_col = \"value\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"value\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"theme2\"\n  )\n\n\n\n\n\n\n\n\nausagg$all_aus |&gt; \n  filter(date == lubridate::ymd(\"2000-01-03\")) |&gt; \n    plot_outcomes(\n    outcome_col = \"value\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"value\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"theme2\"\n  )",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Multi-axis interleaved aggregation"
    ]
  },
  {
    "objectID": "comparer/bar_plots.html",
    "href": "comparer/bar_plots.html",
    "title": "Bar plots",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Bar plots (qual x)"
    ]
  },
  {
    "objectID": "comparer/bar_plots.html#read-in-the-data",
    "href": "comparer/bar_plots.html#read-in-the-data",
    "title": "Bar plots",
    "section": "Read in the data",
    "text": "Read in the data\nWe read in the example data we will use for all plots.\n\nagged_data &lt;- readRDS(file.path(agg_dir, \"achievement_aggregated.rds\"))\n\nThat has all the steps in the aggregation, but most of the plots here will only use a subset to demonstrate.\nTo make visualisation easier, the SDL units data is given a grouping column that puts the many env_obj variables in groups defined by their first two letters, e.g. EF for Ecosystem Function. These correspond to the ‘Target’ level, but it can be useful to have the two groupings together for some examples.\nIf we had used multiple aggregation functions at any step, we should filter down to the one we want here, but we only used one for this example.\nFor simplicity here, we will only look at a small selection of the scenarios (multiplicative changes of 0.5,1, and 2). Thus, we make two small dataframes for our primary examples here.\n\nscenarios_to_plot &lt;- c('climatedown2adapt0', 'climatebaseadapt0', 'climateup2adapt0')\n\nscenarios &lt;- yaml::read_yaml(file.path(hydro_dir, 'scenario_metadata.yml')) |&gt; \n  tibble::as_tibble()\n\nbasin_to_plot &lt;- agged_data$mdb |&gt; \n  dplyr::filter(scenario %in% scenarios_to_plot) |&gt; \n  dplyr::left_join(scenarios, by = 'scenario')\n\n# Create a grouping variable\nobj_sdl_to_plot &lt;- agged_data$sdl_units |&gt;\n    dplyr::filter(scenario %in% scenarios_to_plot) |&gt; \n  dplyr::mutate(env_group = stringr::str_extract(env_obj, \"^[A-Z]+\")) |&gt;\n  dplyr::arrange(env_group, env_obj) |&gt; \n  dplyr::left_join(scenarios, by = 'scenario')",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Bar plots (qual x)"
    ]
  },
  {
    "objectID": "comparer/bar_plots.html#scenario-fills",
    "href": "comparer/bar_plots.html#scenario-fills",
    "title": "Bar plots",
    "section": "Scenario fills",
    "text": "Scenario fills\nBasin scale\nWe can make plots looking at how scenarios differ for each of the outcome categories for a simple case of only one outcome. This uses facet_wrapper to just wrap the single facet axis.\nThe colorset argument is the column that determines color, while the pal_list defines those colors, here as a named colors object, but as we see below it can also be palette names.\n\nplot_outcomes(basin_to_plot,\n  outcome_col = \"ewr_achieved\",\n  x_col = 'climate_code',\n  facet_wrapper = \"Target\",\n  colorset = \"climate_code\",\n  pal_list = scene_pal,\n  sceneorder = sceneorder\n)\n\n\n\n\n\n\n\nHydroBOT retains the axis names as-is from the incoming dataframe, as they provide the true meaning of each value. But we can change them, either inside the plot_outcomes() function (here for y) or post-hoc with ggplot2::labs() (here for x). We can also set the sceneorder with a character vector if that’s easier than setting up a Factor or if we want to change them around for some reason. Because the outputs of plot_outcomes() are just ggplot objects, changing the labels outside the function can be very useful for checking that each axis is in fact what we think it is before giving it clean labels.\n\nplot_outcomes(basin_to_plot,\n  outcome_col = \"ewr_achieved\",\n  y_lab = \"Proportion Objectives\\nAchieved\",\n  x_col = 'climate_code',\n  color_lab = \"Scenario\",\n  facet_wrapper = \"Target\",\n  colorset = \"climate_code\",\n  pal_list = scene_pal,\n  sceneorder = c('climateup2adapt0', 'climatebaseadapt0', 'climatedown2adapt0')\n) +\n  labs(x = 'Scenario')\n\n\n\n\n\n\n\nAnother approach is to put other groupings on the x-axis, and color by scenario.\n\nplot_outcomes(basin_to_plot,\n  outcome_col = \"ewr_achieved\",\n  x_col = \"Target\",\n  colorset = \"climate_code\",\n  pal_list = scene_pal,\n  sceneorder = sceneorder\n)\n\n\n\n\n\n\n\nWe can pass position = 'dodge' to use dodged bars for clearer comparisons, particularly accentuating the variation in sensitivity of the different outcomes to the scenarios.\n\nplot_outcomes(basin_to_plot,\n  outcome_col = \"ewr_achieved\",\n  x_col = \"Target\",\n  colorset = \"climate_code\",\n  pal_list = scene_pal,\n  sceneorder = sceneorder,\n  position = 'dodge'\n)\n\n\n\n\n\n\n\nSDL units\nWe can use the aggregation step of env_obj and SDL units to demonstrate plotting that not only addresses the outcomes for scenarios, but how they differ across space.\nFirst, we look at how the different scenarios perform for the Ecosystem Function objectives in each SDL unit. We also use the ggplot2 functionality to remove the x-axis label, since it is redundant with the color.\n\n\n\n\n\n\nTip\n\n\n\nOften when we have multiple dimensions, we’ll want to do a simple filter to relevant subsets of the data for readability. When this filtering occurs, it is almost always a good idea to do it on the fly (as here), to avoid errors associated with losing track of data manipulations and instead start each figure with the full dataset.\n\n\n\nobj_sdl_to_plot |&gt;\n  filter(grepl(\"^EF\", env_obj)) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    facet_col = \"env_obj\",\n    facet_row = \"SWSDLName\",\n    colorset = \"climate_code\",\n    pal_list = scene_pal,\n    sceneorder = sceneorder\n  ) +\n  theme(axis.text.x = element_blank(), axis.title.x = element_blank())\n\n\n\n\n\n\n\nWe address a few ways to handle groups of outcome variables, one of the simplest is to simply facet these plots by those groups, with all the outcomes in the group getting their own bars. This puts the theme levels on x and colors by scenario, with the groups accentuated by facets. These can be stacked (position = 'stack'- the default) or dodged (demonstrated here).\n\ndodgefacet &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = \"env_obj\",\n    colorset = \"climate_code\",\n    facet_row = \"SWSDLName\",\n    facet_col = \"env_group\",\n    scales = \"free_x\",\n    pal_list = scene_pal,\n    sceneorder = sceneorder,\n    position = \"dodge\"\n  )\n\ndodgefacet + theme(legend.position = \"bottom\") +\n  labs(x = \"Environmental Objective\")",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Bar plots (qual x)"
    ]
  },
  {
    "objectID": "comparer/bar_plots.html#grouped-colors",
    "href": "comparer/bar_plots.html#grouped-colors",
    "title": "Bar plots",
    "section": "Grouped colors",
    "text": "Grouped colors\nHydroBOT can assign different color palettes to different sets of outcomes, yielding what is essentially another axis on which we can plot information. We use this same ability across a number of plot types, particularly causal networks. For example, we might categorize the env_obj outcomes into the larger scale groups (e.g. ‘NF’, ‘EF’, etc). We can then assign each of these a separate palette, and so the individual env_objs get different colors chosen from different palettes.\nAchieving this requires specifying two columns- the colorset, as above, is the column that determines color. The colorgroups column specifies the groupings of those colorset values, and so what palette to use. Thus, the pal_list needs to be either length 1 (everything gets the same palette) or length(unique(data$colorgroups)). Note also that the colorset values must be unique to colorgroups- this cannot be a one-to-many mapping because each colorset value must get a color from a single palette defined by the colorgroup it is in.\nWe demonstrate with env_obj variables mapped to larger environmental groups, making it easier to see at a glance the sorts of environmental objectives that are more or less affected, while also allowing views of the individual environmental objectives. Here we use facet_col and facet_row to ensure the SDL units don’t wrap around. We made the env_groups column when we chose the data initially.\n\n# Create a palette list\nenv_pals &lt;- list(\n  EF = \"grDevices::Purp\",\n  NF = \"grDevices::Mint\",\n  NV = \"grDevices::Burg\",\n  OS = \"grDevices::Blues\",\n  WB = \"grDevices::Peach\"\n)\n\n# need to facet by space sdl unit and give it the colorgroup argument to take multiple palettes\nobj_sdl_to_plot |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = 'climate_code',\n    colorgroups = \"env_group\",\n    colorset = \"env_obj\",\n    pal_list = env_pals,\n    facet_col = \"SWSDLName\",\n    facet_row = \".\"\n  ) +\n  theme(legend.key.size = unit(0.5, 'cm'))\n\n\n\n\n\n\n\nAdding facetting by those groups can make that easier to read if the goal is to focus on changes within groups, but more plots.\n\nobj_sdl_to_plot |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = 'climate_code',\n    colorgroups = \"env_group\",\n    colorset = \"env_obj\",\n    pal_list = env_pals,\n    facet_col = \"SWSDLName\",\n    facet_row = \"env_group\"\n  ) + \n  theme(legend.key.size = unit(0.5, 'cm'))\n\n\n\n\n\n\n\nWe could also split those bars sideways instead of stack them, but that likely makes more sense if there are fewer categories than here. We again use position = 'dodge', but now we don’t need to sum because we’re stacking each row already. I’ve flipped the facetting and taken advantage of the fact that these are just ggplot objects to remove the legend, making it very slightly easier to read (but harder to interpret). This gets very crowded with the full set of scenarios, so we can use the scenariofilter argument to cut it to just a few (here, base and multiply and divide by 1.5).\n\nobj_sdl_to_plot |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = 'climate_code',\n    colorgroups = \"env_group\",\n    colorset = \"env_obj\",\n    pal_list = env_pals,\n    facet_col = \"SWSDLName\",\n    facet_row = \"env_group\",\n    position = 'dodge'\n  ) +\n  theme(legend.key.size = unit(0.5, 'cm'))\n\n\n\n\n\n\n\nAnother approach to groups of outcomes without the colors explicitly grouped is to not use colorgroup, but instead just facet by the group and give every colorset value a color from the same palette. Depending on the palette chosen and the breaks, this can be quicker, but will not accentuate groups as well.\n\nobj_sdl_to_plot |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    colorgroups = NULL,\n    colorset = \"env_obj\",\n    pal_list = list(\"scico::berlin\"),\n    facet_row = \"SWSDLName\",\n    facet_col = \"env_group\",\n    scales = \"free_x\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\n\n\n\n\n\nThese plots are interesting, but in typical use, the plots above using facets for the groups or coloring by the groups themselves are likely to be easier to read, unless we really are interested in this level of granularity. Whatever approach we choose for a given plot, accentuating the differences between outcome groups can be a powerful interpretation tool.",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Bar plots (qual x)"
    ]
  },
  {
    "objectID": "comparer/bar_plots.html#manual-color-definition",
    "href": "comparer/bar_plots.html#manual-color-definition",
    "title": "Bar plots",
    "section": "Manual color definition",
    "text": "Manual color definition\nThough the above examples using {paletteer} palettes are the easiest way to specify coloring, we don’t have to let the palettes auto-choose colors, and can instead pass colors objects, just as we do for scenarios. This can be particularly useful with small numbers of groups (defining too many colors is cumbersome- that’s what palettes are for) when we want to control which is which. Just as with scenarios, we use make_pal(). Here, we will use 'scico::berlin' as the base, but define several ‘reference’ values manually. This demonstration uses includeRef = TRUE so we replace the palette values with the refs, rather than choose them from the set of values with refs removed. This tends to yield better spread of colors (and lets us sometimes ref colors and sometimes not if we also used returnUnref). For example, maybe we want to sometimes really accentuate ecosystem function and native vegetation, but not in all plots.\nFirst, we create the palettes with and without the (garish) ref values.\n\nobj_pal &lt;- make_pal(\n  levels = unique(obj_sdl_to_plot$env_group),\n  palette = \"scico::lisbon\",\n  refvals = c(\"EF\", \"NV\"), refcols = c(\"purple\", \"orange\"), includeRef = TRUE, returnUnref = TRUE\n)\n\nThen we can create an accentuated plot sometimes, if, perhaps, we want to highlight how EF performed.\n\nplot_outcomes(obj_sdl_to_plot,\n  outcome_col = \"ewr_achieved\",\n  x_col = 'climate_code',\n  colorset = \"env_group\",\n  pal_list = obj_pal$refcols,\n  facet_col = \"SWSDLName\",\n  facet_row = \".\",\n  sceneorder = sceneorder\n) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\n\n\n\n\n\nBut for other plots maybe we don’t want that accentuation and we can use the unrefcols to retain the standard coloring- note that ‘NF’, ‘OS’, and ‘WB’ colors are unchanged.\n\nplot_outcomes(obj_sdl_to_plot,\n  outcome_col = \"ewr_achieved\",\n  x_col = 'climate_code',\n  colorset = \"env_group\",\n  pal_list = obj_pal$unrefcols,\n  facet_col = \"SWSDLName\",\n  facet_row = \".\",\n  sceneorder = sceneorder\n) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Bar plots (qual x)"
    ]
  },
  {
    "objectID": "aggregator/aggregation_syntax.html",
    "href": "aggregator/aggregation_syntax.html",
    "title": "Aggregation functions and grouping",
    "section": "",
    "text": "library(HydroBOT)\nTo prepare, we generate some EWR outputs to use as example data.\nproject_dir &lt;- 'hydrobot_scenarios'\nhydro_dir &lt;- file.path(project_dir, 'hydrographs')\newr_results &lt;- file.path(project_dir, \"module_output\", \"EWR\")\n\newr_out &lt;- prep_run_save_ewrs(\n  hydro_dir = hydro_dir,\n  output_parent_dir = project_dir,\n  outputType = list('none'),\n  returnType = list('yearly')\n)\n\nThe function prep_run_save_ewrs() has run, but the scenario(s) base_module_output_EWR_base have failed and have been bypassed after 2 retries.\n• The function prep_run_save_ewrs() has run, but the scenario(s) base_module_output_EWR_base have failed and have been bypassed after 2 retries.\n• The first error is:\n• Index Date invalid\n\n# This is just a simple prep step that is usually done internally to put the geographic coordinates on input data\nsumdat &lt;- prep_ewr_output(ewr_out$yearly, type = 'achievement')",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Aggregation syntax"
    ]
  },
  {
    "objectID": "aggregator/aggregation_syntax.html#selecting-grouping-and-data-columns",
    "href": "aggregator/aggregation_syntax.html#selecting-grouping-and-data-columns",
    "title": "Aggregation functions and grouping",
    "section": "Selecting grouping and data columns",
    "text": "Selecting grouping and data columns\nBoth aggCols and groupers can be character vectors, bare data-variable names, or we might want to use tidyselect syntax. For example, maybe we want to use ends_with('ewr_achieved') to grab pre-aggregated columns with long name histories or starts_with('scenar') instead of specifying 'scenario' as a grouper. This is handled under the hood by selectcreator and careful parsing in the function stack. In general, the safest thing to use is characters.\nHere, we use a combination of tidyselect and bare column names for groupers, a character for the aggCols, and bare function name for the function to apply.\n\n\n\n\n\n\nTip\n\n\n\nmulti_aggregate takes advantage of this tidyselect ability under the hood to deal with the ever-lengthening column names (and sometimes expanding number of value columns if we have multiple aggregation functions at a step). This means, though, that using tidyselect to specify aggCols in multi_aggregate or read_and_agg is fragile. We try to handle it by changing to character, but it can collide with the internal tidyselect, and so characters are a safer option in those outer functions.\nSimilarly, multi_aggregate and read_and_agg have to attempt to extract the character name from bare functions. If this extraction fails, they get lost for the purposes of naming the history by the time they get used in the call stack. So again, characters are safer.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe need to specify env_obj as a grouper here because we are using spatial_aggregate() directly; in normal use with multi_aggregate() or read_and_agg() the dimensional safety will enforce that grouping in the value dimension while doing spatial aggregation.\n\n\nThis throws warnings about its meaning for the EWRs; since the point here is to demonstrate functionality, we will not print them.\n\nobj2poly2 &lt;- spatial_aggregate(\n  dat = simpleThemeAgg,\n  to_geo = sdl_units,\n  groupers = c(starts_with(\"sce\"), env_obj),\n  aggCols = \"env_obj_ArithmeticMean_ewr_code_CompensatingFactor_all_time_ArithmeticMean_ewr_achieved\",\n  funlist = ArithmeticMean,\n  keepAllPolys = TRUE\n)\n\nobj2poly2\n\n\n  \n\n\n\nWe can see we get the same result as obj2poly with different ways of specifying aggCols and groupers.\nThere are times when we might want to send a vector of names, but ignore those not in the data. This typically occurs when there is a set of possible grouping variables but only some exist in a particular dataset, so they should be used if present and ignored if not. It fails by default, because in normal use datasets would match, but setting failmissing = FALSE allows it to pass. Here, the ‘extra_grouper’ column doesn’t exist in the data and so is ignored. The failmissing argument also applies to aggCols; if it is TRUE, aggCols can have values that are not column names in the data.\nWe also now use only characters for groupers, but switch to tidyselect to specify aggCols.\n\nobj2polyF &lt;- spatial_aggregate(\n  dat = simpleThemeAgg,\n  to_geo = sdl_units,\n  groupers = c(\"scenario\", \"env_obj\", \"extra_grouper\"),\n  aggCols = ends_with(\"ewr_achieved\"),\n  funlist = ArithmeticMean,\n  keepAllPolys = TRUE,\n  failmissing = FALSE\n)\nobj2polyF",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Aggregation syntax"
    ]
  },
  {
    "objectID": "aggregator/aggregation_syntax.html#sec-funsyntax",
    "href": "aggregator/aggregation_syntax.html#sec-funsyntax",
    "title": "Aggregation functions and grouping",
    "section": "Function specifications",
    "text": "Function specifications\nWe can pass single bare aggregation function names, characters, or named lists defining functions with arguments (though this is fragile). We can also pass multiple functions to apply; for example we might want to calculate the mean, max, and min all at the same time.\nAbove, we have been specifying the function to apply as just a single bare function name. Now, we explore some other possibilities and capabilities of the aggregator.\nMost simply, we can pass character names of functions instead of bare\n\ndoublesimplechar &lt;- spatial_aggregate(\n  dat = simpleThemeAgg,\n  to_geo = sdl_units,\n  groupers = c(\"scenario\", \"env_obj\"),\n  aggCols = ends_with(\"ewr_achieved\"),\n  funlist = \"ArithmeticMean\",\n  keepAllPolys = TRUE,\n  failmissing = FALSE\n)\ndoublesimplechar\n\n\n  \n\n\n\nIf we want to do two different aggregations on the same data, we can pass a vector of names. Now we have two output columns, starting with spatial_ArithmeticMean_... and spatial_GeometricMean... .\n\nsimplefuns &lt;- c(\"ArithmeticMean\", \"GeometricMean\")\n\ndoublesimplec &lt;- spatial_aggregate(\n  dat = simpleThemeAgg,\n  to_geo = sdl_units,\n  groupers = c(\"scenario\", \"env_obj\"),\n  aggCols = ends_with(\"ewr_achieved\"),\n  funlist = simplefuns,\n  keepAllPolys = TRUE,\n  failmissing = FALSE\n)\ndoublesimplec\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf passing multiple functions, it is unlikely to work to pass bare names. It just gets too complex to handle, and the bare names is really just a convenience shorthand.\n\n\nWe can also use anonymous (lambda) functions in a list. This example is trivial, but it becomes useful (though still rarely a good idea in production) in the next section. It does not work with more modern \\(x) lambda functions. Given the fragility of this approach, it has not been a high priority to get working.\n\nlambda1 &lt;- spatial_aggregate(\n  dat = simpleThemeAgg,\n  to_geo = sdl_units,\n  groupers = c(\"scenario\", \"env_obj\"),\n  aggCols = ends_with(\"ewr_achieved\"),\n  funlist = list(mean = ~mean(.)),\n  keepAllPolys = TRUE,\n  failmissing = FALSE\n)\nlambda1\n\n\n  \n\n\n\nArguments to aggregation functions\nThere are three primary ways to specify function arguments- using ... in spatial_aggregate, writing a wrapper function with the arguments specified (e.g. see ArithmeticMean, which is just mean(x, na.rm = TRUE)), or using anonymous functions with ~ syntax in a named list. The simplest version is to use …, but this really only works in simple cases, like passing na.rm = TRUE. It does work for multiple functions, but starts getting convoluted and unclear if they don’t share arguments or there are many arguments. Thus, the best option is to specify a custom function, and pass it.\nHere, we pass na.rm = TRUE to both mean and sd using ....\n\nsinglearg &lt;- spatial_aggregate(\n  dat = simpleThemeAgg,\n  to_geo = sdl_units,\n  groupers = \"scenario\",\n  aggCols = ends_with(\"ewr_achieved\"),\n  funlist = c(mean, sd),\n  na.rm = TRUE,\n  keepAllPolys = TRUE,\n  failmissing = FALSE\n)\nsinglearg\n\n\n  \n\n\n\nWe can also pass arguments by sending a list of lambda functions with their arguments. This is far more flexible than the … approach, as we can send any arguments to any functions this way. For clarity, we demonstrate it here for the same situation- passing the na.rm argument to mean and sd. This also lets us control the function names, because the list-names do not need to match the function names. The list-names are what get used in history-tracking (see the column names).\n\nsimplelamfuns &lt;- list(\n  meanna = ~ mean(., na.rm = TRUE),\n  sdna = ~ sd(., na.rm = TRUE)\n)\n\ndoublelam &lt;- spatial_aggregate(\n  dat = simpleThemeAgg,\n  to_geo = sdl_units,\n  groupers = c(\"scenario\", \"env_obj\"),\n  aggCols = ends_with(\"ewr_achieved\"),\n  funlist = simplelamfuns,\n  keepAllPolys = TRUE,\n  failmissing = FALSE\n)\ndoublelam\n\n\n  \n\n\n\nNote: if using anonymous functions in a list this way, they need to use rlang ~ syntax, not base \\(x) or function(x){}. Given the fragility of this approach, that hasn’t been a priority for implementation. It’s hopefully not much of a constraint, and anything complex can be written as a standard function and called that way.\nVector arguments\nSee Section 2.2.4 for the only real way to handle vector arguments in HydroBOT workflows. The first part of this deals with how to use vector arguments with [spatial_aggregate()], [theme_aggregate()], and [temporal_aggregate()] directly. If vector arguments other than area are needed in future, HydroBOT will need to accept the vectors themselves as arguments.\nUsing aggregation functions directly\nIt’s fairly common that we’ll have vector arguments, especially for the spatial aggregations. One primary example is weightings. The most flexible approach requires these vectors to be attached to the data before it enters the function (vs. creating them automatically in-function). That works well for one-off use of e.g. spatial_aggregate, but is tricky to make happen in an automated workflow though.\nFirst, we illustrate how it works as a one-off, assuming that the vectors are columns in the dataset, demonstrating with weighted means on dummy weights.\n\n\n\n\n\n\nCaution\n\n\n\nAs of {dplyr} 1.1, if we pass a function with a data-variable argument (e.g. the name of a column in the dataframe) we have to wrap the list in rlang::quo. Otherwise it looks for an object with that name instead of a column. The better solution is to use custom function, not a lambda (see below in Section 2.2.4. If we have several layers of aggregation, the inside level where the function is defined needs to be wrapped. E.g.\nfunlist &lt;- list(c('ArithmeticMean', 'LimitingFactor'),\n                rlang::quo(list(wm = ~weighted.mean(., area, na.rm = TRUE))),\n                rlang::quo(list(wm = ~weighted.mean(., area, na.rm = TRUE))))\n\n\n\nveclamfuns &lt;- rlang::quo(list(\n  meanna = ~ mean(., na.rm = TRUE),\n  sdna = ~ sd(., na.rm = TRUE),\n  wmna = ~ weighted.mean(., wt, na.rm = TRUE)\n))\n\n# Not really meaningful, but weight by the number of gauges.\nwtgauge &lt;- simpleThemeAgg |&gt;\n  dplyr::group_by(scenario, gauge) |&gt;\n  dplyr::mutate(wt = dplyr::n()) |&gt;\n  dplyr::ungroup()\n\ntriplevec &lt;- spatial_aggregate(\n  dat = wtgauge,\n  to_geo = sdl_units,\n  groupers = c(\"scenario\", \"env_obj\"),\n  aggCols = ends_with(\"ewr_achieved\"),\n  funlist = veclamfuns,\n  keepAllPolys = TRUE,\n  failmissing = FALSE\n)\n\ntriplevec\n\n\n  \n\n\n\nIf we want to have custom functions with vector data arguments (i.e. full named functions, not anonymous functions specified in a list), we still need to use the tilde notation to point to those arguments. Making a dummy function that just adds two to the weighted mean, the wt argument doesn’t get seen if we just say funlist = wt2. Instead, we need to use a list.\n\nwt2 &lt;- function(x, wt) {\n  2 + weighted.mean(x, w = wt, na.rm = TRUE)\n}\n\nwt2list &lt;- rlang::quo(list(wt2 = ~ wt2(., wt)))\n\n\nvecnamedfun &lt;- spatial_aggregate(\n  dat = wtgauge,\n  to_geo = sdl_units,\n  groupers = c(\"scenario\", \"env_obj\"),\n  aggCols = ends_with(\"ewr_achieved\"),\n  funlist = wt2list,\n  keepAllPolys = TRUE,\n  failmissing = FALSE\n)\n\nvecnamedfun\n\n\n  \n\n\n\nThe ‘area’ exception and HydroBOT\nThe only exception to attaching vector arguments are situations where the needed vector arguments depend on both sets of from and to data/polygons, and so can’t be pre-attached. Moreover, in a HydroBOT workflow, there is limited ability to pre-attach columns (though it is possible with additional development).\nHowever, area-weighting is almost always needed, and so spatial_joiner() when polygons are present. This means HydroBOT data at polygon scales should always have an area column available for weighting.\nIf specifying a custom function to use this area column (which will often be the case), the trick is to use .data$area in the function definition. This bypasses the need to use rlang::quo, as it makes the argument explicitly a data-variable. See HydroBOT::SpatialWeightedMean, which is just\n\nSpatialWeightedMean &lt;- function (x, na.rm = TRUE) \n{\n    y &lt;- stats::weighted.mean(x = x, w = .data$area, na.rm = na.rm)\n}\n\n\n\n\n\n\n\nTip\n\n\n\nIf using this method, the safest is to define the function, and then pass it to the aggregation list using its character name.\n\n\nIn summary, we can pass single functions and their arguments in ellipses, complex lists of multiple functions using tilde-style anonymous functions, which can have vector arguments (as long as the vector is attached to the data), and lists of multiple function names. In general, the most robust method is to pre-define functions and pass them as character names.",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Aggregation syntax"
    ]
  },
  {
    "objectID": "aggregator/aggregation_syntax.html#spatial-information",
    "href": "aggregator/aggregation_syntax.html#spatial-information",
    "title": "Aggregation functions and grouping",
    "section": "Spatial information",
    "text": "Spatial information\nSpatial aggregation steps are specified with an sf object (polygons) or the name of the sf object, e.g. sdl_units or \"sdl_units\". Allowing specification of spatial steps by character instead of object is a bit more fragile because it relies on get(\"name_of_object\"), but allows the list to be wholly specified with characters. This both saves memory (potentially massively), and makes yaml parameter files (both input and output) much cleaner.",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Aggregation syntax"
    ]
  },
  {
    "objectID": "controller/controller_ewr_wrapped.html",
    "href": "controller/controller_ewr_wrapped.html",
    "title": "Scenario controller",
    "section": "",
    "text": "Load the package\nlibrary(HydroBOT)\nlibrary(future)\nThe controller primarily sets the paths to scenarios, calls the modules, and saves the output and metadata. In normal use, we set the directory and any other needed parameters (e.g. desired saving formats, parallelisation), and the controller functions auto-generate the folder structure, run the ewr, and output the results. This can be taken up a level to the combined workflow, where the controller and subsequent steps are all run at once. A detailed stepthrough of what happens in the controller is also available, useful to see what is happening under the hood.",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Clean simple controller"
    ]
  },
  {
    "objectID": "controller/controller_ewr_wrapped.html#paths",
    "href": "controller/controller_ewr_wrapped.html#paths",
    "title": "Scenario controller",
    "section": "Paths",
    "text": "Paths\nWe need to identify the path to the hydrographs and set up directories for outpus. In use, the hydrograph paths would typically point to external shared directories. The cleanest, default, situation is for everything to be in a single outer directory project_dir, and there should be an inner directory with the input data /hydrographs.\n\n\n\n\n\n\nTip\n\n\n\nWithin the hydrograph directory, scenarios should be kept in separate folders, i.e. files for gauges or all gauges within a catchment, basin, etc, within directories for scenarios (see here). This allows cleaner scenario structures and parallelisation. Any given run needs all the locations within a scenario, but scenarios should run separately (possibly in parallel) because outcomes (e.g. EWRs, fish performance) cannot logically depend on other scenarios representing other hydrological sequences or climates. A common situation that is much more cumbersome is to have the directory structure reflect gauges or other spatial unit, and files within them per scenario. It is worth restructuring your files if this is the case.\n\n\nIt also works to point to a single scenario, as might be the case if HydroBOT runs off the end of a hydrology model that generates that scenario, e.g. /hydrographs/scenario1. This allows both targeting single scenarios for HydroBOT analysis, but also batching hydrology and HydroBOT together. By default, the saved data goes to project_dir/module_output automatically, though this can be changed, see the output_parent_dir and output_subdir arguments.\n\nproject_dir &lt;- file.path(\"hydrobot_scenarios\")\nhydro_dir &lt;- file.path(project_dir, \"hydrographs\")",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Clean simple controller"
    ]
  },
  {
    "objectID": "controller/controller_ewr_wrapped.html#control-output-and-return",
    "href": "controller/controller_ewr_wrapped.html#control-output-and-return",
    "title": "Scenario controller",
    "section": "Control output and return",
    "text": "Control output and return\nTo determine what to save and what to return to the active session, use outputType and returnType, respectively. Each of them can take a list of any of\n\n'none'\n'summary'\n'yearly'\n'all_events'\n'all_successful_events'\n'all_interEvents', and\n'all_successful_interEvents'\n\n(e.g. returnType = list('summary', 'all') in R. These have to be lists, not c() to work right when sent to Python. The easiest to work with in HydroBOT is 'yearly', as that allows assessment of the outcomes, but we will return 'summary' here as well, as it is often nicer to look at as an EWR output.\n\nreturnType &lt;- list(\"yearly\", \"summary\")\noutputType &lt;- list(\"yearly\", \"summary\")",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Clean simple controller"
    ]
  },
  {
    "objectID": "controller/controller_ewr_wrapped.html#parallelism",
    "href": "controller/controller_ewr_wrapped.html#parallelism",
    "title": "Scenario controller",
    "section": "Parallelism",
    "text": "Parallelism\nWith many scenarios, it is often a good idea to parallelise. Because scenarios run independently, massive parallelisation is possible, up to one scenario per core. Speedups can be very large, even on local machines, but are particularly useful on HPCs.\nThe prep_run_save_ewrs function provides this parallelisation internally and seamlessly, provided the user has the suggested package furrr (and its dependency, future). In that case, parallelising is as easy as setting a future::plan and the argument rparallel = TRUE;\n\nplan(multisession)\newr_out &lt;- prep_run_save_ewrs(\n  hydro_dir = hydro_dir,\n  output_parent_dir = project_dir,\n  outputType = outputType,\n  returnType = returnType,\n  rparallel = TRUE\n)",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Clean simple controller"
    ]
  },
  {
    "objectID": "controller/controller_ewr_wrapped.html#selected-scenarios-or-extra-files-in-scenarios",
    "href": "controller/controller_ewr_wrapped.html#selected-scenarios-or-extra-files-in-scenarios",
    "title": "Scenario controller",
    "section": "Selected scenarios or extra files in scenarios",
    "text": "Selected scenarios or extra files in scenarios\nThe file_search argument uses regex to filter what gets run. It’s primarily used in two ways:\nBy default, [prep_run_save_ewrs()] assumes everything with a ‘.csv’ or ‘.nc’ file extension is a hydrograph file. In some cases, however, those directories might have other files in them. For example, maybe there’s a gauges.csv file and a run_info.csv file with metadata or a rainfall.csv with other variables. Trying to run the EWR tool will fail with anything other than gauges. In this example, we could use file_search = 'gauges.csv' to select only the correct file.\nThe file_search argument works on the full filepath as well, and so can be used for selecting a subset of scenarios. For example, we are using ‘down4’, ‘base’, and ‘up4’ scenarios as demonstrations here, but if we wanted to run only the down and up scenarios, we could use file_search = 'down|up'.",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Clean simple controller"
    ]
  },
  {
    "objectID": "controller/controller_ewr_wrapped.html#changing-output-directories",
    "href": "controller/controller_ewr_wrapped.html#changing-output-directories",
    "title": "Scenario controller",
    "section": "Changing output directories",
    "text": "Changing output directories\nParent\nBy default, HydroBOT builds a directory module_output/EWR in the output_parent_dir, which typically contains the hydro_dir:\nproject_dir/\n├─ hydrographs/\n├─ module_output/\n│  ├─ EWR/\nSee here for more detail.\nIf we want to change this, we use some combination of the output_parent_dir and output_subdir arguments, which do slightly different things.\nChanging output_parent_dir often happens for two reasons; either we want to save the modules somewhere other than the folder with the hydrographs, or we are forced to save them inside the relevant hydrograph folder, as might happen in remote batched runs that only have access to single directories.\nSince we always have to set output_parent_dir, the first case just involves setting it to something that does not contain the hydrographs, e.g.\n\newr_out &lt;- prep_run_save_ewrs(\n  hydro_dir = hydro_dir,\n  output_parent_dir = \"new_parent\",\n  outputType = outputType,\n  returnType = returnType,\n  rparallel = TRUE\n)\n\nIf we only have access to a specific scenario, as might happen sometimes in batched jobs, we set both hydro_dir and output_parent_dir to that scenario, which puts the module output within the scenario folder. That makes onward processing with the aggregator a bit more difficult but sometimes it’s all we can do.\n\newr_out &lt;- prep_run_save_ewrs(\n  hydro_dir = \"hydrobot_scenarios/hydrographs/base\",\n  output_parent_dir = \"hydrobot_scenarios/hydrographs/base\",\n  outputType = outputType,\n  returnType = returnType\n)\n\nThis yields\nproject_dir/\n├─ hydrographs/\n│  ├─ base/\n│  │  ├─ gauges.csv\n│  │  ├─ module_output/\n│  │  │  ├─ EWR/\n│  │  │  │  ├─ base/\nSubdirectories\nIt is sometimes the case that we want subdirectories within module_output/EWR. For example, maybe we want to retain results from an earlier run for reproducibility while updating either the arguments used or the EWR tool itself. In this case, we use the output_subdir argument, which simply adds the requested directory inside module_output/EWR:\n\newr_out &lt;- prep_run_save_ewrs(\n  hydro_dir = hydro_dir,\n  output_parent_dir = project_dir,\n  output_subdir = \"new_run\",\n  outputType = outputType,\n  returnType = returnType,\n  rparallel = TRUE\n)\n\nThe function prep_run_save_ewrs() has run, but the scenario(s) base_module_output_EWR_base have failed and have been bypassed after 2 retries.\n• The function prep_run_save_ewrs() has run, but the scenario(s) base_module_output_EWR_base have failed and have been bypassed after 2 retries.\n• The first error is:\n• &lt;pointer: 0x0&gt;\n\n\nThis yields:\nproject_dir/\n├─ hydrographs/\n│  ├─ base/\n│  ├─ down4/\n│  ├─ up4/\n├─ module_output/\n│  ├─ EWR/\n│  │  ├─ new_run/\n│  │  │  ├─ base/\n│  │  │  ├─ down4/\n│  │  │  ├─ up4/",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Clean simple controller"
    ]
  },
  {
    "objectID": "controller/controller_ewr_wrapped.html#scenario-names-and-file-paths",
    "href": "controller/controller_ewr_wrapped.html#scenario-names-and-file-paths",
    "title": "Scenario controller",
    "section": "Scenario names and file paths",
    "text": "Scenario names and file paths\nScenarios need to have unique names. As such, in most cases they are extracted from the file paths, which must be unique. This can get a bit messy, but is the only consistent way to ensure uniqueness. Analysis stages can incorporate cleanup steps. One way to reduce the messiness a bit is to use scenarios_from = 'directory' (the default), which drops the filename, in the case that the filenames are not needed for uniqueness, e.g. in common situations like /scenario1/gauge.csv or /scenario1/scenario1.csv. If more control over the naming is needed, use scenarios. Even in that case, the file names will be unique, but the scenario column will reflect the names in the passed list.",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Clean simple controller"
    ]
  },
  {
    "objectID": "controller/controller_ewr_wrapped.html#full-control-over-scenarios",
    "href": "controller/controller_ewr_wrapped.html#full-control-over-scenarios",
    "title": "Scenario controller",
    "section": "Full control over scenarios",
    "text": "Full control over scenarios\nIn some cases, the scenario names or directory structures might be such that they cannot reasonably be inferred from the directories, or more granular control is needed over the subset of scenarios to run. In this case, the scenarios argument allows passing in a named list of paths, with the names being scenario names and the paths the paths to the files. This then bypasses all the path and scenario inference, giving the user direct control.\nFor example, we can rename scenarios and only run a subset, as illustrated here to run the down4 and up4 scenarios and rename the scenarios to decrease and increase. We do still need a hydro_dir argument, as the paths in scenarios are assumed to be relative to that directory, and it’s where metadata gets saved. That said, using a high-level location for hydro_dir would allow the paths to be essentially anywhere.\n\newr_out &lt;- prep_run_save_ewrs(\n  hydro_dir = hydro_dir,\n  output_parent_dir = project_dir,\n  scenarios = list(\n    decrease = \"down4/down4.csv\",\n    increase = \"up4/up4.csv\"\n  ),\n  outputType = outputType,\n  returnType = returnType,\n  rparallel = TRUE\n)",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Clean simple controller"
    ]
  },
  {
    "objectID": "controller/controller_ewr_wrapped.html#completing-runs",
    "href": "controller/controller_ewr_wrapped.html#completing-runs",
    "title": "Scenario controller",
    "section": "Completing runs",
    "text": "Completing runs\nIn some cases (often, large batched jobs over many scenarios), some subset of the runs will fail. Sometimes this is because of data problems, sometimes because a computer shuts down or an HPC times out. And it could happen as well if additional scenarios are added after an initial run. In these cases, we don’t want to have to either re-run everything or write a unique script for the missing pieces. Instead, we can use the fill_missing = TRUE argument to find the scenarios in the inputs (hydrographs) that are missing in the outputs (module_output/EWR directory). This should work with any directory structure (it uses the same machinery), but isn’t well-tested for nonstandard directory structures (output_subdirs are tested).",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Clean simple controller"
    ]
  },
  {
    "objectID": "controller/controller_ewr_wrapped.html#data-formats",
    "href": "controller/controller_ewr_wrapped.html#data-formats",
    "title": "Scenario controller",
    "section": "Data formats",
    "text": "Data formats\nThe input data formats are limited to those the EWR tool is capable of processing, and the files can be single or multiple, discussed in more detail here.",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Clean simple controller"
    ]
  },
  {
    "objectID": "comparer/maps.html",
    "href": "comparer/maps.html",
    "title": "Maps",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Maps"
    ]
  },
  {
    "objectID": "comparer/maps.html#read-in-the-data",
    "href": "comparer/maps.html#read-in-the-data",
    "title": "Maps",
    "section": "Read in the data",
    "text": "Read in the data\nWe read in the example data we will use for all plots.\n\nagged_data &lt;- readRDS(file.path(agg_dir, \"achievement_aggregated.rds\"))\n\nThat has all the steps in the aggregation, but most of the plots here will only use a subset to demonstrate.\nTo make visualisation easier, the SDL units data is given a grouping column that puts the many env_obj variables in groups defined by their first two letters, e.g. EF for Ecosystem Function. These correspond to the ‘Target’ level, but it can be useful to have the two groupings together for some examples.\nIf we had used multiple aggregation functions at any step, we should filter down to the one we want here, but we only used one for this example.\nFor simplicity here, we will only look at a small selection of the scenarios (multiplicative changes of 0.5,1, and 2). Thus, we make two small dataframes for our primary examples here.\n\nscenarios_to_plot &lt;- c('climatedown2adapt0', 'climatebaseadapt0', 'climateup2adapt0')\n\nscenarios &lt;- yaml::read_yaml(file.path(hydro_dir, 'scenario_metadata.yml')) |&gt; \n  tibble::as_tibble()\n\nbasin_to_plot &lt;- agged_data$mdb |&gt; \n  dplyr::filter(scenario %in% scenarios_to_plot) |&gt; \n  dplyr::left_join(scenarios, by = 'scenario')\n\n# Create a grouping variable\nobj_sdl_to_plot &lt;- agged_data$sdl_units |&gt;\n    dplyr::filter(scenario %in% scenarios_to_plot) |&gt; \n  dplyr::mutate(env_group = stringr::str_extract(env_obj, \"^[A-Z]+\")) |&gt;\n  dplyr::arrange(env_group, env_obj) |&gt; \n  dplyr::left_join(scenarios, by = 'scenario')",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Maps"
    ]
  },
  {
    "objectID": "comparer/maps.html#simple-background-and-foreground",
    "href": "comparer/maps.html#simple-background-and-foreground",
    "title": "Maps",
    "section": "Simple background and foreground",
    "text": "Simple background and foreground\nWe can add a polygon with no mapped aesthetics to the background and foreground using underlay_list and overlay_list. Here, we add the basin shape behind the data and the gauges used originally.\n\n# this can take a while so use a small sheet and pre-filter to make smaller\nrelevant_gauges &lt;- agged_data$ewr_code |&gt; \n  filter(scenario == 'climatebaseadapt0') |&gt; \n select(gauge, geometry) |&gt; \n distinct()\n\nobj_sdl_to_plot |&gt;\n  dplyr::filter(env_group == \"WB\") |&gt; # Need to reduce dimensionality\n  plot_outcomes(\n    outcome_col =  \"ewr_achieved\",\n    y_lab = \"Proportion EWR\\nachieved\",\n    plot_type = \"map\",\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::lapaz\"),\n    pal_direction = -1,\n    facet_col = \"env_obj\",\n    facet_row = \"climate_code\",\n    underlay_list = list(\n      underlay = basin,\n      underlay_pal = \"azure\"\n    ),\n    overlay_list = list(overlay = relevant_gauges, \n                        overlay_pal = 'firebrick')\n  )\n\n\n\n\n\n\nFigure 1",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Maps"
    ]
  },
  {
    "objectID": "comparer/maps.html#other-spatial-units",
    "href": "comparer/maps.html#other-spatial-units",
    "title": "Maps",
    "section": "Other spatial units",
    "text": "Other spatial units\nWe used the sdl units (an intermediate aggregation level) above, but any geometric levels will work. First, we show with gauges as the focal spatial unit, e.g. before any spatial aggregation has occurred (but there has been both temporal and theme).\n\n\n\n\n\n\nImportant\n\n\n\nIf we attempt to run this as-is, it shows the value of plot_outcomes() for catching unintentional overplotting. The filtering to just NF1 is an attempt to only plot one value, but there are hidden and easily missed doubled gauges in this data. Because in the EWR tool some gauges inform several different planning units, there are more than one line for those gauges. plot_outcomes() catches this and forces us to deal with it.\n\n\n\nagged_data$env_obj |&gt; # for readability\n  dplyr::filter(env_obj == \"NF1\" & \n                  scenario %in% scenarios_to_plot) |&gt; # Need to reduce dimensionality\n  plot_outcomes(\n    outcome_col =  \"ewr_achieved\",\n    y_lab = \"Arithmetic Mean\",\n    plot_type = \"map\",\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::lapaz\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    underlay_list = list(\n      underlay = \"basin\",\n      underlay_pal = \"azure\"\n    )\n  ) +\n  ggplot2::theme(legend.position = \"bottom\")\n\nError in `test_overplotting()` at HydroBOT/R/plot_types.R:162:3:\n! Trying to plot multiple values\n(up to 4) in single polygons.\nSomething is duplicated-\ndo you need more facetting or filtering?\n\n\nThe ‘real’ fix of that issue would be to filter to a particular planning unit we wanted or investigate in more granular detail. But here, we just want a simple demonstration of plotting points, so we simply filter to the first record for each gauge.\n\nenv_obj_to_plot &lt;- agged_data$env_obj |&gt; # for readability\n  dplyr::filter(env_obj == \"NF1\" & \n                  scenario %in% scenarios_to_plot) |&gt; # Need to reduce dimensionality\n  group_by(scenario, env_obj) |&gt; \n  filter(!duplicated(gauge)) |&gt; \n  ungroup()\n\n\nenv_obj_to_plot |&gt; \n  plot_outcomes(\n    outcome_col =  \"ewr_achieved\",\n    y_lab = \"Arithmetic Mean\",\n    plot_type = \"map\",\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::lapaz\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    underlay_list = list(\n      underlay = \"basin\",\n      underlay_pal = \"azure\"\n    )\n  ) +\n  ggplot2::theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWe can also plot the basin-scale outcomes (rather than just the shape as above).\n\nbasin_to_plot |&gt;\n  plot_outcomes(\n    outcome_col =  \"ewr_achieved\",\n    y_lab = \"Proportion EWR\\nachieved\",\n    plot_type = \"map\",\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::lapaz\"),\n    pal_direction = -1,\n    facet_col = \"Target\",\n    facet_row = \"climate_code\"\n    )",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Maps"
    ]
  },
  {
    "objectID": "comparer/maps.html#multiple-informative-layers",
    "href": "comparer/maps.html#multiple-informative-layers",
    "title": "Maps",
    "section": "Multiple informative layers",
    "text": "Multiple informative layers\nIn their most simple sense, underlays and overlays can be used to simply add spatial information (the basin or country, the gauge locations). In more complex cases though, information can be plotted in multiple layers. The choice of a ‘primary’ layer from which other layers are underlays or overlays can be a bit arbitrary, since each layer can plot outcome data, but the ‘primary’ layer has access to a few more processing options. It typically makes the most sense to choose a primary layer to address a targeted question and choose the under/overlays to add context.\nUnderlays\nWhile we can’t have informative fill in multiple layers ({ggplot2} won’t allow multiple fills), we can have informative fill in polygons underlying point (gauge) data, because the points use color, not fill.\nFirst, we include a fill in the underlay for SDL unit name (e.g. not a data fill, but still informative) by simply replacing the single color in underlay_pal with a {paletteer} palette. It is hard to find palettes for the full set of catchments. We use a qualitative palette here. Using a continuous palette (e.g. grDevices::Purple-Yellow) works OK as well, but the colors aren’t very well spatially-separated.\n\nenv_obj_to_plot |&gt; \n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    y_lab = \"All arithmetic mean\",\n    plot_type = \"map\",\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::lapaz\"),\n    pal_direction = 1,\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    sceneorder = c('climatedown2adapt0', 'climatebaseadapt0', 'climateup2adapt0'),\n    underlay_list = list(\n      underlay = sdl_units,\n      underlay_ycol = \"SWSDLName\",\n      underlay_pal = \"palettesForR::Muted\"\n    )\n  ) +\n  ggplot2::theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWe can use a continuous variable on the underlay fill, but have to be careful to choose palettes that don’t mask each other. It is very tempting to put e.g. gauges under sdl units and color by the same palette to show the effect of the aggregation, but that is often confusing given the similarity in color (and noting that because here we have to eliminate some planning units to plot the gauges, the full set of aggregated gauge values is not shown).\n\nenv_obj_to_plot |&gt; \n  plot_outcomes(\n    outcome_col =  \"ewr_achieved\",\n    plot_type = \"map\",\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::oslo\"),\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    sceneorder = c('climatedown2adapt0', 'climatebaseadapt0', 'climateup2adapt0'),\n    underlay_list = list(\n        underlay = dplyr::filter(obj_sdl_to_plot, env_obj == \"NF1\"),\n        underlay_ycol = \"ewr_achieved\",\n        underlay_pal = \"scico::oslo\"\n      )\n    ) +\n  ggplot2::theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWhile the aggregation isn’t as obvious here, it is easier to see the different levels:\n\nenv_obj_to_plot |&gt; \n  plot_outcomes(\n    outcome_col =  \"ewr_achieved\",\n    plot_type = \"map\",\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"ggthemes::Orange-Gold\"),\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    sceneorder = c('climatedown2adapt0', 'climatebaseadapt0', 'climateup2adapt0'),\n    underlay_list = list(\n        underlay = dplyr::filter(obj_sdl_to_plot, env_obj == \"NF1\"),\n        underlay_ycol = \"ewr_achieved\",\n        underlay_pal = \"scico::oslo\"\n      )\n    ) +\n  ggplot2::theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWe can also have multiple levels of underlay polygons (e.g. if we want the basin under sdl units), and can use a fill palette in one of them (Figure 2) by using a list of lists for underlay_list. A similar approach also works for overlays (Figure 3). And as we’ve seen above (Figure 1), we can have both underlays and overlays.\n\nenv_obj_to_plot |&gt; \n  plot_outcomes(\n    outcome_col =  \"ewr_achieved\",\n    plot_type = \"map\",\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"ggthemes::Orange-Gold\"),\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    sceneorder = c('climatedown2adapt0', 'climatebaseadapt0', 'climateup2adapt0'),\n    underlay_list = list(\n      list(\n        underlay = \"basin\",\n        underlay_pal = \"cornsilk\"\n      ),\n      list(\n        underlay = dplyr::filter(obj_sdl_to_plot, env_obj == \"NF1\"),\n        underlay_ycol = \"ewr_achieved\",\n        underlay_pal = \"scico::oslo\"\n      )\n    )\n  ) +\n  ggplot2::theme(legend.position = \"bottom\")\n\n\n\n\n\n\nFigure 2\n\n\n\n\nOverlays\nAbove, we used the gauge layer as ‘primary’, and included the sdl units or basin as underlays. We can also overlay, here with the sdl layer as ‘primary’.\nWe start with a single color overlay to show where gauges are. This is the full set of BOM gauges, not just those that went into the aggregation (i.e. are in the EWR tool), which are the ones shown above (Figure 1).\n\n\n\n\n\n\nTip\n\n\n\nWe also show here the use of clip, which auto-clips the under or overlay data to the primary layer, and the use of just an {sf} object name for underlay_list = \"basin\" as shorthand when we don’t want to specify any visual properties.\n\n\n\nobj_sdl_to_plot |&gt;\n  dplyr::filter(env_obj == \"NF1\") |&gt; # Need to reduce dimensionality\n  plot_outcomes(\n    outcome_col =  \"ewr_achieved\",\n    plot_type = \"map\",\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::lapaz\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    underlay_list = \"basin\",\n    sceneorder = c('climatedown2adapt0', 'climatebaseadapt0', 'climateup2adapt0'),\n    overlay_list = list(\n      overlay = \"bom_basin_gauges\",\n      overlay_pal = \"grey40\",\n      clip = TRUE\n    )\n  ) +\n  ggplot2::theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWe can give the overlay informative colors- this outcome is similar to what we’ve done above, but the amount of control we have over scaling etc differs depending on whether a layer is ‘primary’. We also use map_outlinecolor to adjust the color of the primary layer outlines.\n\nobj_sdl_to_plot |&gt;\n  dplyr::filter(env_obj == \"NF1\") |&gt; # Need to reduce dimensionality\n  plot_outcomes(\n    outcome_col =  \"ewr_achieved\",\n    plot_type = \"map\",\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::lapaz\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    underlay_list = \"basin\",\n    sceneorder = c('climatedown2adapt0', 'climatebaseadapt0', 'climateup2adapt0'),\n    map_outlinecolor = 'forestgreen',\n    overlay_list = list(\n      overlay = env_obj_to_plot,\n      overlay_pal = \"scico::oslo\",\n      overlay_ycol = \"ewr_achieved\",\n      clip = TRUE\n    )\n  ) +\n  ggplot2::theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nLike underlays (Figure 2), we can have multiple levels of overlays (Figure 3) by using a list of lists for overlay_list. And as we’ve seen above (Figure 1), we can have both underlays and overlays.\nHere, we demonstrate a primary layer at the basin scale, overlay the sdl units with empty fill so we can see their locations, and then put gauges on with informative values.\n\nbasin_to_plot |&gt; # Need to reduce dimensionality\n  plot_outcomes(\n    outcome_col =  \"ewr_achieved\",\n    plot_type = \"map\",\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::oslo\"),\n    facet_col = \"Target\",\n    facet_row = \"scenario\",\n    sceneorder = c('climatedown2adapt0', 'climatebaseadapt0', 'climateup2adapt0'),\n    overlay_list = list(\n      list(overlay = \"sdl_units\"),\n      list(\n        overlay = env_obj_to_plot,\n        overlay_pal = \"ggthemes::Orange-Gold\",\n        overlay_ycol = \"ewr_achieved\"\n      )\n    )\n  ) +\n  ggplot2::theme(legend.position = \"bottom\")\n\n\n\n\n\n\nFigure 3",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Maps"
    ]
  },
  {
    "objectID": "comparer/maps.html#baselining",
    "href": "comparer/maps.html#baselining",
    "title": "Maps",
    "section": "Baselining",
    "text": "Baselining\nAs with the other plotting functions, we can compare to baseline using arbitrary functions, here difference to get the arithmetic change in outcomes. For both difference and relative, the limits are adjusted internally to center on the reference value (0 for difference, 1 for relative), and so using a diverging palette will make that centering clear.\n\nobj_sdl_to_plot |&gt;\n  dplyr::filter(env_group == \"WB\") |&gt; # Need to reduce dimensionality\n  plot_outcomes(\n    outcome_col =  \"ewr_achieved\",\n    plot_type = \"map\",\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"ggthemes::Orange-Blue-White Diverging\"),\n    facet_col = \"env_obj\",\n    facet_row = \"scenario\",\n    sceneorder = c('climatedown2adapt0', 'climatebaseadapt0', 'climateup2adapt0'),\n    base_list = list(\n    base_lev = \"climatebaseadapt0\",\n    comp_fun = \"difference\",\n    group_cols = c(\"env_obj\", \"polyID\")\n    ),\n    underlay_list = list(underlay = basin, underlay_pal = \"azure\")\n  )\n\n\n\n\n\n\n\nAnd the relative change is likely to be the most informative and appropriate in many cases. We use the add_eps argument to add a small amount (half the minimum value) to zeros, otherwise we end up taking the log of 0.\n\nobj_sdl_to_plot |&gt;\n  dplyr::filter(env_group == \"WB\") |&gt; # Need to reduce dimensionality\n  plot_outcomes(\n    outcome_col =  \"ewr_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"ggthemes::Orange-Blue-White Diverging\"),\n    facet_col = \"env_obj\",\n    facet_row = \"scenario\",\n    sceneorder = c('climatedown2adapt0', 'climatebaseadapt0', 'climateup2adapt0'),\n    base_list = list(base_lev = \"climatebaseadapt0\",\n    comp_fun = \"relative\",\n    group_cols = c(\"env_obj\", \"polyID\"),\n    add_eps = 'auto'),\n    zero_adjust = \"auto\",\n    transoutcome = \"log10\",\n    underlay_list = list(underlay = basin, underlay_pal = \"azure\")\n  )",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Maps"
    ]
  },
  {
    "objectID": "comparer/causal_plots.html",
    "href": "comparer/causal_plots.html",
    "title": "Causal network plots",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(dplyr)",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Outcomes on causal networks"
    ]
  },
  {
    "objectID": "comparer/causal_plots.html#setup",
    "href": "comparer/causal_plots.html#setup",
    "title": "Causal network plots",
    "section": "Setup",
    "text": "Setup\nAs always, we need the input data\n\nproject_dir &lt;- \"hydrobot_scenarios\"\nhydro_dir &lt;- file.path(project_dir, 'hydrographs')\newr_results &lt;- file.path(project_dir, \"module_output\", \"EWR\")\n\nFor simplicity, we just do aggregations along the theme dimension, since otherwise the nodes can incorporate different spatial scales at different steps, which can be misleading. So here we use read_and_agg() to do that aggregation.\n\naggseq &lt;- list(all_time = 'all_time',\n               ewr_code = c('ewr_code_timing', 'ewr_code'),\n               env_obj =  c('ewr_code', \"env_obj\"),\n               Target = c('env_obj', 'Target'))\n\nfunseq &lt;- list('ArithmeticMean', \n               'CompensatingFactor',\n               'ArithmeticMean',\n               'ArithmeticMean')\n\nagged_theme &lt;- read_and_agg(datpath = ewr_results, \n           type = 'achievement',\n           geopath = bom_basin_gauges,\n           causalpath = causal_ewr,\n           groupers = 'scenario',\n           aggCols = 'ewr_achieved',\n           auto_ewr_PU = TRUE,\n           aggsequence = aggseq,\n           funsequence = funseq,\n           saveintermediate = TRUE,\n           namehistory = FALSE,\n           keepAllPolys = FALSE,\n           returnList = TRUE,\n           add_max = FALSE)",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Outcomes on causal networks"
    ]
  },
  {
    "objectID": "comparer/causal_plots.html#causal-plot",
    "href": "comparer/causal_plots.html#causal-plot",
    "title": "Causal network plots",
    "section": "Causal plot",
    "text": "Causal plot\nBy returning values at each stage, we can map those to colour in a causal network. Here, we map the values of the aggregation to node color. To do this, we follow the make_causal_plot() approach of making edges and nodes, and then use a join to attach the value to each node.\nTo keep this demonstration from becoming too unwieldy, we limit the edge creation to a single gauge, and so filter the theme aggregations accordingly (or just rely on the join to drop).\n\nexample_gauge &lt;- \"421001\"\n\nThe first step is to generate the edges and nodes for the network we want to look at.\n\nedges &lt;- make_edges(causal_ewr,\n  fromtos = aggseq[2:length(aggseq)],\n  gaugefilter = example_gauge\n)\n\nnodes &lt;- make_nodes(edges)\n\nNow, extract the values we want from the aggregation and join them to the nodes.\n\n# Get the values for each node\naggvals &lt;- extract_vals_causal(agged_theme,\n                               whichaggs = funseq, # Since only one agg function at each step\n                               valcol = 'ewr_achieved',\n                               targetlevels = names(aggseq)[2:length(aggseq)]) # don't use the first one, it's time at _timing \n\n\n# filter to a single gauge. Multiple gauges should have separate networks or otherwise split the gauge-specific nodes. And include the larger scales pertaining to that gauge.\n\n# if we stay within the gauge, and just do value, this works\naggvals &lt;- aggvals |&gt; \n  filter(gauge == example_gauge) |&gt; \n  st_drop_geometry()\n\n# join to the nodes\nnodes_with_vals &lt;- dplyr::left_join(nodes, aggvals)\n\nNow we can make the causal network plot with the nodes we chose and colour them by the values we’ve just attached to them from the aggregation. At present, it is easiest to make separate plots per scenario or other grouping ( Figure 1 , Figure 3, Figure 2 ). For example, in the increased watering scenario, we see more light colours, and so better performance across the range of outcomes. Further network outputs are provided in the Comparer.\n\n\n\n\n\n\nTip\n\n\n\nCausal networks render fine in notebooks, but often fail to on export. The trick is to save them as objects and explicitly call Diagrammer::render_graph().\n\n\n\naggNetwork_base &lt;- make_causal_plot(\n  nodes = dplyr::filter(\n    nodes_with_vals,\n    scenario == \"base\"\n  ),\n  edges = edges,\n                 setLimits = c(0, 1),\n  edge_pal = \"black\",\n  node_pal = list(value = \"scico::lapaz\"),\n  node_pal_direction = -1,\n  node_colorset = \"ewr_achieved\",\n  render = FALSE\n)\n\nDiagrammeR::render_graph(aggNetwork_base)\n\n\n\n\n\n\nFigure 1: Causal network for baseline scenario at example gauge, coloured by proportion passing at each node, e.g. Arithmetic Means at every step. Light yellow is 0, dark blue is 1.\n\n\n\n\naggNetwork_down &lt;- make_causal_plot(\n  nodes = dplyr::filter(\n    nodes_with_vals,\n    scenario == \"down4\"\n  ),\n  edges = edges,\n                 setLimits = c(0, 1),\n  edge_pal = \"black\",\n  node_pal = list(value = \"scico::lapaz\"),\n  node_pal_direction = -1,\n  node_colorset = \"ewr_achieved\",\n  render = FALSE\n)\n\nDiagrammeR::render_graph(aggNetwork_down)\n\n\n\n\n\n\nFigure 2: Causal network for the 0.25x scenario at example gauge, coloured by proportion passing at each node, e.g. Arithmetic Means at every step. Light yellow is 0, dark blue is 1.\n\n\n\n\naggNetwork_4 &lt;- make_causal_plot(\n  nodes = dplyr::filter(\n    nodes_with_vals,\n    scenario == \"up4\"\n  ),\n  edges = edges,\n                 setLimits = c(0, 1),\n  edge_pal = \"black\",\n  node_pal = list(value = \"scico::lapaz\"),\n  node_pal_direction = -1,\n  node_colorset = \"ewr_achieved\",\n  render = FALSE\n)\n\nDiagrammeR::render_graph(aggNetwork_4)\n\n\n\n\n\n\nFigure 3: Causal network for 4x scenario at example gauge, coloured by proportion passing at each node, e.g. Arithmetic Means at every step.Light yellow is 0, dark blue is 1.\n\n\n\nBaselined data\nWe can also baseline the data and then plot those changes on the network.\n\n\n\n\n\n\nCaution\n\n\n\nThe causal network defines how values at one level are linked to values at others. It is these values that propagate through the network. Comparisons do not propagate through the network. Thus, plotting them on the network shows only how each node is compared across scenarios, not how those comparisons propagate through the network (they don’t, and because they are often nonlinear, the outcome would be highly misleading if they did). The links in this case show the existence of relationships, but are not appropriate to interpret as causal models as they are for the values themselves.\n\n\n\n# pre-filter to make plotting simpler\nbasenodes &lt;- nodes_with_vals |&gt; \n  filter(gauge == example_gauge) |&gt; \n  baseline_compare(compare_col = 'scenario', \n                   base_lev = 'base',\n                   values_col = 'ewr_achieved',\n                   group_cols = c('Name', 'NodeType', 'nodeorder'),\n                   comp_fun = 'relative',\n                   add_eps = 0.01) |&gt;\n  mutate(logrel_ewr_achieved = log(relative_ewr_achieved))\n\nNow make the networks.\n\n\n\n\n\n\nTip\n\n\n\nCausal networks look good in html, but often do not export well. The trick is to use save = TRUE and specify a savedir and savename. This will save them as vector objects and so retain quality.\n\n\n\naggNetworkdown_rel &lt;- basenodes |&gt;\n  dplyr::filter(scenario == 'down4') |&gt;\n  make_causal_plot(edges = edges,\n                 edge_pal = 'black',\n                 node_pal = list(value = 'ggthemes::Orange-Blue-White Diverging'),\n                 node_colorset = 'logrel_ewr_achieved',\n                 render = FALSE,\n                 setLimits = c(min(basenodes$logrel_ewr_achieved),\n                               0,\n                               max(basenodes$logrel_ewr_achieved)),\n                 save = TRUE,\n                 savedir = tempdir(),\n                 savename = 'aggNetworkdown_rel')\n\nDiagrammeR::render_graph(aggNetworkdown_rel)\n\n\n\n\n\n\nFigure 4: Relative change between the 0.25x scenario and baseline for each node in the causal network. Blue indicates increases, white is no change, and orange is decrease.\n\n\n\n\naggNetworkup_rel &lt;- basenodes |&gt;\n  dplyr::filter(scenario == 'up4') |&gt;\n  make_causal_plot(edges = edges,\n                 edge_pal = 'black',\n                 node_pal = list(value = 'ggthemes::Orange-Blue-White Diverging'),\n                 node_colorset = 'logrel_ewr_achieved',\n                 render = FALSE,\n                 setLimits = c(min(basenodes$logrel_ewr_achieved),\n                               0,\n                               max(basenodes$logrel_ewr_achieved)),\n                 save = TRUE,\n                 savedir = tempdir(),\n                 savename = 'aggNetworkup_rel')\n\nDiagrammeR::render_graph(aggNetworkup_rel)\n\n\n\n\n\n\nFigure 5: Relative change between the 4x scenario and baseline for each node in the causal network. Blue indicates increases, white is no change, and orange is decrease.",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Outcomes on causal networks"
    ]
  },
  {
    "objectID": "workflows/workflow_save_steps.html",
    "href": "workflows/workflow_save_steps.html",
    "title": "Saving each step",
    "section": "",
    "text": "A common way to run HydroBOT is to use a single document (i.e. not running the Controller, Aggregator, and Comparer as separate notebooks), but save the outputs at each step. This allows re-starting analyses if necessary and is particularly useful for large jobs, especially if batching over many scenarios. That said, in practice, once batching takes more than several minutes per stage, it often does make most sense to split into separate notebooks or scripts for each step.\nRetaining everything in-memory is a very simple flip of a switch, demoed in its own doc.\nThe getting started page uses this sort of workflow.\nlibrary(HydroBOT)",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "Saving steps"
    ]
  },
  {
    "objectID": "workflows/workflow_save_steps.html#directories",
    "href": "workflows/workflow_save_steps.html#directories",
    "title": "Saving each step",
    "section": "Directories",
    "text": "Directories\nHere, we will use the example hydrographs that come with HydroBOT.\nNormally project_dir and hydro_dir would point somewhere external (and typically, having hydro_dir inside project_dir will make life easier).\n\nhydro_dir &lt;- system.file(\"extdata/testsmall/hydrographs\", package = \"HydroBOT\")\n\nproject_dir &lt;- file.path(\"test_dir\")\n\n# Generated data\n# EWR outputs (will be created here in controller, read from here in aggregator)\newr_results &lt;- file.path(project_dir, \"module_output\", \"EWR\")\n\n# outputs of aggregator. There may be multiple modules\nagg_results &lt;- file.path(project_dir, \"aggregator_output\")\n\nWe need the ‘yearly’ EWR outputs for HydroBOT processing, though we can get the EWR to return any of its options.\n\newr_out &lt;- prep_run_save_ewrs(\n  hydro_dir = hydro_dir,\n  output_parent_dir = project_dir,\n  outputType = list(\"yearly\"),\n  returnType = list(\"yearly\")\n)",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "Saving steps"
    ]
  },
  {
    "objectID": "workflows/workflow_save_steps.html#aggregation",
    "href": "workflows/workflow_save_steps.html#aggregation",
    "title": "Saving each step",
    "section": "Aggregation",
    "text": "Aggregation\nWe need to define an aggregation sequence, which specifies the steps along space, time, and value dimensions, as well as a matching sequence of aggregation functions at each step. The example here is a reasonable default for many situations, but should be considered carefully. Please see the aggregator section for a detailed treatment of the syntax and capabilities of these sequences.\nFor this example, if any of the ‘versions’ of an EWR pass, say it passes in step 2. If we think the ‘versions’ are actually separate EWRs for different purposes, we might use ‘ArithmeticMean’ in step 2 intead.\n\naggseq &lt;- list(\n  all_time = \"all_time\",\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  env_obj = c(\"ewr_code\", \"env_obj\"),\n  sdl_units = sdl_units,\n  Target = c(\"env_obj\", \"Target\"),\n  mdb = basin,\n  target_5_year_2024 = c(\"Target\", \"target_5_year_2024\")\n)\n\nfunseq &lt;- list(\n  all_time = \"ArithmeticMean\",\n  ewr_code = \"CompensatingFactor\",\n  env_obj = \"ArithmeticMean\",\n  sdl_units = \"ArithmeticMean\",\n  Target = \"ArithmeticMean\",\n  mdb = \"SpatialWeightedMean\",\n  target_5_year_2024 = \"ArithmeticMean\"\n)\n\nRun the aggregation. Use the auto_ewr_PU shortcut to auto-specify some needed grouping for EWR outputs (see here for more detail).\nRead the messages. Most of these are saying it’s more explicit to avoid auto_ewr_PU = TRUE, but there are also messages saying that the causal network is missing links.\n\naggout &lt;- read_and_agg(\n  datpath = ewr_results,\n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  auto_ewr_PU = TRUE,\n  aggsequence = aggseq,\n  funsequence = funseq,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = FALSE,\n  returnList = TRUE,\n  add_max = FALSE,\n  savepath = agg_results\n)\n\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`.\n\n\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`\n.\n! Unmatched links in causal network\n• 29 from ewr_code_timing to ewr_code\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`\n.\nℹ EWR gauges joined to larger units pseudo-spatially.\n• Done automatically because `auto_ewr_PU = TRUE`\n• Non-spatial join needed because gauges may inform areas they are not within\n• Best to explicitly use `pseudo_spatial = 'sdl_units'` in `multi_aggregate()` or `read_and_agg()`.\n\n! Unmatched links in causal network\n• 4 from Target to target_5_year_2024",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "Saving steps"
    ]
  },
  {
    "objectID": "workflows/workflow_save_steps.html#comparer",
    "href": "workflows/workflow_save_steps.html#comparer",
    "title": "Saving each step",
    "section": "Comparer",
    "text": "Comparer\nNow we can make a couple quick plots to see what we’ve made. For more detail about plotting options and controls, see the comparer.\nMaps and spatial scaling\n\nmap_example &lt;- aggout$env_obj |&gt;\n  dplyr::filter(env_obj == \"NF1\") |&gt; # Need to reduce dimensionality\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    sceneorder = c(\"down4\", \"base\", \"up4\"),\n    underlay_list = \"basin\"\n  ) +\n  ggplot2::theme(legend.position = \"bottom\")\n\nmap_example\n\n\n\n\n\n\n\nBars- SDL units and scenarios\nSDL unit differences across all environmental objectives\n\ncatchcompare &lt;- aggout$env_obj |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    colorset = \"SWSDLName\",\n    pal_list = list(\"calecopal::lake\"),\n    sceneorder = c(\"down4\", \"base\", \"up4\"),\n    position = \"dodge\"\n  )\n\ncatchcompare",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "Saving steps"
    ]
  },
  {
    "objectID": "comparer/setting_limits.html",
    "href": "comparer/setting_limits.html",
    "title": "Setting limits for consistency",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(patchwork)",
    "crumbs": [
      "Comparer",
      "Syntax and details",
      "Setting limits"
    ]
  },
  {
    "objectID": "comparer/setting_limits.html#read-in-the-data",
    "href": "comparer/setting_limits.html#read-in-the-data",
    "title": "Setting limits for consistency",
    "section": "Read in the data",
    "text": "Read in the data\nWe read in the example data we will use for all plots.\n\nagged_data &lt;- readRDS(file.path(agg_dir, \"achievement_aggregated.rds\"))\n\nThat has all the steps in the aggregation, but most of the plots here will only use a subset to demonstrate.\nTo make visualization easier, the SDL units data is given a grouping column that puts the many env_obj variables in groups defined by their first two letters, e.g. EF for Ecosystem Function. These correspond to the ‘Target’ level, but it can be useful to have the two groupings together for some examples.\nIf we had used multiple aggregation functions at any step, we should filter down to the one we want here, but we only used one for this example.\nFor simplicity here, we will only look at a small selection of the scenarios (multiplicative changes of 0.5,1, and 2).\n\nscenarios_to_plot &lt;- c('climatedown2adapt0', 'climatebaseadapt0', 'climateup2adapt0')\n\nscenarios &lt;- yaml::read_yaml(file.path(hydro_dir, 'scenario_metadata.yml')) |&gt; \n  tibble::as_tibble() |&gt; \n  dplyr::filter(scenario %in% scenarios_to_plot)\n\nsceneorder &lt;- forcats::fct_reorder(scenarios$scenario,\n                                   scenarios$flow_multiplier)\n\nscene_pal &lt;- make_pal(unique(scenarios$climate_code),\n  palette = \"ggsci::nrc_npg\",\n  refvals = \"E\", refcols = \"black\"\n)\n\nscene_pal\n\n&lt;colors&gt;\nblack #E64B35FF #4DBBD5FF \n\n\nWe make two small dataframes for our primary examples here.\n\nbasin_to_plot &lt;- agged_data$mdb |&gt; \n  dplyr::filter(scenario %in% scenarios_to_plot) |&gt; \n  dplyr::left_join(scenarios, by = 'scenario')\n\n# Create a grouping variable\nobj_sdl_to_plot &lt;- agged_data$sdl_units |&gt;\n    dplyr::filter(scenario %in% scenarios_to_plot) |&gt; \n  dplyr::mutate(env_group = stringr::str_extract(env_obj, \"^[A-Z]+\")) |&gt;\n  dplyr::arrange(env_group, env_obj) |&gt; \n  dplyr::left_join(scenarios, by = 'scenario')",
    "crumbs": [
      "Comparer",
      "Syntax and details",
      "Setting limits"
    ]
  },
  {
    "objectID": "aggregator/temporal_agg.html",
    "href": "aggregator/temporal_agg.html",
    "title": "Temporal aggregation",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(dplyr)",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Temporal aggregation"
    ]
  },
  {
    "objectID": "aggregator/temporal_agg.html#data",
    "href": "aggregator/temporal_agg.html#data",
    "title": "Temporal aggregation",
    "section": "Data",
    "text": "Data\nInput data to should be a dataframe (e.g. a dataframe of EWR outputs, sf object if they are spatial outcomes). If we want to pass a path instead of a dataframe (as we might for large runs), we would use read_and_agg, which wraps multi_aggregate, demonstrated in its own notebook. Thus, for the demonstration, we pull in the the EWR output produced from the HydroBOT-provided hydrographs (system.file('extdata/testsmall/hydrographs', package = 'HydroBOT'), which we have processed already here and are at the paths above.\nWe’ll pull in the data to use for demonstration so we can use temporal_aggregate() directly. If we want to feed a path instead of a dataframe, we would use read_and_agg().\nThe data comes in as a timeseries but at fine ecological detail and across several gauges. We will do a first-pass aggregation to Target groups and sdl units before the temporal aggregation for clarity.\n\newr_out &lt;- prep_run_save_ewrs(\n  hydro_dir = hydro_dir,\n  output_parent_dir = project_dir,\n  outputType = list('none'),\n  returnType = list('yearly')\n)\n\n\n# This is just a simple prep step that is usually done internally to put the geographic coordinates on input data\newrdata &lt;- prep_ewr_output(ewr_out$yearly, type = 'achievement', add_max = FALSE)\n\n# This gets us to Target at the SDL unit every year\npreseq &lt;- list(\n  env_obj = c(\"ewr_code_timing\", \"env_obj\"),\n  sdl_units = sdl_units,\n  Target = c(\"env_obj\", \"Target\")\n  )\n\n\nfunseq &lt;- list(\n  all_time = 'ArithmeticMean',\n  ewr_code = 'ArithmeticMean',\n  env_obj = 'ArithmeticMean'\n)\n\n# Do the aggregation to get output at each gauge averaged over time\nsimpleAgg &lt;- multi_aggregate(\n  dat = ewrdata,\n  causal_edges = causal_ewr,\n  groupers = c(\"scenario\"),\n  auto_ewr_PU = TRUE,\n  aggCols = \"ewr_achieved\",\n  aggsequence = preseq,\n  funsequence = funseq\n)\n\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`\n.\n! Unmatched links in causal network\n• 29 from ewr_code_timing to env_obj\nℹ EWR gauges joined to larger units pseudo-spatially.\n• Done automatically because `auto_ewr_PU = TRUE`\n• Non-spatial join needed because gauges may inform areas they are not within\n• Best to explicitly use `pseudo_spatial = 'sdl_units'` in `multi_aggregate()` or `read_and_agg()`.\n\nsimpleAgg\n\n\n  \n\n\n\nAnd to confirm, that has retained years, though there are only 5 in the test data.\n\nunique(simpleAgg$date)\n\n[1] \"2014-07-01\" \"2015-07-01\" \"2016-07-01\" \"2017-07-01\" \"2018-07-01\"\n[6] \"2019-07-01\"\n\n\nWe can plot that data (for just one scenario) to better see what it looks like.\n\nsimpleAgg |&gt; \n  agg_names_to_cols(aggsequence = names(preseq),\n                    funsequence = funseq,\n                    aggCols = 'ewr_achieved') |&gt; \n  filter(scenario == 'base') |&gt; \n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    y_lab = \"Arithmetic Mean\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"Target\",\n    facet_row = \"date\"\n  )",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Temporal aggregation"
    ]
  },
  {
    "objectID": "aggregator/temporal_agg.html#collapse-the-full-timeseries",
    "href": "aggregator/temporal_agg.html#collapse-the-full-timeseries",
    "title": "Temporal aggregation",
    "section": "Collapse the full timeseries",
    "text": "Collapse the full timeseries\nWe often just want to simply get some summary over the full timeseries. It uses the special value 'all_time' in breaks for simplicity, though specifying the start and end works as well. Think carefully though, it very well make more sense to weight this by recency or similar.\n\nfull_period &lt;- temporal_aggregate(simpleAgg, \n                                  breaks = 'all_time',\n                                  groupers = c('scenario', 'SWSDLName', 'Target'),\n                                  aggCols = 'ewr_achieved',\n                                  funlist = 'ArithmeticMean')\n\nNow we only have one value and have lost the date column\n\nfull_period |&gt; \n  agg_names_to_cols(aggsequence = c(names(preseq), 'all_time'),\n                    funsequence = c(funseq, list('ArithmeticMean')),\n                    aggCols = 'ewr_achieved') |&gt; \n  filter(scenario == 'base') |&gt; \n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    y_lab = \"Arithmetic Mean\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"Target\"\n  )",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Temporal aggregation"
    ]
  },
  {
    "objectID": "aggregator/temporal_agg.html#specified-breaks",
    "href": "aggregator/temporal_agg.html#specified-breaks",
    "title": "Temporal aggregation",
    "section": "Specified breaks",
    "text": "Specified breaks\nWe may want to specify breaks, which we can do by feeding times at the breakpoints. Here, we demonstrate with two-year intervals since the period is short, but these might be e.g. decades (if long) or seasonal (if fine-scaled data).\n\ntime_breaks &lt;- c('2014-01-01', '2016-01-01', '2018-01-01', '2020-01-01')\ntg &lt;- lubridate::ymd(time_breaks)\n\ntwo_years &lt;- temporal_aggregate(simpleAgg, \n                                  breaks = tg,\n                                  groupers = c('scenario', 'SWSDLName', 'Target'),\n                                  aggCols = 'ewr_achieved',\n                                  funlist = 'ArithmeticMean')\n\nAnd the plot\n\ntwo_years |&gt; \n  agg_names_to_cols(aggsequence = c(names(preseq), 'all_time'),\n                    funsequence = c(funseq, list('ArithmeticMean')),\n                    aggCols = 'ewr_achieved') |&gt; \n  filter(scenario == 'base') |&gt; \n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    y_lab = \"Arithmetic Mean\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"Target\",\n    facet_row = 'date'\n  )",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Temporal aggregation"
    ]
  },
  {
    "objectID": "aggregator/temporal_agg.html#interval-specifications",
    "href": "aggregator/temporal_agg.html#interval-specifications",
    "title": "Temporal aggregation",
    "section": "Interval specifications",
    "text": "Interval specifications\nThe temporal_aggregate() function relies on base::cut.POSIXt() internally, and so can use the break specifications allowed there, where characters can be used to define lengths and units of the intervals. See ?base::cut.POSIXt for more information. This can be very useful for sub-yearly aggregations (e.g. ‘2 months’, ‘quarter’, etc), or with long timeseries allowing the use of things like ‘10 years’. Here we demonstrate with 3 years due to the short demonstration timeseries.\n\nthree_years_cut &lt;- temporal_aggregate(simpleAgg, \n                                  breaks = '3 years',\n                                  groupers = c('scenario', 'SWSDLName', 'Target'),\n                                  aggCols = 'ewr_achieved',\n                                  funlist = 'ArithmeticMean')\n\nAnd the plot\n\nthree_years_cut |&gt; \n  agg_names_to_cols(aggsequence = c(names(preseq), 'all_time'),\n                    funsequence = c(funseq, list('ArithmeticMean')),\n                    aggCols = 'ewr_achieved') |&gt; \n  filter(scenario == 'base') |&gt; \n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    y_lab = \"Arithmetic Mean\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"Target\",\n    facet_row = 'date'\n  )",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Temporal aggregation"
    ]
  },
  {
    "objectID": "provided_data/spatial_data.html",
    "href": "provided_data/spatial_data.html",
    "title": "Spatial data",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(ggplot2)",
    "crumbs": [
      "Provided spatial data",
      "Spatial data"
    ]
  },
  {
    "objectID": "provided_data/spatial_data.html#visualizing-spatial-data-from-hydrobot",
    "href": "provided_data/spatial_data.html#visualizing-spatial-data-from-hydrobot",
    "title": "Spatial data",
    "section": "Visualizing spatial data from HydroBOT",
    "text": "Visualizing spatial data from HydroBOT\nThe {HydroBOT} package provides a standard set of spatial data commonly needed for analyses in the Murray-Darling Basin. Here, we provide visualisation of these datasets.\nThe datasets are bom_basin_gauges (points), and basin (the MDB as a single polygon), sdl_units, resource_plan_areas, cewo_valleys, planning_units, and basin_rivers.\nBasin\n\nggplot(basin) +\n  geom_sf(fill = \"powderblue\")\n\n\n\n\n\n\n\nResource plan areas\n\nggplot(resource_plan_areas) +\n  geom_sf(aes(fill = SWWRPANAME), show.legend = FALSE) +\n  geom_sf_label(aes(label = SWWRPANAME), size = 3, label.padding = unit(0.1, \"lines\")) +\n  colorspace::scale_fill_discrete_qualitative(palette = \"Set2\")\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\nThese have ‘SW’ codes\n\nresource_plan_areas\n\n\n  \n\n\n\nSDL plan areas\n\nggplot(sdl_units) +\n  geom_sf(aes(fill = SWSDLName), show.legend = FALSE) +\n  geom_sf_label(aes(label = SWSDLName),\n    size = 3, label.padding = unit(0.1, \"lines\")\n  ) +\n  colorspace::scale_fill_discrete_qualitative(palette = \"Set2\")\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\nThese have ‘SS’ codes.\n\nsdl_units\n\n\n  \n\n\n\nCatchments\n\nggplot(cewo_valleys) +\n  geom_sf(aes(fill = ValleyName), show.legend = FALSE) +\n  geom_sf_label(aes(label = ValleyName),\n    size = 3, label.padding = unit(0.1, \"lines\")\n  ) +\n  colorspace::scale_fill_discrete_qualitative(palette = \"Set2\")\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\nThese have names, ID, and ValleyCodes\n\ncewo_valleys\n\n\n  \n\n\n\nGauges\n\nggplot() +\n  geom_sf(data = basin, fill = \"powderblue\") +\n  geom_sf(data = bom_basin_gauges)\n\n\n\n\n\n\n\n\nbom_basin_gauges\n\n\n  \n\n\n\nGauges, sdl, basin\n\ngauges_sdl &lt;- ggplot() +\n  geom_sf(data = sdl_units, fill = \"cadetblue\", color = \"grey40\") +\n  geom_sf(data = bom_basin_gauges, color = \"goldenrod\", alpha = 0.5) +\n  colorspace::scale_fill_discrete_qualitative(palette = \"Set2\") +\n  theme_hydrobot()\ngauges_sdl\n\n\n\n\n\n\n\nPlanning units\nThese are most relevant in NSW; analysis at basin scale often skips and goes straight to the sdl unit.\n\nplanning_sdl &lt;- ggplot() +\n  geom_sf(data = planning_units, mapping = aes(fill = PlanningUnitName)) +\n  colorspace::scale_fill_discrete_qualitative(palette = \"Set2\") +\n  theme_hydrobot(legend.position = \"none\")\nplanning_sdl\n\n\n\n\n\n\n\nRivers\nDespite the gauges falling on rivers, analyses tend to be in polygons (planning units, sdl units, etc) and not restricted to the channel. That said, visualising the location of the rivers can be very helpful.\n\ngauges_rivers &lt;- ggplot() +\n  geom_sf(data = basin, fill = NA, color = \"grey40\") +\n  geom_sf(\n    data = bom_basin_gauges, color = \"goldenrod\",\n    alpha = 0.5\n  ) +\n  geom_sf(data = basin_rivers, color = \"cadetblue\") +\n  theme_hydrobot()\ngauges_rivers",
    "crumbs": [
      "Provided spatial data",
      "Spatial data"
    ]
  },
  {
    "objectID": "controller/controller_ewr_stepthrough.html",
    "href": "controller/controller_ewr_stepthrough.html",
    "title": "Scenario controller internals",
    "section": "",
    "text": "library(HydroBOT)\nThe controller primarily sets the paths to scenarios, calls the modules, and saves the output and metadata. In normal use, the series of steps below is wrapped with prep_run_save_ewrs, as shown here and here, which allows cleaner automation and saves the metadata. I’m stepping through the internal parts of prep_run_save_ewrs here to show more clearly what’s happening when that runs; this document is intended to expose some of the inner workings of the black box. Wrapped versions of the controller alone and in a combined workflow are available to illustrate this more normal use.\nThis document focuses on the parts of the internal flow that help understand what’s happening and those that may sometimes be useful to run ad-hoc for e.g. troubleshooting to see what HydroBOT is doing on a more granular level. This document skips over quite a lot of error-checking, safeguarding, and formatting, as well as parallelisation structure and metadata saving that happens within prep_run-save_ewrs.",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Detailed step through"
    ]
  },
  {
    "objectID": "controller/controller_ewr_stepthrough.html#set-paths",
    "href": "controller/controller_ewr_stepthrough.html#set-paths",
    "title": "Scenario controller internals",
    "section": "Set paths",
    "text": "Set paths\nWe need to identify the path to the hydrographs and set up directories for outpus. In use, the hydrograph paths would typically point to external shared directories. The cleanest, default, situation is for everything to be in a single outer directory project_dir, and there should be an inner directory with the input data /hydrographs.\n\n\n\n\n\n\nTip\n\n\n\nWithin /hydrographs, scenarios should be kept in separate folders, i.e. files for gauges or all gauges within a catchment, basin, etc, within directories for scenarios (see here). This allows cleaner scenario structures and parallelisation. Any given run needs all the locations within a scenario, but scenarios should run separately (possibly in parallel) because outcomes (e.g. EWRs, fish performance) cannot logically depend on other scenarios representing other hydrological sequences or climates. A common situation that is much more cumbersome is to have the directory structure reflect gauges or other spatial unit, and files within them per scenario. It is worth restructuring your files if this is the case.\n\n\nIt also works to point to a single scenario, as might be the case if HydroBOT runs off the end of a hydrology model that generates that scenario, e.g. /hydrographs/scenario1. This allows both targeting single scenarios for HydroBOT analysis, but also batching hydrology and HydroBOT together. By default, the saved data goes to project_dir/module_output automatically, though this can be changed, see the output_parent_dir and output_subdir arguments.\n\nproject_dir &lt;- file.path(\"hydrobot_scenarios\")\nhydro_dir &lt;- file.path(project_dir, \"hydrographs\")",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Detailed step through"
    ]
  },
  {
    "objectID": "controller/controller_ewr_stepthrough.html#format",
    "href": "controller/controller_ewr_stepthrough.html#format",
    "title": "Scenario controller internals",
    "section": "Format",
    "text": "Format\nWe need to pass the data format to the downstream modules so they can parse the data. Currently the demo csvs are created in a format that parses like Standard time-series, and the demo netcdfs parse in the bespoke IQQM - netcdf format. Any available option in the EWR tool will work, see ?prep_run_save_ewrs.\nWe also set the output type from the EWR tool. The ‘yearly’ is needed in most cases, but any option is available, see ?prep_run_save_ewrs. It must be a list in order to pass correctly to python.\n\nmodel_format &lt;- \"Standard time-series\"\noutputType &lt;- list(\"yearly\", \"summary\")",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Detailed step through"
    ]
  },
  {
    "objectID": "controller/controller_ewr_stepthrough.html#set-up-output-directories",
    "href": "controller/controller_ewr_stepthrough.html#set-up-output-directories",
    "title": "Scenario controller internals",
    "section": "Set up output directories",
    "text": "Set up output directories\nWe get the information about the gauges and filepaths project_dir with find_scenario_paths. The names of the resulting list of paths are the names of the scenarios. Note that including a scenarios argument to [prep_run_save_ewrs()] overrides this, allowing passing in this list explicitly if there is no good way to parse scenarios and file names (e.g. perhaps to run a subset of scenarios across several different directories).\n\n# get the paths to all the hydrographs\nhydro_paths &lt;- find_scenario_paths(hydro_dir, type = \"csv\")\n\nThe output directory and subdirs for scenarios is created by make_output_dir, which also returns that outer directory location. Note that output_subdir allows subdirectories. This can be useful for running different analyses on the same set of hydrographs.\n\n# set up the output directory\noutput_path &lt;- make_output_dir(\n  parent_dir = project_dir,\n  scenarios = names(hydro_paths),\n  module_name = \"EWR\",\n  subdir = \"example\",\n  ewr_outtypes = unlist(outputType)\n)\n\nThis directory machinery makes the file_search and fill_missing arguments possible to ensure only a subset of files are run or missing files are able to be run if, for example, a long run crashed. See ?prep_run_save_ewrs and the main controller page.",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Detailed step through"
    ]
  },
  {
    "objectID": "controller/controller_ewr_stepthrough.html#run-the-ewr-tool",
    "href": "controller/controller_ewr_stepthrough.html#run-the-ewr-tool",
    "title": "Scenario controller internals",
    "section": "Run the ewr tool",
    "text": "Run the ewr tool\nNow we run the ewr tool with the parameters given and save the output. The EWR tool is in python, and HydroBOT provides some linking python functions. This is all handled internally to HydroBOT, but here we need to import these functions to demonstrate. It is designed to loop over scenarios (in parallel with rparallel = TRUE), and so here we only run the first scenario.\nThe outputType argument we’ve seen earlier as it determines the saved outputs and so matters for the directory setup, while the returnType argument determines what gets returned to the active R session.\n\ncontroller_functions &lt;- reticulate::import_from_path(\"controller_functions\",\n  path = system.file(\"python\",\n    package = \"HydroBOT\"\n  ),\n  delay_load = TRUE\n)\n\newr_out &lt;- controller_functions$run_save_ewrs(\n  hydro_paths[[1]],\n  output_path,\n  model_format,\n  outputType = outputType,\n  returnType = list(\"summary\"),\n  scenario_name = names(hydro_paths)[1],\n  scenarios_from = \"directory\"\n)\n\nBriefly, we can see that that has returned dataframes from the EWR. Typically, though, we just save this out.\n\newr_out$summary\n\n\n  \n\n\n\nWithout running prep_run_save_ewrs, we have not saved the metadata.",
    "crumbs": [
      "Controller",
      "Simple demonstration",
      "Detailed step through"
    ]
  },
  {
    "objectID": "aggregator/user_functions.html",
    "href": "aggregator/user_functions.html",
    "title": "User-provided functions",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(dplyr)\nlibrary(ggplot2)\nIn most of the demonstrations, we have used some of the named functions provided by {HydroBOT} or those that are provided by R itself. However, it is possible for a user to develop custom functions and use those as well in the funsequence of multi_aggregate() and read_and_agg() (and in some other functions as well like baseline_compare() (see a contrived example). The most stable and best practice for specifying aggregation functions is to specify them as a named function, and supply that to the funsequence. See the aggregation syntax for a full treatment of the way functions can be specified. We demonstrate here with the best-supported method for with read_and_agg(), since that is the most common situation, but it also works with multi_aggregate.\nAs always, we need to point to the data.\nproject_dir &lt;- \"hydrobot_scenarios\"\nhydro_dir &lt;- file.path(project_dir, 'hydrographs')\newr_results &lt;- file.path(project_dir, \"module_output\", \"EWR\")\nagg_results &lt;- file.path(project_dir, \"aggregator_output\", \"demo\")",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "User-supplied functions"
    ]
  },
  {
    "objectID": "aggregator/user_functions.html#defining-functions",
    "href": "aggregator/user_functions.html#defining-functions",
    "title": "User-provided functions",
    "section": "Defining functions",
    "text": "Defining functions\nWe demonstrate here with a threshold function and a slightly-modified median with na.rm = TRUE. For example, we might want to know the mean of all values greater than 0 for some stages and use the median at others.\n\nmean_given_occurred &lt;- function(x) {\n  mean(ifelse(x &gt; 0, x, NA), na.rm = TRUE)\n}\n\nmedna &lt;- function(x) {\n  median(x, na.rm = TRUE)\n}\n\nNow we can put those in the funsequence:\n\naggseq_funs &lt;- list(\n  all_time = 'all_time',\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  env_obj = c(\"ewr_code\", \"env_obj\"),\n  sdl_units = sdl_units,\n  Target = c(\"env_obj\", \"Target\")\n  )\n\n\nfunseq_funs &lt;- list(\n  all_time = 'ArithmeticMean',\n  ewr_code = 'ArithmeticMean',\n  env_obj = 'mean_given_occurred',\n  sdl_units = 'medna',\n  Target = 'mean_given_occurred'\n)\n\nThose changes are then reflected in the aggregation history and determine the aggregated values.\n\nagged_custom_funs &lt;- read_and_agg(\n  datpath = ewr_results,\n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  group_until = list(\n    SWSDLName = is_notpoint,\n    planning_unit_name = is_notpoint,\n    gauge = is_notpoint\n  ),\n  pseudo_spatial = \"sdl_units\",\n  aggsequence = aggseq_funs,\n  funsequence = funseq_funs,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = FALSE,\n  returnList = TRUE,\n  savepath = agg_results,\n  add_max = FALSE\n)\n\nWe can plot them to double check it worked.\n\nagged_custom_funs$Target |&gt; \n  dplyr::filter(scenario != 'MAX')  |&gt;  \n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    y_lab = \"Arithmetic Mean\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    facet_col = \"Target\",\n    facet_row = \"scenario\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n)",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "User-supplied functions"
    ]
  },
  {
    "objectID": "aggregator/user_module.html",
    "href": "aggregator/user_module.html",
    "title": "Unincorporated module outputs",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nIn some cases, response models may not yet be integrated into HydroBOT, or may not be able to be integrated (e.g. if they are proprietary or unscriptable). In these cases, we can still use read_and_agg() for processing with the Aggregator and then Comparer. All we need is a csv of module outputs. For aggregation along the theme, space, and time dimensions, it needs to have information about those dimensions. It should have a column identified by readr::read_csv() as time, a column that matches a column in the causal network, and a column that matches a column in a spatial dataframe (sf object), as these last two are accomplished with joins. It will typically have a ‘scenario’ column as well. We will demonstrate here using read_and_agg() with the same examples used for multi_aggregate(). The difference here is that read_and_agg() is how most people will interact with HydroBOT’s aggregator and has to read the files from disk, rather than receive ready-prepared dataframes.\nSetup\nFirst, we create some dummy ‘module’ data, as in the multi_aggregate example, but here we save it out.\nWe need to know the spatial units to create the data:\n\naustates &lt;- rnaturalearth::ne_states(country = 'australia') |&gt; \n  dplyr::select(state = name, geometry)\n\nall_aus &lt;- rnaturalearth::ne_countries(country = 'australia') |&gt; \n  dplyr::select(geounit)\n\nThen we create the data and save it out\n\n# This is all copied from multi_aggregate except the last saving line.\n\n  # add a date column\n  state_inputs &lt;- austates |&gt;\n    dplyr::mutate(date = lubridate::ymd('20000101'))\n\n  # add some values\n  withr::with_seed(17,\n                   state_inputs &lt;- state_inputs |&gt;\n                     dplyr::mutate(value = runif(nrow(state_inputs)))\n  )\n\n  withr::with_seed(17,\n                   # add some more days, each with different values\n                   state_inputs &lt;- purrr::map(0:10,\n                                              \\(x) dplyr::mutate(state_inputs,\n                                                                 date = date + x,\n                                                                 value = value * rnorm(nrow(state_inputs),\n                                                                                       mean = x, sd = x/2))) |&gt;\n                     dplyr::bind_rows()\n                   )\n  \n    # add a scenario column, each with different values\n  state_inputs &lt;- purrr::imap(letters[1:4],\n                              \\(x,y) dplyr::mutate(state_inputs,\n                                            scenario = x,\n                                            value = value + y)) |&gt;\n    dplyr::bind_rows()\n\n    # add a theme-relevant column, each with different values\n  state_inputs &lt;- purrr::imap(c(\"E\", \"F\", \"G\", \"H\", \"I\", \"J\"),\n                              \\(x,y) dplyr::mutate(state_inputs,\n                                            theme1 = x,\n                                            value = value + y)) |&gt;\n    dplyr::bind_rows()\n\n\n  # This bit saves it out so we can use read_and_agg.\n  purrr::map(unique(state_inputs$scenario),\n             \\(x) dir.create(file.path(tempdir(), 'module_output', 'fake_module', x), \n                             recursive = TRUE))\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n  si &lt;- state_inputs |&gt; \n    sf::st_drop_geometry() |&gt; \n    split(state_inputs$scenario) |&gt; \n    purrr::iwalk(\n      \\(x,y) readr::write_csv(x, file.path(tempdir(), 'module_output', 'fake_module',\n                                       y, paste0(\"fakeout_\", y, '.csv')))\n      )\n\nAnd we need a simple causal network.\n\n# make a simple 'causal' network\n  state_theme &lt;- tibble::tibble(theme1 = c(\"E\", \"F\", \"G\", \"H\", \"I\", \"J\"),\n                                theme2 = c(\"vowel\", \"consonant\", \"consonant\",\n                                           \"consonant\", \"vowel\", \"consonant\")) |&gt; \n    list()\n\nUsing the Aggregator\nFirst, we set up some aggregation steps. Will just use means throughout.\n\n# This will aggregate into weeks, then to type, and then to the country.\n  ausseq &lt;- list(\n    week = 'week',\n    theme2 = c('theme1', 'theme2'),\n    all_aus = all_aus\n  )\n\n  # just use mean, since there are no NA in the data.\n  ausfuns &lt;- list(\n    week = 'mean',\n    type = 'mean',\n    all_aus = 'mean'\n  )\n\nDo the aggregation. Note that this warns about some built-in checks for the EWR module that are not relevant here.\n\n  # Do the aggregation\n  ausagg &lt;-  ausagg &lt;- read_and_agg(\n  datpath = file.path(tempdir(), 'module_output', 'fake_module'),\n  type = \"everything\",\n  geopath = austates,\n  causalpath = state_theme,\n  groupers = \"scenario\",\n  aggCols = \"value\",\n    aggsequence = ausseq,\n  funsequence = ausfuns,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = FALSE,\n  returnList = TRUE,\n  add_max = FALSE,\n  savepath = file.path(tempdir(), 'aggregator_output', 'dummy')\n)\n\nWarning in filtergroups(thisdf, fromcol = p[1], tocol = p[2], fromfilter =\nfromfilter, : Unable to cross-check gauges and planning units, trusting the\nuser they work together\n\n\nQuick plots of a couple levels\n\nausagg$week |&gt; \n  filter(theme1 == 'E') |&gt; \n    plot_outcomes(\n    outcome_col = \"value\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"value\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"date\"\n  )\n\n\n\n\n\n\n\n\nausagg$theme2 |&gt; \n  filter(date == lubridate::ymd(\"2000-01-03\")) |&gt; \n    plot_outcomes(\n    outcome_col = \"value\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"value\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"theme2\"\n  )\n\n\n\n\n\n\n\n\nausagg$all_aus |&gt; \n  filter(date == lubridate::ymd(\"2000-01-03\")) |&gt; \n    plot_outcomes(\n    outcome_col = \"value\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"value\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"theme2\"\n  )",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "User-supplied modules"
    ]
  },
  {
    "objectID": "aggregator/user_spatial.html",
    "href": "aggregator/user_spatial.html",
    "title": "User-provided spatial data",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(dplyr)\nlibrary(ggplot2)\nIn most of the demonstrations, we have used the spatial data provided by {HydroBOT}. However, spatial data is an argument to multi_aggregate() and read_and_agg(), and so it is possible for the user to pass arbitrary spatial datasets. as {sf} objects. The only requirement is that they are sf objects with a geometry column. To demonstrate, we’ll download Australian states and aggregate into those. We do that here with read_and_agg(), since that is the most common situation, but it also works with multi_aggregate.\nAs always, we need to point to the data.\nproject_dir &lt;- \"hydrobot_scenarios\"\nhydro_dir &lt;- file.path(project_dir, 'hydrographs')\newr_results &lt;- file.path(project_dir, \"module_output\", \"EWR\")\nagg_results &lt;- file.path(project_dir, \"aggregator_output\", \"demo\")",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "User-supplied spatial data"
    ]
  },
  {
    "objectID": "aggregator/user_spatial.html#new-spatial-units",
    "href": "aggregator/user_spatial.html#new-spatial-units",
    "title": "User-provided spatial data",
    "section": "New spatial units",
    "text": "New spatial units\n\naustates &lt;- rnaturalearth::ne_states(country = 'australia') |&gt; \n  dplyr::select(state = name, geometry)\n\nWe’ll set keepAllPolys = TRUE since the data is all in NSW and we want to show that the other states are there.\n\naggseq_states &lt;- list(\n  all_time = 'all_time',\n  env_obj = c(\"ewr_code_timing\", \"env_obj\"),\n  Target = c(\"env_obj\", 'Target'),\n  state = austates\n)\n\nfunseq_states &lt;- list(\n  all_time = 'ArithmeticMean',\n  env_obj = 'ArithmeticMean',\n  Target = 'ArithmeticMean',\n  state = 'ArithmeticMean'\n)\n\nstate_agg &lt;- read_and_agg(\n  datpath = ewr_results,\n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  group_until = list(\n    SWSDLName = is_notpoint,\n    planning_unit_name = is_notpoint,\n    gauge = is_notpoint\n  ),\n  aggsequence = aggseq_states,\n  funsequence = funseq_states,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = TRUE,\n  returnList = TRUE,\n  add_max = FALSE\n)\n\n! Unmatched links in causal network\n• 29 from ewr_code_timing to env_obj\n\nstate_agg\n\n$agg_input\nSimple feature collection with 2808 features and 12 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 144.8811 ymin: -33.8695 xmax: 148.6839 ymax: -30.4577\nGeodetic CRS:  GDA94\n# A tibble: 2,808 × 13\n   scenario  year date       gauge  planning_unit_name  state SWSDLName ewr_code\n   &lt;chr&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;  &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;   \n 1 base      2014 2014-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 2 base      2015 2015-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 3 base      2016 2016-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 4 base      2017 2017-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 5 base      2018 2018-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 6 base      2019 2019-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 7 base      2014 2014-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 8 base      2015 2015-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 9 base      2016 2016-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n10 base      2017 2017-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n# ℹ 2,798 more rows\n# ℹ 5 more variables: ewr_code_timing &lt;chr&gt;, event_years &lt;dbl&gt;,\n#   ewr_achieved &lt;dbl&gt;, interevent_achieved &lt;dbl&gt;, geometry &lt;POINT [°]&gt;\n\n$all_time\nSimple feature collection with 414 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 144.8811 ymin: -33.8695 xmax: 148.6839 ymax: -30.4577\nGeodetic CRS:  GDA94\n# A tibble: 414 × 10\n   scenario SWSDLName planning_unit_name            gauge ewr_code_timing polyID\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;                         &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt; \n 1 base     Lachlan   Lachlan River - Lake Cargell… 4120… BF1_a           r4pdr…\n 2 base     Lachlan   Lachlan River - Lake Cargell… 4120… BF1_b           r4pdr…\n 3 base     Lachlan   Lachlan River - Lake Cargell… 4120… BF2_a           r4pdr…\n 4 base     Lachlan   Lachlan River - Lake Cargell… 4120… BF2_b           r4pdr…\n 5 base     Lachlan   Lachlan River - Lake Cargell… 4120… BK1_P           r4pdr…\n 6 base     Lachlan   Lachlan River - Lake Cargell… 4120… BK1_S           r4pdr…\n 7 base     Lachlan   Lachlan River - Lake Cargell… 4120… CF1_b           r4pdr…\n 8 base     Lachlan   Lachlan River - Lake Cargell… 4120… CF1_c           r4pdr…\n 9 base     Lachlan   Lachlan River - Lake Cargell… 4120… LF1_P           r4pdr…\n10 base     Lachlan   Lachlan River - Lake Cargell… 4120… LF1_S           r4pdr…\n# ℹ 404 more rows\n# ℹ 4 more variables: geometry &lt;POINT [°]&gt;, ewr_achieved &lt;dbl&gt;, aggfun_1 &lt;chr&gt;,\n#   aggLevel_1 &lt;chr&gt;\n\n$env_obj\nSimple feature collection with 384 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 144.8811 ymin: -33.8695 xmax: 148.6839 ymax: -30.4577\nGeodetic CRS:  GDA94\n# A tibble: 384 × 12\n   scenario SWSDLName planning_unit_name gauge  polyID      env_obj\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;              &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;  \n 1 base     Lachlan   Merrimajeel Creek  412005 r1zp2f5py7d EF1    \n 2 base     Lachlan   Merrimajeel Creek  412005 r1zp2f5py7d EF2    \n 3 base     Lachlan   Merrimajeel Creek  412005 r1zp2f5py7d EF3    \n 4 base     Lachlan   Merrimajeel Creek  412005 r1zp2f5py7d EF4    \n 5 base     Lachlan   Merrimajeel Creek  412005 r1zp2f5py7d EF5    \n 6 base     Lachlan   Merrimajeel Creek  412005 r1zp2f5py7d EF6    \n 7 base     Lachlan   Merrimajeel Creek  412005 r1zp2f5py7d NF1    \n 8 base     Lachlan   Merrimajeel Creek  412005 r1zp2f5py7d NF3    \n 9 base     Lachlan   Merrimajeel Creek  412005 r1zp2f5py7d NF7    \n10 base     Lachlan   Merrimajeel Creek  412005 r1zp2f5py7d NV1    \n# ℹ 374 more rows\n# ℹ 6 more variables: geometry &lt;POINT [°]&gt;, ewr_achieved &lt;dbl&gt;, aggfun_1 &lt;chr&gt;,\n#   aggLevel_1 &lt;chr&gt;, aggfun_2 &lt;chr&gt;, aggLevel_2 &lt;chr&gt;\n\n$Target\nSimple feature collection with 72 features and 13 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 144.8811 ymin: -33.8695 xmax: 148.6839 ymax: -30.4577\nGeodetic CRS:  GDA94\n# A tibble: 72 × 14\n   scenario SWSDLName planning_unit_name  gauge  polyID      Target             \n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;              \n 1 base     Lachlan   Merrimajeel Creek   412005 r1zp2f5py7d Native fish        \n 2 base     Lachlan   Merrimajeel Creek   412005 r1zp2f5py7d Native vegetation  \n 3 base     Lachlan   Merrimajeel Creek   412005 r1zp2f5py7d Other species      \n 4 base     Lachlan   Merrimajeel Creek   412005 r1zp2f5py7d Priority ecosystem…\n 5 base     Lachlan   Merrimajeel Creek   412005 r1zp2f5py7d Waterbird          \n 6 base     Lachlan   Upper Lachlan River 412002 r3cxx2uk3zx Native fish        \n 7 base     Lachlan   Upper Lachlan River 412002 r3cxx2uk3zx Native vegetation  \n 8 base     Lachlan   Upper Lachlan River 412002 r3cxx2uk3zx Other species      \n 9 base     Lachlan   Upper Lachlan River 412002 r3cxx2uk3zx Priority ecosystem…\n10 base     Lachlan   Upper Lachlan River 412002 r3cxx2uk3zx Waterbird          \n# ℹ 62 more rows\n# ℹ 8 more variables: geometry &lt;POINT [°]&gt;, ewr_achieved &lt;dbl&gt;, aggfun_1 &lt;chr&gt;,\n#   aggLevel_1 &lt;chr&gt;, aggfun_2 &lt;chr&gt;, aggLevel_2 &lt;chr&gt;, aggfun_3 &lt;chr&gt;,\n#   aggLevel_3 &lt;chr&gt;\n\n$state\nSimple feature collection with 165 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 112.9194 ymin: -54.75042 xmax: 159.1065 ymax: -9.240167\nGeodetic CRS:  WGS 84\n# A tibble: 165 × 14\n   scenario Target  polyID state                  geometry ewr_achieved aggfun_1\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;        &lt;MULTIPOLYGON [°]&gt;        &lt;dbl&gt; &lt;chr&gt;   \n 1 base     Native… r60v0… New … (((153.4723 -28.17528, 1…        0.502 Arithme…\n 2 base     Native… r60v0… New … (((153.4723 -28.17528, 1…        0.319 Arithme…\n 3 base     Other … r60v0… New … (((153.4723 -28.17528, 1…        0.349 Arithme…\n 4 base     Priori… r60v0… New … (((153.4723 -28.17528, 1…        0.426 Arithme…\n 5 base     Waterb… r60v0… New … (((153.4723 -28.17528, 1…        0.254 Arithme…\n 6 down4    Native… r60v0… New … (((153.4723 -28.17528, 1…        0.292 Arithme…\n 7 down4    Native… r60v0… New … (((153.4723 -28.17528, 1…        0.138 Arithme…\n 8 down4    Other … r60v0… New … (((153.4723 -28.17528, 1…        0.129 Arithme…\n 9 down4    Priori… r60v0… New … (((153.4723 -28.17528, 1…        0.232 Arithme…\n10 down4    Waterb… r60v0… New … (((153.4723 -28.17528, 1…        0.05  Arithme…\n# ℹ 155 more rows\n# ℹ 7 more variables: aggLevel_1 &lt;chr&gt;, aggfun_2 &lt;chr&gt;, aggLevel_2 &lt;chr&gt;,\n#   aggfun_3 &lt;chr&gt;, aggLevel_3 &lt;chr&gt;, aggfun_4 &lt;chr&gt;, aggLevel_4 &lt;chr&gt;\n\n\nAnd a quick plot of that to see the different spatial units (though this example is all in New South Wales so other states have no data).\n\nstate_agg$state |&gt; \n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"Target\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n  )",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "User-supplied spatial data"
    ]
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Saving each step",
    "section": "",
    "text": "The {HydroBOT} package needs to be installed to provide all functions used here. It also provides some necessary data for the causal network relationships, and canonical shapefiles that have been prepped.\nInstallation is slightly more complex than the usual devtools::install_github(MDBAuth/HydroBOT) because the repo is private. The SSH method seems to be the most robust, though https with a github PAT is often the recommendation for private repos. In general, just use the below. A templaterepo is available, however, with necessary setup scripts and test notebooks for a range of systems and uses.\n\nTo set up SSH, go to the github instructions here and then here\nTo set up a PAT, {usethis} has a good walkthrough.\n\n\n# GITHUB INSTALL\n# SSH- seems most stable, but sometimes git2r loses its ssh ability and so use `git = 'external'`\n\ndevtools::install_git(\"git@github.com:MDBAuth/HydroBOT.git\",\n  ref = \"master\", force = TRUE, upgrade = \"ask\", git = 'external'\n)\n\n# HTTPS- may work, if you have gitcreds and a PAT set up\n# activate the PAT\ngitcreds::gitcreds_get()\nrenv::install(\"MDBAuth/HydroBOT\")\n\n# pak also requires a PAT\npak::pkg_install(\"MDBAuth/HydroBOT\")\n\n\n\n\n\n\n\nPython dependencies\n\n\n\nHydroBOT depends on the py-ewr python package. In most cases, users do not need to do anything, and {reticulate} will install that package automatically on first use. If more control over python environments is desired, users will need to set up an environment containing py-ewr. There is a pyproject.toml file in this repo and the HydroBOT repo that should allow you to build an environment with poetry, for more detail see developer notes. The template repository provides installation templates that handle installing poetry, pyenv, py-ewr and HydroBOT itself on a range of systems.\n\n\nLoad the package\n\nlibrary(HydroBOT)\n\nLoading required package: sf\n\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE"
  },
  {
    "objectID": "getting_started.html#installing-hydrobot",
    "href": "getting_started.html#installing-hydrobot",
    "title": "Saving each step",
    "section": "",
    "text": "The {HydroBOT} package needs to be installed to provide all functions used here. It also provides some necessary data for the causal network relationships, and canonical shapefiles that have been prepped.\nInstallation is slightly more complex than the usual devtools::install_github(MDBAuth/HydroBOT) because the repo is private. The SSH method seems to be the most robust, though https with a github PAT is often the recommendation for private repos. In general, just use the below. A templaterepo is available, however, with necessary setup scripts and test notebooks for a range of systems and uses.\n\nTo set up SSH, go to the github instructions here and then here\nTo set up a PAT, {usethis} has a good walkthrough.\n\n\n# GITHUB INSTALL\n# SSH- seems most stable, but sometimes git2r loses its ssh ability and so use `git = 'external'`\n\ndevtools::install_git(\"git@github.com:MDBAuth/HydroBOT.git\",\n  ref = \"master\", force = TRUE, upgrade = \"ask\", git = 'external'\n)\n\n# HTTPS- may work, if you have gitcreds and a PAT set up\n# activate the PAT\ngitcreds::gitcreds_get()\nrenv::install(\"MDBAuth/HydroBOT\")\n\n# pak also requires a PAT\npak::pkg_install(\"MDBAuth/HydroBOT\")\n\n\n\n\n\n\n\nPython dependencies\n\n\n\nHydroBOT depends on the py-ewr python package. In most cases, users do not need to do anything, and {reticulate} will install that package automatically on first use. If more control over python environments is desired, users will need to set up an environment containing py-ewr. There is a pyproject.toml file in this repo and the HydroBOT repo that should allow you to build an environment with poetry, for more detail see developer notes. The template repository provides installation templates that handle installing poetry, pyenv, py-ewr and HydroBOT itself on a range of systems.\n\n\nLoad the package\n\nlibrary(HydroBOT)\n\nLoading required package: sf\n\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE"
  },
  {
    "objectID": "getting_started.html#example-workflow",
    "href": "getting_started.html#example-workflow",
    "title": "Saving each step",
    "section": "Example workflow",
    "text": "Example workflow\nThe basic workflow is to point to the hydrographs and run the modules in the controller, then scale the outcomes in the aggregator, and synthesise outputs with the comparer. More details are available at those links, and several different workflows are demonstrated as well; a simple example follows.\nA common way to run HydroBOT is to use a single document (i.e. not running the Controller, Aggregator, and Comparer as separate notebooks), but save the outputs at each step. This allows re-starting analyses if necessary and is particularly useful for large jobs, especially if batching over many scenarios. That said, in practice, once batching takes more than several minutes per stage, it often does make most sense to split into separate notebooks or scripts for each step.\nRetaining everything in-memory is a very simple flip of a switch, demoed in its own doc.\nThe getting started page uses this sort of workflow.\n\nlibrary(HydroBOT)"
  },
  {
    "objectID": "getting_started.html#directories",
    "href": "getting_started.html#directories",
    "title": "Saving each step",
    "section": "Directories",
    "text": "Directories\nHere, we will use the example hydrographs that come with HydroBOT.\nNormally project_dir and hydro_dir would point somewhere external (and typically, having hydro_dir inside project_dir will make life easier).\n\nhydro_dir &lt;- system.file(\"extdata/testsmall/hydrographs\", package = \"HydroBOT\")\n\nproject_dir &lt;- file.path(\"test_dir\")\n\n# Generated data\n# EWR outputs (will be created here in controller, read from here in aggregator)\newr_results &lt;- file.path(project_dir, \"module_output\", \"EWR\")\n\n# outputs of aggregator. There may be multiple modules\nagg_results &lt;- file.path(project_dir, \"aggregator_output\")\n\nWe need the ‘yearly’ EWR outputs for HydroBOT processing, though we can get the EWR to return any of its options.\n\newr_out &lt;- prep_run_save_ewrs(\n  hydro_dir = hydro_dir,\n  output_parent_dir = project_dir,\n  outputType = list(\"yearly\"),\n  returnType = list(\"yearly\")\n)"
  },
  {
    "objectID": "getting_started.html#aggregation",
    "href": "getting_started.html#aggregation",
    "title": "Saving each step",
    "section": "Aggregation",
    "text": "Aggregation\nWe need to define an aggregation sequence, which specifies the steps along space, time, and value dimensions, as well as a matching sequence of aggregation functions at each step. The example here is a reasonable default for many situations, but should be considered carefully. Please see the aggregator section for a detailed treatment of the syntax and capabilities of these sequences.\nFor this example, if any of the ‘versions’ of an EWR pass, say it passes in step 2. If we think the ‘versions’ are actually separate EWRs for different purposes, we might use ‘ArithmeticMean’ in step 2 intead.\n\naggseq &lt;- list(\n  all_time = \"all_time\",\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  env_obj = c(\"ewr_code\", \"env_obj\"),\n  sdl_units = sdl_units,\n  Target = c(\"env_obj\", \"Target\"),\n  mdb = basin,\n  target_5_year_2024 = c(\"Target\", \"target_5_year_2024\")\n)\n\nfunseq &lt;- list(\n  all_time = \"ArithmeticMean\",\n  ewr_code = \"CompensatingFactor\",\n  env_obj = \"ArithmeticMean\",\n  sdl_units = \"ArithmeticMean\",\n  Target = \"ArithmeticMean\",\n  mdb = \"SpatialWeightedMean\",\n  target_5_year_2024 = \"ArithmeticMean\"\n)\n\nRun the aggregation. Use the auto_ewr_PU shortcut to auto-specify some needed grouping for EWR outputs (see here for more detail).\nRead the messages. Most of these are saying it’s more explicit to avoid auto_ewr_PU = TRUE, but there are also messages saying that the causal network is missing links.\n\naggout &lt;- read_and_agg(\n  datpath = ewr_results,\n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  auto_ewr_PU = TRUE,\n  aggsequence = aggseq,\n  funsequence = funseq,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = FALSE,\n  returnList = TRUE,\n  add_max = FALSE,\n  savepath = agg_results\n)\n\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`.\n\n\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`\n.\n! Unmatched links in causal network\n• 29 from ewr_code_timing to ewr_code\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`\n.\nℹ EWR gauges joined to larger units pseudo-spatially.\n• Done automatically because `auto_ewr_PU = TRUE`\n• Non-spatial join needed because gauges may inform areas they are not within\n• Best to explicitly use `pseudo_spatial = 'sdl_units'` in `multi_aggregate()` or `read_and_agg()`.\n\n! Unmatched links in causal network\n• 4 from Target to target_5_year_2024"
  },
  {
    "objectID": "getting_started.html#comparer",
    "href": "getting_started.html#comparer",
    "title": "Saving each step",
    "section": "Comparer",
    "text": "Comparer\nNow we can make a couple quick plots to see what we’ve made. For more detail about plotting options and controls, see the comparer.\nMaps and spatial scaling\n\nmap_example &lt;- aggout$env_obj |&gt;\n  dplyr::filter(env_obj == \"NF1\") |&gt; # Need to reduce dimensionality\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    sceneorder = c(\"down4\", \"base\", \"up4\"),\n    underlay_list = \"basin\"\n  ) +\n  ggplot2::theme(legend.position = \"bottom\")\n\nmap_example\n\n\n\n\n\n\n\nBars- SDL units and scenarios\nSDL unit differences across all environmental objectives\n\ncatchcompare &lt;- aggout$env_obj |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    colorset = \"SWSDLName\",\n    pal_list = list(\"calecopal::lake\"),\n    sceneorder = c(\"down4\", \"base\", \"up4\"),\n    position = \"dodge\"\n  )\n\ncatchcompare"
  },
  {
    "objectID": "provided_data/causal_overview.html",
    "href": "provided_data/causal_overview.html",
    "title": "Causal networks",
    "section": "",
    "text": "Causal networks are models that describe the relationships between climate, adaptation options and outcomes for environmental, cultural, social, and economic values and assets (that is, the elements of the quadruple bottom line) (adapted from (Peeters et al. 2022)). They include the links which form the basis of the Driver-indicator-response models (e.g. EWR tool: hydrology to indicators), in addition to the links that connect indicators to objectives that are defined for key values.\nCausal networks can show many relationships and outcomes. To illustrate here, we show the links between EWRs (hydrologic indicators), proximate environmental objectives, larger-scale objectives, and finally broad-based ecological groupings.\n\n\n\n\n\n\n\nThe causal networks for environmental, cultural, social, or economic values are derived from specific documents detailing the relationships between indicators and objectives for those bottom-line elements. For the present example, we focus on environmental values in two catchments of the Murray-Darling Basin and so we draw the causal relationships from the Long Term Water Plan (LTWP). The LTWP reports the environmental water requirements (EWRs) for spatially explicit objectives to be achieved. These objectives are aimed to support the completion of all elements of a lifecycle of an organism or group of organisms (taxonomic or spatial) (LTWP doc). Objectives are described for five target groups and are associated with long-term targets (5, 10, and 20 year) of the LTWP’s management strategies. The links from EWRs to environmental objectives to long-term targets are captured in the causal networks to enable assessment of outcomes in direct equivalence to the LTWP’s management strategies. \n\nThe causal networks enables 1) visual representation of the complex inter-relationships between scenario inputs (hydrographs) and river-related outcomes and 2) assessment of outcomes aggregated along the thematic dimension. The former, aids transparency, elucidating the targets and causal relationships behind the Driver-indicator-response models and is a useful device for communication about HydroBOT and its outputs. The latter, allows outcomes to be assessed for individual (or sets of) environmental objectives, target groups, long-term targets, or at the level of the quadruple bottom line to identify synergies and trade-offs among values. \n\nHydroBOT provides various functions for creation and manipulation of causal networks, depending on what needs to be plotted or investigated, and the causal plots page provides some examples of various ways they can be visualised with HydroBOT.\nHydroBOT provides the causal network for the EWR tool in the HydroBOT::causal_ewr data. Until recently, this was the only source. More recently (early 2025), the EWR tool itself provides its own causal networks. These are brought over to HydroBOT and become HydroBOT::causal_ewr after testing. This process happens frequently, but if there is any question about whether the networks are current, the network in the installed version of py-ewr can be accessed with HydroBOT::get_causal_ewr().\nOther networks can be supplied by the user to all functions that require them. In this case, the network requirements are to be a data frame or list of dataframes with the relevant mapping between levels and any other grouping variables. I.e. there might be columns for spatial units if the network varies in space, and then the ‘level1’ column might contain several rows with different values that all have the same value in the ‘level2’ column. See the structure of causal_ewr for an example.",
    "crumbs": [
      "Provided data and Causal networks",
      "Causal networks",
      "Causal network overview"
    ]
  },
  {
    "objectID": "provided_data/causal_overview.html#overview",
    "href": "provided_data/causal_overview.html#overview",
    "title": "Causal networks",
    "section": "",
    "text": "Causal networks are models that describe the relationships between climate, adaptation options and outcomes for environmental, cultural, social, and economic values and assets (that is, the elements of the quadruple bottom line) (adapted from (Peeters et al. 2022)). They include the links which form the basis of the Driver-indicator-response models (e.g. EWR tool: hydrology to indicators), in addition to the links that connect indicators to objectives that are defined for key values.\nCausal networks can show many relationships and outcomes. To illustrate here, we show the links between EWRs (hydrologic indicators), proximate environmental objectives, larger-scale objectives, and finally broad-based ecological groupings.\n\n\n\n\n\n\n\nThe causal networks for environmental, cultural, social, or economic values are derived from specific documents detailing the relationships between indicators and objectives for those bottom-line elements. For the present example, we focus on environmental values in two catchments of the Murray-Darling Basin and so we draw the causal relationships from the Long Term Water Plan (LTWP). The LTWP reports the environmental water requirements (EWRs) for spatially explicit objectives to be achieved. These objectives are aimed to support the completion of all elements of a lifecycle of an organism or group of organisms (taxonomic or spatial) (LTWP doc). Objectives are described for five target groups and are associated with long-term targets (5, 10, and 20 year) of the LTWP’s management strategies. The links from EWRs to environmental objectives to long-term targets are captured in the causal networks to enable assessment of outcomes in direct equivalence to the LTWP’s management strategies. \n\nThe causal networks enables 1) visual representation of the complex inter-relationships between scenario inputs (hydrographs) and river-related outcomes and 2) assessment of outcomes aggregated along the thematic dimension. The former, aids transparency, elucidating the targets and causal relationships behind the Driver-indicator-response models and is a useful device for communication about HydroBOT and its outputs. The latter, allows outcomes to be assessed for individual (or sets of) environmental objectives, target groups, long-term targets, or at the level of the quadruple bottom line to identify synergies and trade-offs among values. \n\nHydroBOT provides various functions for creation and manipulation of causal networks, depending on what needs to be plotted or investigated, and the causal plots page provides some examples of various ways they can be visualised with HydroBOT.\nHydroBOT provides the causal network for the EWR tool in the HydroBOT::causal_ewr data. Until recently, this was the only source. More recently (early 2025), the EWR tool itself provides its own causal networks. These are brought over to HydroBOT and become HydroBOT::causal_ewr after testing. This process happens frequently, but if there is any question about whether the networks are current, the network in the installed version of py-ewr can be accessed with HydroBOT::get_causal_ewr().\nOther networks can be supplied by the user to all functions that require them. In this case, the network requirements are to be a data frame or list of dataframes with the relevant mapping between levels and any other grouping variables. I.e. there might be columns for spatial units if the network varies in space, and then the ‘level1’ column might contain several rows with different values that all have the same value in the ‘level2’ column. See the structure of causal_ewr for an example.",
    "crumbs": [
      "Provided data and Causal networks",
      "Causal networks",
      "Causal network overview"
    ]
  },
  {
    "objectID": "workflows/workflow_overview.html",
    "href": "workflows/workflow_overview.html",
    "title": "Workflow overview",
    "section": "",
    "text": "HydroBOT may be used stepwise, that is calling the Controller, Aggregator, and Comparer in separate scripts or notebooks. This may in fact be the best solution for large jobs, especially where Aggregator and Comparer steps might need to be re-run without re-running the response modules. It can also be called in one go, feeding all necessary parameters in at once. In this case, we can think of the Controller as simply having larger scope, passing arguments all the way through instead of just to the modules. This can be done in-memory, or saving outputs at each step. In either case, it can be controlled interactively in notebooks, or with parameters which might be passed from a params.yml file, parameters in a notebook, or arguments passed to Rscript at the command line.\nThe workflows section here provides three examples. Similar but not identical examples are available in the template repository, which contains helper scripts for setup on several systems as well as skeleton scripts for various workflows.\n\n\n\n\n\n\nParameters and metadata\n\n\n\nHydroBOT auto-documents itself, saving the settings from runs to [prep_run_save_ewrs()] and [read_and_agg()] into *.yml files. These files also attempt to find the metadata for the scenarios if it exists.\nThese yaml files are fully-specified parameters files for running HydroBOT, along with some additional run information such as the time of the run, and software versions of the EWR tool and HydroBOT. As such, runs can be replicated by re-running HydroBOT with run_hydrobot_params(yamlpath = 'path/to/generated/metadata.yml').\n\n\nIn practice, with large jobs, the typical approach is often to run the response modules (EWR tool) and a default Aggregator as a large parallel job over scenarios, whether that parallelisation happens locally, on an HPC, Azure, or databricks. By saving out the results of each step, additional aggregations can be re-run in parallel with dedicated scripts without having to re-run the EWR tool, as adjustments need to be made to address the question of interest. The comparer is almost always run interactively in a notebook or notebooks for three reasons; first, comparisons across scenarios cannot be parallelised, second, comparisons tend to be fast relative to the processing steps in the controller and aggregator, and third, the comparison step is often quite interactive and iterative as relevant outputs are developed and adjusted to target the questions of interest. If a final, large, set of outputs is needed, such notebooks would still be used to settle on those outputs, and then re-run if needed, e.g. with updated input data.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to supply your own spatial data, causal networks, or aggregation functions, see examples in the aggregator section for detail, and they would typically be done through read_and_agg in the full workflow.",
    "crumbs": [
      "Workflows",
      "Workflow overview"
    ]
  },
  {
    "objectID": "provided_data/data_overview.html",
    "href": "provided_data/data_overview.html",
    "title": "Data",
    "section": "",
    "text": "Some spatial units and networks are needed for spatial and value-dimension aggregation. Users can supply their own, but HydroBOT includes a set of spatial data and causal networks relevant to the Murray-Darling Basin and EWRs.\nHydroBOT also includes example hydrographs suitable for example analyses in both csv and netcdf formats in extdata. To access, use\nsystem.file('extdata/testsmall/hydrographs', package = 'HydroBOT')\nfor csvs or\nsystem.file('extdata/ncdfexample/nchydros', package = 'HydroBOT')\nfor netcdfs.\nFor this website, we use a larger set of example scenarios that are too large to provide with the package but make more interesting demonstrations. These are in the more_scenarios directory in this repository, and created here.",
    "crumbs": [
      "Provided data and Causal networks",
      "Data"
    ]
  },
  {
    "objectID": "controller/controller_overview.html",
    "href": "controller/controller_overview.html",
    "title": "Controller",
    "section": "",
    "text": "HydroBOT takes hydrographs as input data and then processes them through downstream modules, performs aggregation and analyses, and produces outputs. The ‘Controller’ component points to that input data, and sends it off to the modules with arguments controlling those modules. It may also determine how ongoing processing, especially aggregation, occurs.\nIn typical use, the controller simply points to the input data and initiates processing steps according to the user. Examples of this for the controller alone and the whole toolkit are available to illustrate this, as well as a stepthrough to better understand what the controller is doing.\nIn practice, it often makes the most sense to run the controller in a notebook or script, and then have a separate notebook or script for aggregation. This is because running the EWR tool is a long process relative to other steps, and aggregations may need to change as projects progress. Achieving this without re-running the modules is thus a common situation. That said, the workflows section illustrates various workflows that run them together.\nWhether run alone or as a combined workflow, the controller, specifically prep_run_save_ewrs() auto-documents itself by saving metadata yaml files with all arguments used. These parameter files not only document a run, but also allow replication since they are fully-specified parameter files acceptable for [run_hydrobot_params()].\n\n\n\n\n\n\nNote\n\n\n\nScenarios need to have unique names. As such, in most cases they are extracted from the file paths, which must be unique. This can get a bit messy, but is the only consistent way to ensure uniqueness. Analysis stages can incorporate cleanup steps. See additional detail.",
    "crumbs": [
      "Controller"
    ]
  },
  {
    "objectID": "controller/controller_overview.html#overview",
    "href": "controller/controller_overview.html#overview",
    "title": "Controller",
    "section": "",
    "text": "HydroBOT takes hydrographs as input data and then processes them through downstream modules, performs aggregation and analyses, and produces outputs. The ‘Controller’ component points to that input data, and sends it off to the modules with arguments controlling those modules. It may also determine how ongoing processing, especially aggregation, occurs.\nIn typical use, the controller simply points to the input data and initiates processing steps according to the user. Examples of this for the controller alone and the whole toolkit are available to illustrate this, as well as a stepthrough to better understand what the controller is doing.\nIn practice, it often makes the most sense to run the controller in a notebook or script, and then have a separate notebook or script for aggregation. This is because running the EWR tool is a long process relative to other steps, and aggregations may need to change as projects progress. Achieving this without re-running the modules is thus a common situation. That said, the workflows section illustrates various workflows that run them together.\nWhether run alone or as a combined workflow, the controller, specifically prep_run_save_ewrs() auto-documents itself by saving metadata yaml files with all arguments used. These parameter files not only document a run, but also allow replication since they are fully-specified parameter files acceptable for [run_hydrobot_params()].\n\n\n\n\n\n\nNote\n\n\n\nScenarios need to have unique names. As such, in most cases they are extracted from the file paths, which must be unique. This can get a bit messy, but is the only consistent way to ensure uniqueness. Analysis stages can incorporate cleanup steps. See additional detail.",
    "crumbs": [
      "Controller"
    ]
  },
  {
    "objectID": "aggregator/aggregation_overview.html",
    "href": "aggregator/aggregation_overview.html",
    "title": "Aggregation overview",
    "section": "",
    "text": "Tip\n\n\n\nIf you just want to see an example of how the Aggregator typically works in practice, see here, and [here for its use in a full workflow](/workflows/workflow_overview().\nIncoming data from modules is typically very granular in many dimensions (as it should be if the modules are modelling data near the scale of processes). However, this means that there are thousands of different outcomes across the basin and through time. To make that useful for anything other than targeted local planning, we need to scale up in space, time, and along the ‘value’ dimension (causal network). For example, scaling along the value dimension involves modelling how flow requirements influence fish spawning which influence fish populations which contribute to overall environmental success.\nHydroBOT supports aggregation along each of these dimensions with any number of aggregation functions (e.g. mean, min, max, more complex) to reflect the processes being aggregated or the desired assessment. For example, we may want to know the average passing rate of EWRs, whether any EWRs passed, or whether they all passed. Acceptable functions are highly flexible and in general, any summary statistic will work, as will user-developed summary functions.\nThe aggregation steps can be interleaved, e.g. aggregate along the value dimension to some intermediate level, then aggregate in space, then time, then more value levels, then more time and more space. Each step can have more than one aggregation, e.g. we might calculate both the min and max over space.\nTo achieve this, HydroBOT contains a flexible set of aggregation functions that take a list of aggregation steps, each of which can be along any dimension, and a matching list of aggregation functions to apply to that step in the aggregation. It is possible to ask for multiple functions per step.",
    "crumbs": [
      "Aggregator",
      "Aggregation overview"
    ]
  },
  {
    "objectID": "aggregator/aggregation_overview.html#flexibility-and-consistency",
    "href": "aggregator/aggregation_overview.html#flexibility-and-consistency",
    "title": "Aggregation overview",
    "section": "Flexibility and consistency",
    "text": "Flexibility and consistency\nUsers need to be able to conduct summaries in a number of ways, and so the aggregation functions allow a wide range of summary functions to provide this needed flexibility. Within the aggregators, particularly multi_aggregate and read_and_agg, these aggregations are done consistently and produce standard outputs with standard formats. Intermediate data manipulations are handled internally, and so the user can feed raw module output data and an aggregation sequence and receive clean output data, without having to manage many intermediate data groupings and summaries that are liable to introduce errors and differences between runs. By enforcing a consistent format for specifying the aggregations, it is easier to see what is being aggregated, rather than what is being grouped which greatly reduces the complexity of calls and and makes forgetting grouping levels much more difficult.\nWhile HydroBOT provides useful connections to modules, spatial data and causal networks, as well as aggregation fucntions, it also has the flexibility for uers to provide their own causal networks, spatial information, and aggregation functions. Moreover, aggregation can happen on arbitrary input data, possibly from unincorporated modules. See those pages for more detail using read_and_agg(), along with doing the same in multi_aggregate().",
    "crumbs": [
      "Aggregator",
      "Aggregation overview"
    ]
  },
  {
    "objectID": "aggregator/aggregation_overview.html#dimensional-safety",
    "href": "aggregator/aggregation_overview.html#dimensional-safety",
    "title": "Aggregation overview",
    "section": "Dimensional safety",
    "text": "Dimensional safety\nThe incoming data has known grouping structure in the space, time, and value dimensions, and nearly always has a ‘scenario’ column. The outer aggregation functions multi_aggregate and read_and_agg check and manage this structure, ensuring that aggregation over one dimension does not also collapse over others. Because these functions manage the steps through a sequence of aggregations, they ensure the data is set up appropriately for aggregating over a given dimension while holding the others constant, including optimisations for faster nonspatial aggregations of spatial data. These features eliminates many accidental errors in manual setup in both setting up the grouping structure, especially when copy-pasting to make small changes. This also eliminates the large amounts of data manipulation and obsolete objects in the environment and the attention that must be devoted for manual control and makes the code far more readable.\nThis dimensional awareness also enhances speed. Typically, aggregating (and some other operations) on sf dataframes with geometry is much slower than without. So HydroBOT puts a heavy focus on safely stripping and re-adding geometry. This allows us to use dataframes that reference geometry without the geometry itself attached and only take the performance hit from geometry if it is needed. We’re doing the absolute minimum spatially-aware processing, and doing that in a way that early spatial processing does not slow down later non-spatial processing.",
    "crumbs": [
      "Aggregator",
      "Aggregation overview"
    ]
  },
  {
    "objectID": "aggregator/aggregation_overview.html#automation",
    "href": "aggregator/aggregation_overview.html#automation",
    "title": "Aggregation overview",
    "section": "Automation",
    "text": "Automation\nA manual approach to multi-step aggregation (and applying that to many scenarios) can be incredibly cumbersome, leading to complex scripts to manage. This makes manual automation incredibly difficult, and a full processing script would need to be looped over to handle a set of scenarios to run the same analysis on each scenario. The situation gets worse if some parameters need to change between runs. For example, what if step three needs to change the aggregation function or move from a value to a spatial aggregation? This will have cascading effects through the rest of the script, making errors more likely and automation more difficult.\nThe HydroBOT aggregator manages all of this internally, and so a change to the aggregation sequences as described would involve changing a single item in two lists (the aggsequence and funsequence). The read_and_agg function is explicitly designed to apply the same aggregations to a set of scenarios, and can do so in parallel for speed without any need for the user to manage loops or parallelisation beyond installing the furrr package. Moreover, these lists can be specified in very simple scripts that are run remotely, and can also be specified in yml parameter files and changed programatically. Thus, automating the same set of aggregations over e.g. scenarios is straightforward, and automating different aggregations is a matter of a simple change of a small number of arguments rather than re-developing or tweaking and testing a complex script with many sources of error.",
    "crumbs": [
      "Aggregator",
      "Aggregation overview"
    ]
  },
  {
    "objectID": "aggregator/aggregation_overview.html#tracking",
    "href": "aggregator/aggregation_overview.html#tracking",
    "title": "Aggregation overview",
    "section": "Tracking",
    "text": "Tracking\nIn general, aggregation over many steps can get quite complicated to track, particularly if some of the steps have multiple aggregation functions, and is nearly impossible if using a manual approach of a sequence of dplyr::group_by() and dplyr::summarise(). Tracking the provenance of the final values is therefore critical to understand their meaning. By default, HydroBOT aggregation outputs have column names that track their provenance, e.g. step2_function2_step1_function1_originalName. This is memory-friendly but ugly, and so we can also stack this information into columns (two for each step- one the step, the second the function) by setting the argument namehistory = FALSE (which calls the agg_names_to_cols() function; this can be called on a dataframe with tracking names post-hoc as well).\nIf using the read_and_agg() function, it saves out a yml metadata file that has all needed arguments to replicate the aggregation run and saves it with the outputs. This both documents the provenance of the outputs, and can allow re-running the same aggregation by passing that parameter file to run_hydrobot_params() (see the parameterised workflow).\nIn the case of a multi-step aggregation, we can either save only the final output (better for memory) or save the entire stepwise procedure, which can be very useful both for investigating results and visualisation, and it is often the case that we want to ask questions of several levels anyway.",
    "crumbs": [
      "Aggregator",
      "Aggregation overview"
    ]
  },
  {
    "objectID": "aggregator/aggregation_overview.html#module-idiosyncracies",
    "href": "aggregator/aggregation_overview.html#module-idiosyncracies",
    "title": "Aggregation overview",
    "section": "Module idiosyncracies",
    "text": "Module idiosyncracies\nModules (currently the EWR tool) will produce data with specific idiosyncracies, such as particular ways it should not be aggregated. For example, the EWR data is linked to gauges, but some gauges provide information for spatial units that do not contain them (planning units or sustainable diversion limit units). The HydroBOT aggregator provides internal checks that infers whether EWR data is being aggregated in an inappropriate way, and throws warnings or errors. It also contains functionality to address these issues in a general way (in this example, retaining grouping with group_until and using non-spatial joins of spatial data with pseudo_spatial), as well as some functionality (e.g. auto_ewr_PU = TRUE) that can automatically address these known issues.",
    "crumbs": [
      "Aggregator",
      "Aggregation overview"
    ]
  },
  {
    "objectID": "comparer/comparer_syntax.html",
    "href": "comparer/comparer_syntax.html",
    "title": "Comparer syntax and reasoning",
    "section": "",
    "text": "Most of the arguments to plot_outcomes() are relatively straightforward, and control the basic look of the plot. A few, however, are unusual and permit important but quite complex behaviour. They are briefly outlined here, with links to where their use is demonstrated more fulsomely.",
    "crumbs": [
      "Comparer",
      "Syntax and details",
      "Comparer syntax"
    ]
  },
  {
    "objectID": "comparer/comparer_syntax.html#outcome_col",
    "href": "comparer/comparer_syntax.html#outcome_col",
    "title": "Comparer syntax and reasoning",
    "section": "outcome_col",
    "text": "outcome_col\nA key difference between plot_outcomes() and more typical plotting functions, e.g. ggplot() is the use of the outcome_col argument. This treats the outcome of interest as a special value, which may need data processing to occur (baselining, scaling, etc), as it is fundamentally the value of interest, wherever it gets plotted. It allows that processing to occur consistently to that column whether it is plotted on the y-axis (e.g. bars or points and lines) or as color in maps or heatmaps. The placement of this outcome_col is then controlled by plot_type, which is \"2d\" by default and puts it on x, but also might be heatmap or map, which makes it a color.",
    "crumbs": [
      "Comparer",
      "Syntax and details",
      "Comparer syntax"
    ]
  },
  {
    "objectID": "comparer/comparer_syntax.html#consistent-limits",
    "href": "comparer/comparer_syntax.html#consistent-limits",
    "title": "Comparer syntax and reasoning",
    "section": "Consistent limits",
    "text": "Consistent limits\nWe often need to control limits for visual consistency across plots or to ensure things like midpoints of color ramps are appropriate. The setLimits argument allows us to do all of those for the values in outcome_col. It behaves differently depending on length, making it worth for setting ranges but also e.g. midpoints of diverging axes. In addition, it is used internally when data is baselined to set appropriate breaks (especially for diverging color ramps, see maps example).",
    "crumbs": [
      "Comparer",
      "Syntax and details",
      "Comparer syntax"
    ]
  },
  {
    "objectID": "comparer/comparer_syntax.html#list-arguments-for-complex-plots",
    "href": "comparer/comparer_syntax.html#list-arguments-for-complex-plots",
    "title": "Comparer syntax and reasoning",
    "section": "List-arguments for complex plots",
    "text": "List-arguments for complex plots\nMaps in particular can have under- and overlays, and so we use the underlay_list and overlay_list lists to specify how this works, with much more detail and examples in the maps demonstration.\nSmoothers and contours are both fitting functions, which may need additional arguments. These can be provided through the smooth_arglist and contour_arglist respectively, with demonstration in line plots and heatmaps.\nThe base_list argument takes a list of arguments that get passed to baseline_compare(), allowing baselining to happen internally to plot_outcomes() and so avoiding carrying around extra dataframes of compared data. See the hydrographs for examples.",
    "crumbs": [
      "Comparer",
      "Syntax and details",
      "Comparer syntax"
    ]
  },
  {
    "objectID": "comparer/comparer_syntax.html#grouped-colors",
    "href": "comparer/comparer_syntax.html#grouped-colors",
    "title": "Comparer syntax and reasoning",
    "section": "Grouped colors",
    "text": "Grouped colors\nSometimes we might want to specify different palettes for different groups, so that we can distinguish both large groupings and the subsidiary parts. we use the pal_list as a list for this, in conjunction with colorgroups. See the bar example and causal networks.",
    "crumbs": [
      "Comparer",
      "Syntax and details",
      "Comparer syntax"
    ]
  },
  {
    "objectID": "provided_data/causal_manipulation.html",
    "href": "provided_data/causal_manipulation.html",
    "title": "Causal network functions",
    "section": "",
    "text": "Purpose\nThis document will cover the functions provided by {HydroBOT} to interact with and manipulate causal networks.\nFor example\n\nmake_nodes\nmake_edges\nThe pruning function find_related_nodes\nothers",
    "crumbs": [
      "Provided data and Causal networks",
      "Causal networks",
      "Building and manipulating causal networks"
    ]
  },
  {
    "objectID": "workflows/scenarios_and_directories.html",
    "href": "workflows/scenarios_and_directories.html",
    "title": "Scenarios and directory structure",
    "section": "",
    "text": "HydroBOT is designed to run both modules (at present, EWRs) and aggregation within each scenario, followed by comparison between scenarios in the comparer step. This means it is straightforward to automate (and parallelise) runs over many scenarios.\nWhile many different directory structures are possible, life is easiest if the directory structure represents scenarios, with single hydrograph files for each scenario. This structure can be nested, as the path then provides the unique scenario naming. For a simple example of a factorial combination of three climate scenarios and two management scenarios, we might have this structure:\nhydrographs/\n├─ historical flow/\n│  ├─ management/\n│  │  ├─ gauges.csv\n│  ├─ no management/\n│  │  ├─ gauges.csv\n├─ increased flow/\n│  ├─ management/\n│  │  ├─ gauges.csv\n│  ├─ no management/\n│  │  ├─ gauges.csv\n├─ decreased flow/\n│  ├─ management/\n│  │  ├─ gauges.csv\n│  ├─ no management/\n│  │  ├─ gauges.csv\nIn such a structure, HydroBOT can treat each gauges.csv file as the realisation for that unique scenario, and so run them in parallel (with argument rparallel = TRUE). It will reproduce this structure with the module outputs.\nOther directory structures are possible, though they often make parallelisation and automation more difficult. In general, the key to success in those cases is to set the scenarios_from argument in [prep_run_save_ewrs()].",
    "crumbs": [
      "Workflows",
      "Scenarios and directory structure"
    ]
  },
  {
    "objectID": "workflows/scenarios_and_directories.html#directory-structure",
    "href": "workflows/scenarios_and_directories.html#directory-structure",
    "title": "Scenarios and directory structure",
    "section": "",
    "text": "HydroBOT is designed to run both modules (at present, EWRs) and aggregation within each scenario, followed by comparison between scenarios in the comparer step. This means it is straightforward to automate (and parallelise) runs over many scenarios.\nWhile many different directory structures are possible, life is easiest if the directory structure represents scenarios, with single hydrograph files for each scenario. This structure can be nested, as the path then provides the unique scenario naming. For a simple example of a factorial combination of three climate scenarios and two management scenarios, we might have this structure:\nhydrographs/\n├─ historical flow/\n│  ├─ management/\n│  │  ├─ gauges.csv\n│  ├─ no management/\n│  │  ├─ gauges.csv\n├─ increased flow/\n│  ├─ management/\n│  │  ├─ gauges.csv\n│  ├─ no management/\n│  │  ├─ gauges.csv\n├─ decreased flow/\n│  ├─ management/\n│  │  ├─ gauges.csv\n│  ├─ no management/\n│  │  ├─ gauges.csv\nIn such a structure, HydroBOT can treat each gauges.csv file as the realisation for that unique scenario, and so run them in parallel (with argument rparallel = TRUE). It will reproduce this structure with the module outputs.\nOther directory structures are possible, though they often make parallelisation and automation more difficult. In general, the key to success in those cases is to set the scenarios_from argument in [prep_run_save_ewrs()].",
    "crumbs": [
      "Workflows",
      "Scenarios and directory structure"
    ]
  },
  {
    "objectID": "workflows/scenarios_and_directories.html#files",
    "href": "workflows/scenarios_and_directories.html#files",
    "title": "Scenarios and directory structure",
    "section": "Files",
    "text": "Files\nThe scenarios should to be in separate directories inside /hydrographs, but the files in those directories could come in multiple arrangements. Currently, we allow multiple csvs of single-gauge hydrographs or single csvs of multiple-gauge hydrographs. It is generally better to include all gauges in a single file, as any multigage EWRs cannot be assessed if they are separate. The catch, particularly with historical data, is that the timespans will differ, yielding periods of NA for some columns, which the EWR tool treats as 0. If this is the case, some post-processing will be needed to ignore zeros in the EWR output prior to the first real data for a gauge.\n\nFile types\nHydroBOT currently handles csvs, netcdfs, and zipped netcdfs. Netcdfs should be in the WERP format parseable by the EWR tool as IQQM - netcdf, while csvs can be in any other option parsed by the EWR tool, see ?prep_run_save_ewrs for options.\n\n‘Standard time-series’: (default, among other things accepts a csv with a Date column followed by gauge columns, with _flow or _level appended to the gauge number)\n‘IQQM - netcdf’: finds all netcdf files in hydro_dir. Should also work when hydro_dir is a .zip with netcdfs inside\n‘ten thousand year’: old default (IQQM - NSW 10,000 years), works nearly the same as standard time-series\n‘All Bigmod’: previously ‘Bigmod - MDBA’\n‘Source - NSW (res.csv)’",
    "crumbs": [
      "Workflows",
      "Scenarios and directory structure"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HydroBOT demonstration",
    "section": "",
    "text": "This website and associated github repository provides examples of using HydroBOT in various ways, as well as its capabilities and pitfalls. To install and use, see get started.\nHydroBOT has several goals, and provides workflow and analysis capabilities to achieve them. It treats scenario comparison as a first-class objective, and so is structured to automate over many scenarios, performing the same processing to each, followed by comparison across scenarios (though it can run single scenarios if desired). It produces metadata files to document settings at each step, which double as runnable parameter files for reproducibility.\nHydroBOT is built to provide general capacity for synthesis, to then be tailored for specific analysis: what questions are being asked of the scenarios, what synthesis do they need, and how should they be compared. It is designed to be modular, incorporating many different response models, though at this point it contains only the EWR tool. It provides important guardrails for both idiosyncrasies of the EWR outputs and multidimensional scaling.\nUsers should consider the various capabilities demonstrated here, and then develop a streamlined flow with just those parameters relevant to the particular analysis.",
    "crumbs": [
      "HydroBOT demonstration"
    ]
  },
  {
    "objectID": "index.html#workflow",
    "href": "index.html#workflow",
    "title": "HydroBOT demonstration",
    "section": "Workflow",
    "text": "Workflow\nThe basic workflow of HydroBOT involves identifying and selecting data inputs representing scenarios, running response models, aggregation, scaling, and onward modelling of those outputs, and finally synthesis and production of output figures, tables, and other products. In concept, the HydroBOT workflow is split into three components: the controller which points to data and runs response models, the aggregator which scales the outputs of those models in space, time, and along causal networks, and the comparer which synthesizes and produces interpretable outputs. There are many ways to structure this workflow in practice, with examples here.\n\n\n\n\n\nFigure 1: Conceptualisation of HydroBOT architecture (blue arrows) and links to management decision making (yellow arrows). Scenarios, represented by hydrographs, reflect observed flows or modeled scenarios (produced via hydrological modelling outside HydroBOT). These hydrographs are fed in a consistent way to various response models via the HydroBOT Controller. These response models could assess response of any values of interest, here illustrated as a range of value types across environmental and human dimensions. Additional inputs of spatial data and causal networks provide grouping information for scaling via the HydroBOT Aggregator. The HydroBOT Comparer synthesizes the results into management-relevant outputs that aid decision making. This decision-making process is also supported by feedback from stakeholders, other expert advice and the social and political landscape. Water planning and policy decisions can then guide the development of scenarios to assess potential management options, which in turn can be assessed with HydroBOT.",
    "crumbs": [
      "HydroBOT demonstration"
    ]
  },
  {
    "objectID": "index.html#components",
    "href": "index.html#components",
    "title": "HydroBOT demonstration",
    "section": "Components",
    "text": "Components\nEach of the components are discussed in more detail, with this detail used to modify the sorts of workflows in the workflow examples to tailor them for a particular analysis.\n\n\nController\n\nin detail\nsimplified\nPython notebooks exist, but are not maintained. Contact authors if needed.\n\n\n\nAggregator\n\nTheme and Space\nTheme alone with more options and plot examples\nSpace alone with more options- includes multilevel spatial agg into different polygons and complex aggregation functions\n\n\n\nComparer\n\nhydrographs\nbar, line, maps, and causal networks all provide plotting of outcomes from HydroBOT\n\n\n\nDependencies\nHydroBOT relies on external information in three main ways. First, it uses externally-defined response models, currently the MDBA EWR tool. It also requires causal networks to link those outputs to larger-scale values. Finally, it requires the input scenarios, without which it would have nothing to model from.\n\n\nCausal networks\n\nCausal networks are needed for aggregation and comparison, but are not part of the flow per se.\n\n\n\nScenario creation\n\nScenario creation is not part of HydroBOT, but scenarios are needed to run HydroBOT\n\n\n\nIn use, HydroBOT expects that scenario hydrographs are available and the causal networks are defined.",
    "crumbs": [
      "HydroBOT demonstration"
    ]
  },
  {
    "objectID": "index.html#development",
    "href": "index.html#development",
    "title": "HydroBOT demonstration",
    "section": "Development",
    "text": "Development\nSee the repo readme for additional dev info and more complex package installation issues. See the {HydroBOT} repo repo for development of the package itself.",
    "crumbs": [
      "HydroBOT demonstration"
    ]
  },
  {
    "objectID": "comparer/comparer_overview.html",
    "href": "comparer/comparer_overview.html",
    "title": "Comparer overview",
    "section": "",
    "text": "The Comparer has two components- underlying functions and structure to perform comparisons and other analyses, and plotting capabilities to produce some standardized plots that capture important data visualisation.\nMost importantly for the plots implement through the plot_outcomes() function, the design philosophy is that an outcome to be plotted should be declared, and then however that outcome is presented (maps, bars, lines, etc), the necessary data transformations happen inside plot_outcomes(), and so greatly enhancing testing and rigour while reducing the errors associated with copy-pasting or inadvertently mis-specifying steps. Moreover, plot_outcomes() is dimensionally aware and throws warnings when data is being silently overplotted and so producing misleading results (and alerting the user to forgotten dimensions).\nThere is quite a lot of flexibility built into all of the comparer, because different uses and different questions will require different outputs, whether that means different scales of analysis, different types of plots, or different numerical comparisons. Not only will these differ within projects, the act of finding an ‘ideal’ set of plots for any given project is necessarily iterative, and so the flexibility here provides the user with much control over that process.\nWhile this is called the ‘Comparer’ and most plots use the function plot_outcomes(), it also contains other functionality related to analysis generally, and can produce plots that do not include comparisons or outcomes, e.g. hydrographs to simply illustrate historical flows.\nNearly all plots of outcomes are made with plot_outcomes, including bars, lines, heatmaps, and maps. This is because at their foundation, they area all plotting a quantitative outcome with grouping of some sort. The data preparation is the same across all of them, as well as many of the arguments to ggplot().\nNearly all plots (with the current exception of the causal networks) are made internally with ggplot2 and return ggplot2 objects, which can easily be further modified. The plot functions here wrap the ggplot2 to standardise appearance and data preparation and ensure dimensions are handled appropriately. Though it can be annoying to not use ggplot() directly to make the plots, one MAJOR advantage of the plotting function here is that any data changes that clean it for a given plot aren’t preserved, and so it’s far easier to keep the data clean, know what the data is, and avoid accidental overwriting or mislabeling of data. Further, the internal data manipulation remains the same whether the outcome is plotted as a y-axis, color, etc or the plot type changes.",
    "crumbs": [
      "Comparer",
      "Comparer overview"
    ]
  },
  {
    "objectID": "comparer/comparer_overview.html#theme",
    "href": "comparer/comparer_overview.html#theme",
    "title": "Comparer overview",
    "section": "Theme",
    "text": "Theme\nHydroBOT provides the theme_hydrobot() ggplot theme that we use to get a consistent look, but other themes can always be used post-hoc. Additional theme arguments can be passed to it, if we want to change any of the other arguments in ggplot2::theme() on a per-plot basis. By default, theme_hydrobot is applied when making the plots inside plot_outcomes(), though it can be applied to any ggplot object.",
    "crumbs": [
      "Comparer",
      "Comparer overview"
    ]
  },
  {
    "objectID": "comparer/comparer_overview.html#colour",
    "href": "comparer/comparer_overview.html#colour",
    "title": "Comparer overview",
    "section": "Colour",
    "text": "Colour\nHydroBOT does not enforce a standard set of colors, instead, it provides the user the tools they need to achieve the color standardisations they need for a particular project or plot type. These color sets will change between scenarios/projects and there are too many possibilities of what we might plot. It is generally good practice to enforce palettes within projects, and HydroBOT provides the tools to do this. In general, colors can either be specified manually (usually with the help of make_pal() to generate named color objects) or with {paletteer} palettes because of the wide range of options with standard interface and ability to choose based on names. A good reference for the available palettes is here, and demonstrations of color specification are throughout the examples, but specifically bar plots.\nIn some cases, we can set multiple levels of colors based on different palettes, which can be a useful way to indicate grouping variables. This is available everywhere, but is best demonstrated in the bar plots and causal plots. Though it achieves a different purpose, there is also the ability to set separate color palettes for different spatial scales in the same map.",
    "crumbs": [
      "Comparer",
      "Comparer overview"
    ]
  },
  {
    "objectID": "comparer/comparer_overview.html#internal-calculations-and-structure",
    "href": "comparer/comparer_overview.html#internal-calculations-and-structure",
    "title": "Comparer overview",
    "section": "Internal calculations and structure",
    "text": "Internal calculations and structure\nWhile plots are the typical outputs of the Comparer, it has a set of useful functions for preparing data, including calculating values relative to a baseline (baseline_compare()) using either default functions difference and relative, or with any user-supplied function.\nThere is an internal function plot_prep() that does all the data prep, including applying baseline_compare(), finding colors, and setting established scenario orders. This keeps plots and the data processing consistent, and dramatically reduces the error-prone copy-pasting of data processing with minor changes for different plots. Instead, we can almost always feed the plotting functions the same set of clean data straight out of the aggregator, and just change the arguments to the plot functions.\nBaselining is available as a standalone function (baseline_compare()) and can be done automatically in the plot_outcomes() (and plot_prep()) functions. This capacity is demonstrated in all the plot examples, but in most detail in the hydrographs.\nOne critical issue, particularly with complex data, is being unaware of silently overplotted values. The plot_outcomes() function has internal checks that the number of rows of data matches the number of axes on which the data is plotted (including facets, colors, linetype, etc). This prevents things like plotting a map of env_obj data facetted only by scenario, and so each fill represents outcomes for all env_obj, which is meaningless but very easy to do. The exception is that points are allowed to overplot, though we can use the position = 'position_jitter' argument to avoid that, as is typical with ggplot().",
    "crumbs": [
      "Comparer",
      "Comparer overview"
    ]
  },
  {
    "objectID": "comparer/comparer_overview.html#scenario-information",
    "href": "comparer/comparer_overview.html#scenario-information",
    "title": "Comparer overview",
    "section": "Scenario information",
    "text": "Scenario information\nThe ‘scenarios’ used here for examples are a factorial combination of multiplicative and additive changes to flow, based on historical hydrographs. We use a more complex set for the Comparer examples than for the Controller and Aggregator in order to have something more interesting to plot.\nIn an ideal world, scenario metadata would be auto-acquired from the directory defining the hydrographs. In practice, that’s rarely available, but we can do it here for the example scenarios.\n\nproject_dir &lt;- file.path(\"more_scenarios\")\nhydro_dir &lt;- file.path(project_dir, \"hydrographs\")\n\nscenarios &lt;- yaml::read_yaml(file.path(hydro_dir, 'scenario_metadata.yml')) |&gt; \n  tibble::as_tibble()\n\nTo scale flow, we apply nine flow multipliers, ranging from 0.5 to 2.0, to the historical hydrographs (SI Table 1). We refer to these as ‘climate’ scenarios, reflecting a common representation where entire hydrographs might shift to represent climate change. To achieve pulsed change for each of the ‘climate’ scenarios, four flow additions were applied including 1) no addition (baseline), 2) addition of 250 ML/d, 3) addition of 6500 ML/d, and 4) addition of 12000 ML/d (Table 1). These additional flows were added throughout the period of September to December. We refer to these scenarios as ‘climate adaptations’ because management options are often available in the form of altering water availability for short time periods through mechanisms like water releases, though the options here do not represent proposed actions. These scenarios should not be interpreted as potential climate impacts or adaptations, but instead as different ways flows might change (multiplicative or additive) and different magnitudes of change.\n\nadapt_scenes &lt;- scenarios |&gt; \n  dplyr::filter(scenario != 'MAX') |&gt; \n  dplyr::mutate(flow_addition = as.integer(flow_addition)) |&gt; \n dplyr::select(`Adaptation code` = adapt_code,\n         `Flow addition (ML/d)` = flow_addition) |&gt;\n  dplyr::distinct()\n\nclimate_scenes &lt;- scenarios |&gt; \n  dplyr::filter(scenario != 'MAX') |&gt; \n  dplyr::select(`Climate code` = climate_code,\n         `Flow multiplier` = flow_multiplier) |&gt;\n  dplyr::distinct()\n\nadapt_scenes &lt;- adapt_scenes |&gt; \n  dplyr::bind_rows(tibble::tibble(`Adaptation code` = rep(NA, nrow(climate_scenes) - \n                                             nrow(adapt_scenes)),\n       `Flow addition (ML/d)` = rep(NA, nrow(climate_scenes) - \n                                      nrow(adapt_scenes))))\n\nclimate_scenes |&gt; \n               dplyr::mutate(`Flow multiplier` = signif(`Flow multiplier`, 2)) |&gt; \n  dplyr::bind_cols(adapt_scenes) |&gt; \n  flextable::flextable()  |&gt; \n  flextable::font(fontname = 'Calibri') |&gt; \n  flextable::fontsize(size = 10, part = 'all') |&gt; \n  flextable::set_table_properties(layout = \"autofit\", width = 1) |&gt; \n  flextable::vline(j = 2)\n\n\nTable 1: Demonstration scenarios are a factorial combination of ‘climate’ (scaled flow) and ‘adaptation’ (pulsed additions).\n\n\n\n\n\n\nClimate code\nFlow multiplier\nAdaptation code\nFlow addition (ML/d)\n\n\n\nA\n0.50\n1\n0\n\n\nB\n0.67\n2\n250\n\n\nC\n0.80\n3\n6,500\n\n\nD\n0.91\n4\n12,000\n\n\nE\n1.00\n\n\n\n\nF\n1.10\n\n\n\n\nG\n1.20\n\n\n\n\nH\n1.50\n\n\n\n\nI\n2.00",
    "crumbs": [
      "Comparer",
      "Comparer overview"
    ]
  },
  {
    "objectID": "aggregator/pseudo_spatial_group_until.html",
    "href": "aggregator/pseudo_spatial_group_until.html",
    "title": "Pseudo-spatial and group_until",
    "section": "",
    "text": "library(HydroBOT)\n\nLoading required package: sf\n\n\nLinking to GEOS 3.13.0, GDAL 3.10.1, PROJ 9.5.1; sf_use_s2() is TRUE\nThe nature of multi-step and multidimensional aggregation sometimes requires grouping only for part of the sequence (e.g. grouping by a column only relevant at some scales) or bypassing the automatic treatment of geographic data to do non-spatial joins or groupings of spatial data. We have introduced two arguments to multi_aggregate() and read_and_agg() to handle these issues, namely group_until and pseudo_spatial. The EWR-specific argument auto_ewr_PU manages these arguments according to best practices for the EWR tool outputs.",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Unusual grouping requirements"
    ]
  },
  {
    "objectID": "aggregator/pseudo_spatial_group_until.html#group_until",
    "href": "aggregator/pseudo_spatial_group_until.html#group_until",
    "title": "Pseudo-spatial and group_until",
    "section": "group_until",
    "text": "group_until\nBy default, anything passed to the groupers argument is retained throughout the sequence, with the most common value being 'scenario'. The group_until argument allows a list of groupers to be passed along with the stage to which they should be retained. The names of this list are column names in the data, and the values at each name specify the aggregation step to retain until. The step can be\n\nnumeric index, e.g. group_until = list(gauge = 2) retains grouping by gauge in steps 1 and 2, but drops it in 3.\ncharacter name of a step, e.g. if the aggsequence argument is aggseq &lt;- list( all_time = \"all_time\", ewr_code = c(\"ewr_code_timing\", \"ewr_code\"), sdl_units = sdl_units), then group_until = list(gauge = 'ewr_code' would retain gauge groupings until the ewr_code step, but drop them when aggregating to sdl_units.\nA function that evaluates to TRUE or FALSE on the aggregation sequence. A common use here is group_until = list(gauge = is_notpoint), which groups by the ‘gauge’ column until the data is no longer geographic points, but polygons.\n\nThe list passed to group_until can have multiple entries, e.g. group_until = list(gauge = is_notpoint, planning_unit_name = is_notpoint) would retain both the ‘gauge’ and ‘planning_unit_name’ columns until the data is aggregated into polygons. The stages can differ for the items, though they do not in this example.",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Unusual grouping requirements"
    ]
  },
  {
    "objectID": "aggregator/pseudo_spatial_group_until.html#pseudo_spatial",
    "href": "aggregator/pseudo_spatial_group_until.html#pseudo_spatial",
    "title": "Pseudo-spatial and group_until",
    "section": "pseudo_spatial",
    "text": "pseudo_spatial\nBy default, the joins and groupings that happen in the aggregation sequence treat geographic data as geographic; they group by the spatial units. However, in some cases, we might want to do nonspatial groupings or joins of the data. The most common example arises because gauges can provide information to planning units or sdl units they are not located within, and so we need to link EWR outputs or other data at those gauges to those distant units (and sometimes multiple units). The pseudo_spatial argument allows these steps to happen non-spatially, and retains the spatial information of the level being joined/aggregated into.\nThis argument should be set to the name or index of the aggregation step that should be performed nonspatially. A common example is pseudo_spatial = 'sdl_units' , though the numeric index for the sdl_units step would work as well.",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Unusual grouping requirements"
    ]
  },
  {
    "objectID": "aggregator/pseudo_spatial_group_until.html#an-example",
    "href": "aggregator/pseudo_spatial_group_until.html#an-example",
    "title": "Pseudo-spatial and group_until",
    "section": "An example",
    "text": "An example\nIn practice, use of group_until and pseudo_spatial often works like this:\n\naggseq &lt;- list(\n  all_time = \"all_time\",\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  sdl_units = sdl_units,\n  env_obj = c(\"ewr_code\", \"env_obj\"),\n  mdb = basin\n)\n\nfunseq &lt;- list(\n  \"ArithmeticMean\",\n  \"CompensatingFactor\",\n  \"ArithmeticMean\",\n  \"ArithmeticMean\",\n  \"SpatialWeightedMean\"\n)\n\nGrouping by SWSDLName, gauge, and planning_unit name columns is retained until the sdl_unit aggregation, specified in three different ways. Likewise, the aggregation into SDL units is done non-spatially.\n\naggout &lt;- read_and_agg(\n  datpath = 'hydrobot_scenarios/module_output/EWR',\n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  group_until = list(\n    SWSDLName = 3,\n    planning_unit_name = 'sdl_units',\n    gauge = is_notpoint\n  ),\n  pseudo_spatial = \"sdl_units\",\n  aggsequence = aggseq,\n  funsequence = funseq,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = FALSE,\n  returnList = TRUE,\n  add_max = FALSE\n)\n\n! Unmatched links in causal network\n• 29 from ewr_code_timing to ewr_code",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Unusual grouping requirements"
    ]
  },
  {
    "objectID": "aggregator/pseudo_spatial_group_until.html#auto_ewr_pu",
    "href": "aggregator/pseudo_spatial_group_until.html#auto_ewr_pu",
    "title": "Pseudo-spatial and group_until",
    "section": "auto_ewr_PU",
    "text": "auto_ewr_PU\nThe auto_ewr_PU = TRUE argument in [read_and_agg()] and [multi_aggregate()] is a shortcut to do known best-practices for EWR outputs. It sets any aggregations to planning units or sdl units to pseudo-spatial, since gauges informing those units may not be geographically located within them. It also uses group_until to hold their groupings and not accidentially collapse over them in preceding aggregation steps.\nBest practice is to be explicit (as above), with the following arguments to [read_and_agg()] and [multi_aggregate()], but it is often faster to use auto_ewr_PU = TRUE.\ngroup_until = list(SWSDLName = is_notpoint, \n                   planning_unit_name = is_notpoint, \n                   gauge = is_notpoint),\npseudo_spatial = 'sdl_units'\nUsing auto_ewr_PU to achieve the same results as above is done with this call, but note the messages about being explicit.\n\naggout_auto &lt;- read_and_agg(\n  datpath = 'hydrobot_scenarios/module_output/EWR',\n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  auto_ewr_PU = TRUE,\n  aggsequence = aggseq,\n  funsequence = funseq,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = FALSE,\n  returnList = TRUE,\n  add_max = FALSE\n)\n\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`.\n\n\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`\n.\n! Unmatched links in causal network\n• 29 from ewr_code_timing to ewr_code\nℹ EWR gauges joined to larger units pseudo-spatially.\n• Done automatically because `auto_ewr_PU = TRUE`\n• Non-spatial join needed because gauges may inform areas they are not within\n• Best to explicitly use `pseudo_spatial = 'sdl_units'` in `multi_aggregate()` or `read_and_agg()`.\n\n\nAnd we see that those are the same\n\nall(aggout$sdl_units$ewr_achieved == aggout_auto$sdl_units$ewr_achieved)\n\n[1] TRUE",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "Unusual grouping requirements"
    ]
  },
  {
    "objectID": "workflows/run_hydrobot_website.html",
    "href": "workflows/run_hydrobot_website.html",
    "title": "Build demo data",
    "section": "",
    "text": "This notebook regenerates outputs for the demonstrations. We generally do not want to run it every time.\nlibrary(HydroBOT)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nThis is a big run, best to do it in parallel.\nfuture::plan(future::multisession)\n# Outer directory for scenario\nproject_dir &lt;- file.path(\"more_scenarios\")\n\n# Preexisting data\n# Hydrographs (expected to exist already)\nhydro_dir &lt;- file.path(project_dir, \"hydrographs\")\n\n# Generated data\n# EWR outputs (will be created here in controller, read from here in aggregator)\newr_results &lt;- file.path(project_dir, \"module_output\", \"EWR\")\n\n# outputs of aggregator. There may be multiple modules\nagg_results &lt;- file.path(project_dir, \"aggregator_output\")",
    "crumbs": [
      "Workflows",
      "Examples",
      "To create this website"
    ]
  },
  {
    "objectID": "workflows/run_hydrobot_website.html#run-the-ewr-tool",
    "href": "workflows/run_hydrobot_website.html#run-the-ewr-tool",
    "title": "Build demo data",
    "section": "Run the EWR tool",
    "text": "Run the EWR tool\nUsing rparallel = TRUE for speed to parallelise over all scenarios.\n\nif (params$REBUILD_EWR) {\n  ewr_out &lt;- prep_run_save_ewrs(\n    hydro_dir = hydro_dir,\n    output_parent_dir = project_dir,\n    outputType = list(\"yearly\"),\n    rparallel = TRUE\n  )\n}",
    "crumbs": [
      "Workflows",
      "Examples",
      "To create this website"
    ]
  },
  {
    "objectID": "workflows/run_hydrobot_website.html#aggregate",
    "href": "workflows/run_hydrobot_website.html#aggregate",
    "title": "Build demo data",
    "section": "Aggregate",
    "text": "Aggregate\nUse a ‘standard’ aggregation sequence, as in getting started.\nSome of the example gauges provide information into other sdl units, but since we do not have full gauge coverage in those units, we clip to only those that are relevant.\n\nsdl_clip &lt;- sdl_units |&gt; \n  filter(SWSDLName %in% c(\"Lachlan\", \"Namoi\", \"Macquarie-Castlereagh\"))\n\ncausal_clip &lt;- causal_ewr\n\ncausal_clip[1:2] &lt;- causal_clip[1:2] |&gt; \n  purrr::map(\\(x) filter(x, SWSDLName %in% c(\"Lachlan\", \"Namoi\", \"Macquarie-Castlereagh\")))\n\n\naggseq &lt;- list(\n  all_time = \"all_time\",\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  env_obj = c(\"ewr_code\", \"env_obj\"),\n  sdl_units = sdl_clip,\n  Target = c(\"env_obj\", \"Target\"),\n  mdb = basin,\n  target_5_year_2024 = c(\"Target\", \"target_5_year_2024\")\n)\n\nfunseq &lt;- list(\n  all_time = \"ArithmeticMean\",\n  ewr_code = \"CompensatingFactor\",\n  env_obj = \"ArithmeticMean\",\n  sdl_units = \"ArithmeticMean\",\n  Target = \"ArithmeticMean\",\n  mdb = \"SpatialWeightedMean\",\n  target_5_year_2024 = \"ArithmeticMean\"\n)\n\nBe explicit instead of using auto_ewr_PU\n\nif (params$REBUILD_AGG) {\n  aggout &lt;- read_and_agg(\n    datpath = ewr_results,\n    type = \"achievement\",\n    geopath = bom_basin_gauges,\n    causalpath = causal_clip,\n    groupers = \"scenario\",\n    aggCols = \"ewr_achieved\",\n    group_until = list(\n      SWSDLName = is_notpoint,\n      planning_unit_name = is_notpoint,\n      gauge = is_notpoint\n    ),\n    pseudo_spatial = \"sdl_units\",\n    aggsequence = aggseq,\n    funsequence = funseq,\n    saveintermediate = TRUE,\n    namehistory = FALSE,\n    keepAllPolys = FALSE,\n    returnList = TRUE,\n    savepath = agg_results,\n    rparallel = TRUE,\n    add_max = FALSE\n  )\n}",
    "crumbs": [
      "Workflows",
      "Examples",
      "To create this website"
    ]
  },
  {
    "objectID": "comparer/heatmap.html",
    "href": "comparer/heatmap.html",
    "title": "Heatmaps and other surfaces",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Surfaces (heatmaps and contours)"
    ]
  },
  {
    "objectID": "comparer/heatmap.html#read-in-the-data",
    "href": "comparer/heatmap.html#read-in-the-data",
    "title": "Heatmaps and other surfaces",
    "section": "Read in the data",
    "text": "Read in the data\nWe read in the example data we will use for all plots.\n\nagged_data &lt;- readRDS(file.path(agg_dir, \"achievement_aggregated.rds\"))\n\nThat has all the steps in the aggregation, but most of the plots here will only use a subset to demonstrate.\nTo make visualisation easier, the SDL units data is given a grouping column that puts the many env_obj variables in groups defined by their first two letters, e.g. EF for Ecosystem Function. These correspond to the ‘Target’ level, but it can be useful to have the two groupings together for some examples.\nIf we had used multiple aggregation functions at any step, we should filter down to the one we want here, but we only used one for this example.\n\nscenarios &lt;- yaml::read_yaml(file.path(hydro_dir, 'scenario_metadata.yml')) |&gt; \n  tibble::as_tibble()\n\n# join the scenario information to each of those sheets\nagged_data &lt;- agged_data |&gt;\n  purrr::map(\\(x) left_join(x, scenarios, by = 'scenario'))",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Surfaces (heatmaps and contours)"
    ]
  },
  {
    "objectID": "comparer/heatmap.html#qualitative-axes",
    "href": "comparer/heatmap.html#qualitative-axes",
    "title": "Heatmaps and other surfaces",
    "section": "Qualitative axes",
    "text": "Qualitative axes\nIf we have qualitative descriptions on x- and y-, the heatmap will have even steps. Note here the simple cleanup of the names and the need to make the adapt_code column non-numeric.\n\nqual_heatmap &lt;- agged_data$Target |&gt;\n  dplyr::filter(adapt_code %in% c(1,2,3, 4)) |&gt; \n  dplyr::filter(!is.na(Target)) |&gt; \n  # clean names\n  mutate(SWSDLName = stringr::str_replace(SWSDLName, '–', '-\\n'),\n         Target = stringr::str_replace(Target, 'Priority e', 'E'),\n         Target = stringr::str_wrap(Target, width = 12),\n         adapt_code = as.character(adapt_code)) |&gt;\n  plot_outcomes(outcome_col = 'ewr_achieved',\n                outcome_lab = 'Condition',\n                y_col = 'adapt_code', \n                y_lab = 'Adaptation option',\n                x_col = 'climate_code', \n                x_lab = 'Climate scenario',\n                plot_type = 'heatmap',\n                colorset = 'ewr_achieved',\n                pal_list = 'grDevices::Viridis',\n                facet_row = 'Target',\n                facet_col = 'SWSDLName')\n\nqual_heatmap",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Surfaces (heatmaps and contours)"
    ]
  },
  {
    "objectID": "comparer/heatmap.html#quantitative-axes",
    "href": "comparer/heatmap.html#quantitative-axes",
    "title": "Heatmaps and other surfaces",
    "section": "Quantitative axes",
    "text": "Quantitative axes\nHere, our scenarios are defined quantitatively, and so we can allow x and y to be quantitative. A key issue here is the spacing of the scenarios. We log-transform the axes to help a bit, but if this were a targetted output, even scenario spacing would be desirable.\n\n# Quantitative axes- it's more informative, but uglier\nquant_heatmap &lt;- agged_data$Target |&gt;\n  dplyr::filter(adapt_code %in% c(1,2,3)) |&gt; \n  # clean names\n  mutate(SWSDLName = stringr::str_replace(SWSDLName, '–', '-\\n'),\n         Target = stringr::str_replace(Target, 'Priority e', 'E'),\n         Target = stringr::str_wrap(Target, width = 12)) |&gt;\n  mutate(flow_addition = flow_addition + 1) |&gt; \n  plot_outcomes(outcome_col = 'ewr_achieved',\n                outcome_lab = 'Condition',\n                y_col = 'flow_addition', \n                y_lab = \"Flow addition ('adaptation')\",\n                x_col = 'flow_multiplier', \n                x_lab = \"Flow multiplier ('climate')\",\n                plot_type = 'heatmap',\n                transy = 'log10',\n                transx = 'log10',\n                colorset = 'ewr_achieved',\n                pal_list = 'grDevices::Viridis',\n                facet_row = 'Target',\n                facet_col = 'SWSDLName')\n\nquant_heatmap",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Surfaces (heatmaps and contours)"
    ]
  },
  {
    "objectID": "comparer/heatmap.html#interpolated-raster",
    "href": "comparer/heatmap.html#interpolated-raster",
    "title": "Heatmaps and other surfaces",
    "section": "Interpolated raster",
    "text": "Interpolated raster\nWe can try to fill in the missing locations by interpolating a raster using the contour_arglist, which passes arguments to geom_raster (if it contains ‘interpolate’) or geom_contour (if it does not). That helps, but the scenarios are still really too far apart, and so closer scenario spacing would be better.\n\nquant_interp_heatmap &lt;- agged_data$Target |&gt;\n  dplyr::filter(adapt_code %in% c(1,2,3)) |&gt;\n  # clean names\n  mutate(SWSDLName = stringr::str_replace(SWSDLName, '–', '-\\n'),\n         Target = stringr::str_replace(Target, 'Priority e', 'E'),\n         Target = stringr::str_wrap(Target, width = 12)) |&gt;\n  mutate(flow_addition = flow_addition + 1) |&gt; \n  plot_outcomes(outcome_col = 'ewr_achieved',\n                outcome_lab = 'Condition',\n                y_col = 'flow_addition', \n                y_lab = \"Flow addition ('adaptation')\",\n                x_col = 'flow_multiplier', \n                x_lab = \"Flow multiplier ('climate')\",\n                plot_type = 'heatmap',\n                transy = 'log10',\n                transx = 'log10',\n                colorset = 'ewr_achieved',\n                pal_list = 'grDevices::Viridis',\n                facet_row = 'Target',\n                facet_col = 'SWSDLName',\n                contour_arglist = list(interpolate = TRUE))\n\nquant_interp_heatmap\n\nWarning: Raster pixels are placed at uneven horizontal intervals and will be shifted\nℹ Consider using `geom_tile()` instead.\nRaster pixels are placed at uneven horizontal intervals and will be shifted\nℹ Consider using `geom_tile()` instead.",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Surfaces (heatmaps and contours)"
    ]
  },
  {
    "objectID": "comparer/heatmap.html#contours",
    "href": "comparer/heatmap.html#contours",
    "title": "Heatmaps and other surfaces",
    "section": "Contours",
    "text": "Contours\nWe can also just fit contour surfaces, done by passing a list of arguments to contour_arglist. If it is an empty list, as here, it simply triggers the contour. Again, these contours would be better if they were fit to tighter scenario spacing.\n\n  contour_heatmap &lt;- agged_data$Target |&gt;\n  dplyr::filter(adapt_code %in% c(1,2,3)) |&gt; \n  # clean names\n  mutate(scenario = paste0(climate_code, adapt_code), \n         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\\n'),\n         Target = stringr::str_replace(Target, 'Priority e', 'E'),\n         Target = stringr::str_wrap(Target, width = 12),\n         adapt_code = as.character(adapt_code)) |&gt;\n  mutate(flow_addition = flow_addition + 1) |&gt; \n  plot_outcomes(outcome_col = 'ewr_achieved',\n                outcome_lab = \"Proportion\\nEWR achieved\",\n                y_col = 'flow_addition', \n                y_lab = \"Flow addition ('adaptation')\",\n                x_col = 'flow_multiplier', \n                x_lab = \"Flow multiplier ('climate')\",\n                plot_type = 'heatmap',\n                transy = 'log10',\n                transx = 'log10',\n                colorset = 'ewr_achieved',\n                pal_list = 'grDevices::Viridis',\n                facet_row = 'Target',\n                facet_col = 'SWSDLName',\n                contour_arglist = list()\n  )\n\ncontour_heatmap\n\n\n\n\n\n\n\nWe can use contour_arglist to pass arguments to geom_contour, e.g. changing the bins, binwidth, or breaks.\n\n  contour_heatmap_adj &lt;- agged_data$Target |&gt;\n  dplyr::filter(adapt_code %in% c(1,2,3)) |&gt; \n  # clean names\n  mutate(scenario = paste0(climate_code, adapt_code), \n         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\\n'),\n         Target = stringr::str_replace(Target, 'Priority e', 'E'),\n         Target = stringr::str_wrap(Target, width = 12),\n         adapt_code = as.character(adapt_code)) |&gt;\n  mutate(flow_addition = flow_addition + 1) |&gt; \n  plot_outcomes(outcome_col = 'ewr_achieved',\n                outcome_lab = \"Proportion\\nEWR achieved\",\n                y_col = 'flow_addition', \n                y_lab = \"Flow addition ('adaptation')\",\n                x_col = 'flow_multiplier', \n                x_lab = \"Flow multiplier ('climate')\",\n                plot_type = 'heatmap',\n                transy = 'log10',\n                transx = 'log10',\n                colorset = 'ewr_achieved',\n                pal_list = 'grDevices::Viridis',\n                facet_row = 'Target',\n                facet_col = 'SWSDLName',\n                contour_arglist = list(bins = 30)\n  )\n\ncontour_heatmap_adj",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Surfaces (heatmaps and contours)"
    ]
  },
  {
    "objectID": "aggregator/user_causal.html",
    "href": "aggregator/user_causal.html",
    "title": "User-provided causal networks",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(dplyr)\nlibrary(ggplot2)\nIn most of the demonstrations, we have used the HydroBOT::causal_ewr causal networks. However, the causal network is an argument to multi_aggregate() and read_and_agg(), and so it is possible for the user to pass arbitrary networks. Here, we demonstrate how to pass a causal network that isn’t provided by HydroBOT. We do that here with read_and_agg(), since that is the most common situation, but it also works with multi_aggregate.",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "User-supplied causal networks"
    ]
  },
  {
    "objectID": "aggregator/user_causal.html#network-from-ewr-tool",
    "href": "aggregator/user_causal.html#network-from-ewr-tool",
    "title": "User-provided causal networks",
    "section": "Network from EWR tool",
    "text": "Network from EWR tool\nOne use that is likely to be common is to extract the (sometimes newer, but less tested) causal networks from the EWR tool with get_causal_ewr().\n\nagg_ewr_causal &lt;- read_and_agg(\n  datpath = ewr_results,\n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = get_causal_ewr(),\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  group_until = list(\n    SWSDLName = is_notpoint,\n    planning_unit_name = is_notpoint,\n    gauge = is_notpoint\n  ),\n  pseudo_spatial = \"sdl_units\",\n  aggsequence = aggseq,\n  funsequence = funseq,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = FALSE,\n  returnList = TRUE,\n  savepath = agg_results,\n  add_max = FALSE\n)\n\n! Unmatched links in causal network\n• 11 from env_obj to Specific_goal\n! Unmatched links in causal network\n• 7 from Objective to target_5_year_2024",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "User-supplied causal networks"
    ]
  },
  {
    "objectID": "aggregator/user_causal.html#arbitrary-network",
    "href": "aggregator/user_causal.html#arbitrary-network",
    "title": "User-provided causal networks",
    "section": "Arbitrary network",
    "text": "Arbitrary network\nIt is also possible to use any arbitrary network with the needed links (columns). Here, we make up a very simple one. See causal_ewr for needed structure; the main key is it needs to be a list of dataframe(s).\n\nfakegroups &lt;- c('a', 'b', 'c')\nfake_causal &lt;- tibble::tibble(ewr_code_timing = unique(agg_ewr_causal$agg_input$ewr_code_timing),\n                              fake_group = sample(fakegroups,\n                                                  length(unique(agg_ewr_causal$agg_input$ewr_code_timing)), \n                                                  replace = TRUE))\n\n\naggseq_fakecausal &lt;- list(\n  all_time = 'all_time',\n  fake_group = c(\"ewr_code_timing\", \"fake_group\"),\n  sdl_units = sdl_units\n)\n\nfunseq_fakecausal &lt;- list(\n  all_time = 'ArithmeticMean',\n  fake_group = \"CompensatingFactor\",\n  sdl_units = \"ArithmeticMean\"\n  )\n\n\n# fake_causal_agg &lt;- multi_aggregate(   \n#   dat = ewrdata,      \n#   causal_edges = list(fake_causal),   \n#   groupers = \"scenario\",   \n#   aggCols = \"ewr_achieved\",\n#   aggsequence = aggseq_fakecausal,   \n#   funsequence = funseq_fakecausal,\n#   auto_ewr_PU = TRUE,\n#   namehistory = FALSE) \n\nfake_causal_agg &lt;- read_and_agg(\n  datpath = ewr_results,\n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = list(fake_causal),\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  group_until = list(\n    SWSDLName = is_notpoint,\n    planning_unit_name = is_notpoint,\n    gauge = is_notpoint\n  ),\n  pseudo_spatial = \"sdl_units\",\n  aggsequence = aggseq_fakecausal,\n  funsequence = funseq_fakecausal,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = FALSE,\n  returnList = TRUE,\n  savepath = agg_results,\n  add_max = FALSE\n)\n\nWarning in filtergroups(thisdf, fromcol = p[1], tocol = p[2], fromfilter =\nfromfilter, : Unable to cross-check gauges and planning units, trusting the\nuser they work together\n\n\nWarning: Causal network does not have all groupers.\n• Joining ewr_code_timing to fake_group\n• Groupers are scenario, SWSDLName, planning_unit_name, gauge, polyID.\n• expect causal network to have SWSDLName, planning_unit_name, gauge; it has ewr_code_timing, fake_group, fromtype, totype, edgeorder.\n• Do you need to use `group_until`? Or is your network missing columns?\n\nfake_causal_agg\n\n$agg_input\nSimple feature collection with 2808 features and 12 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 144.8811 ymin: -33.8695 xmax: 148.6839 ymax: -30.4577\nGeodetic CRS:  GDA94\n# A tibble: 2,808 × 13\n   scenario  year date       gauge  planning_unit_name  state SWSDLName ewr_code\n   &lt;chr&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;  &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;   \n 1 base      2014 2014-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 2 base      2015 2015-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 3 base      2016 2016-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 4 base      2017 2017-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 5 base      2018 2018-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 6 base      2019 2019-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 7 base      2014 2014-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 8 base      2015 2015-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n 9 base      2016 2016-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n10 base      2017 2017-07-01 421004 Baroona to Warren … NSW   Macquari… BF1     \n# ℹ 2,798 more rows\n# ℹ 5 more variables: ewr_code_timing &lt;chr&gt;, event_years &lt;dbl&gt;,\n#   ewr_achieved &lt;dbl&gt;, interevent_achieved &lt;dbl&gt;, geometry &lt;POINT [°]&gt;\n\n$all_time\nSimple feature collection with 414 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 144.8811 ymin: -33.8695 xmax: 148.6839 ymax: -30.4577\nGeodetic CRS:  GDA94\n# A tibble: 414 × 10\n   scenario SWSDLName planning_unit_name            gauge ewr_code_timing polyID\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;                         &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt; \n 1 base     Lachlan   Lachlan River - Lake Cargell… 4120… BF1_a           r4pdr…\n 2 base     Lachlan   Lachlan River - Lake Cargell… 4120… BF1_b           r4pdr…\n 3 base     Lachlan   Lachlan River - Lake Cargell… 4120… BF2_a           r4pdr…\n 4 base     Lachlan   Lachlan River - Lake Cargell… 4120… BF2_b           r4pdr…\n 5 base     Lachlan   Lachlan River - Lake Cargell… 4120… BK1_P           r4pdr…\n 6 base     Lachlan   Lachlan River - Lake Cargell… 4120… BK1_S           r4pdr…\n 7 base     Lachlan   Lachlan River - Lake Cargell… 4120… CF1_b           r4pdr…\n 8 base     Lachlan   Lachlan River - Lake Cargell… 4120… CF1_c           r4pdr…\n 9 base     Lachlan   Lachlan River - Lake Cargell… 4120… LF1_P           r4pdr…\n10 base     Lachlan   Lachlan River - Lake Cargell… 4120… LF1_S           r4pdr…\n# ℹ 404 more rows\n# ℹ 4 more variables: geometry &lt;POINT [°]&gt;, ewr_achieved &lt;dbl&gt;, aggfun_1 &lt;chr&gt;,\n#   aggLevel_1 &lt;chr&gt;\n\n$fake_group\nSimple feature collection with 81 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 144.8811 ymin: -33.8695 xmax: 148.6839 ymax: -30.4577\nGeodetic CRS:  GDA94\n# A tibble: 81 × 12\n   scenario SWSDLName planning_unit_name                 gauge polyID fake_group\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;                              &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     \n 1 base     Lachlan   Lachlan River - Lake Cargelligo t… 4120… r4pdr… a         \n 2 base     Lachlan   Lachlan River - Lake Cargelligo t… 4120… r4pdr… b         \n 3 base     Lachlan   Lachlan River - Lake Cargelligo t… 4120… r4pdr… c         \n 4 base     Lachlan   Merrimajeel Creek                  4120… r1zp2… a         \n 5 base     Lachlan   Merrimajeel Creek                  4120… r1zp2… b         \n 6 base     Lachlan   Merrimajeel Creek                  4120… r1zp2… c         \n 7 base     Lachlan   Merrowie Creek                     4120… r4pdr… a         \n 8 base     Lachlan   Merrowie Creek                     4120… r4pdr… b         \n 9 base     Lachlan   Merrowie Creek                     4120… r4pdr… c         \n10 base     Lachlan   Muggabah Creek                     4120… r1zp2… a         \n# ℹ 71 more rows\n# ℹ 6 more variables: geometry &lt;POINT [°]&gt;, ewr_achieved &lt;dbl&gt;, aggfun_1 &lt;chr&gt;,\n#   aggLevel_1 &lt;chr&gt;, aggfun_2 &lt;chr&gt;, aggLevel_2 &lt;chr&gt;\n\n$sdl_units\nSimple feature collection with 18 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 143.8092 ymin: -34.98763 xmax: 150.3614 ymax: -29.91875\nGeodetic CRS:  GDA94\n# A tibble: 18 × 14\n   scenario fake_group polyID      SWSDLID StateID SWSDLName            \n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                \n 1 base     a          r602ydft049 SS16    NSW     Lachlan              \n 2 base     a          r6367k2uy6m SS20    NSW     Macquarie-Castlereagh\n 3 base     b          r602ydft049 SS16    NSW     Lachlan              \n 4 base     b          r6367k2uy6m SS20    NSW     Macquarie-Castlereagh\n 5 base     c          r602ydft049 SS16    NSW     Lachlan              \n 6 base     c          r6367k2uy6m SS20    NSW     Macquarie-Castlereagh\n 7 down4    a          r602ydft049 SS16    NSW     Lachlan              \n 8 down4    a          r6367k2uy6m SS20    NSW     Macquarie-Castlereagh\n 9 down4    b          r602ydft049 SS16    NSW     Lachlan              \n10 down4    b          r6367k2uy6m SS20    NSW     Macquarie-Castlereagh\n11 down4    c          r602ydft049 SS16    NSW     Lachlan              \n12 down4    c          r6367k2uy6m SS20    NSW     Macquarie-Castlereagh\n13 up4      a          r602ydft049 SS16    NSW     Lachlan              \n14 up4      a          r6367k2uy6m SS20    NSW     Macquarie-Castlereagh\n15 up4      b          r602ydft049 SS16    NSW     Lachlan              \n16 up4      b          r6367k2uy6m SS20    NSW     Macquarie-Castlereagh\n17 up4      c          r602ydft049 SS16    NSW     Lachlan              \n18 up4      c          r6367k2uy6m SS20    NSW     Macquarie-Castlereagh\n# ℹ 8 more variables: geometry &lt;MULTIPOLYGON [°]&gt;, ewr_achieved &lt;dbl&gt;,\n#   aggfun_1 &lt;chr&gt;, aggLevel_1 &lt;chr&gt;, aggfun_2 &lt;chr&gt;, aggLevel_2 &lt;chr&gt;,\n#   aggfun_3 &lt;chr&gt;, aggLevel_3 &lt;chr&gt;\n\n\nAnd a quick plot of the random groupings implied there.\n\nfake_causal_agg$sdl_units |&gt; \n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"fake_group\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n  )",
    "crumbs": [
      "Aggregator",
      "Syntax and details",
      "User-supplied causal networks"
    ]
  },
  {
    "objectID": "provided_data/causal_descriptive_plots.html",
    "href": "provided_data/causal_descriptive_plots.html",
    "title": "Causal Network Plotting",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(dplyr)\nThere are many reasons to present causal networks, which we might want to visualise in different ways. Causal networks are inherently complex, and there are different aspects we might want to accentuate, and some capability we need to prune/focus them. Thus, {HydroBOT} provides a number of helper functions and plotting options.",
    "crumbs": [
      "Provided data and Causal networks",
      "Causal networks",
      "Plotting causal networks"
    ]
  },
  {
    "objectID": "provided_data/causal_descriptive_plots.html#setup",
    "href": "provided_data/causal_descriptive_plots.html#setup",
    "title": "Causal Network Plotting",
    "section": "Setup",
    "text": "Setup\nFirst we need the relationships defining the network, which are extracted from tables in {HydroBOT} and provided as the dataset HydroBOT::causal_ewrs.\nProcess data for network plots\nCausal networks need an edges dataframe specifying all pairwise connections, and a nodes dataframe specifying the nodes and their attributes. We make those here, illustrating some key points.\n\n\nThe full set of possible links is massively factorial, and so we want to choose only those useful for the needs of a given analysis.\n\nAnalyses may differ depending on network detail, spatial resolution, or use in HydroBOT outside the network (e.g. theme aggregation)\n\n\nA key feature of the network isn’t just the existence of connections, but their directionality, and so we want to specify that explicitly.\n\nWe need to have the general ability to make edges and nodes available for other uses, e.g. aggregations\n\n{HydroBOT} exports make_edges() and make_nodes(), along with some other network-manipulation functions\n\n\n\nTo build the nodes and edges for a specific plot or set of plots, we first build a dataframe of edges, and then extract nodes.\nConstruct edge dataframe\nThe edges dataframe contains all pairwise links between nodes in from and to columns. To get that, we need to pass it dataframes specifying links. There will often be multiple datasets, reflecting the differing scales of the nodes (as in the causal_ewrs list provided by {HydroBOT}). These dataframes may include many columns of potential nodes, e.g. they might provide the mapping for several steps in the network. Thus, we need to provide the node columns we actually want to map and their directionality- what are the ‘from-to’ pairings. We may want to filter to only some subset of nodes; for example we may only be interested in the environmental objectives related to waterbirds. While we demonstrate this here for causal_ewr, as long as the from-to groupings are available, any arbitrary dataframe specifying the groupings can be used as a causal network, allowing users to specify their own.\nAs an example, we can make the relationships present at gauge 409025 linking EWRs to environmental objectives, environmental objectives to specific goals, Specific goals to Targets, and environmental objectives to 5-year targets. There are many more possible connections to include in the fromtos, which to include will depend on the questions being asked.\n\nedges &lt;- make_edges(\n  dflist = causal_ewr,\n  fromtos = list(\n    c(\"ewr_code\", \"env_obj\"),\n    c(\"env_obj\", \"Specific_goal\"),\n    c(\"Specific_goal\", \"Target\"),\n    c(\"env_obj\", \"target_5_year_2024\")\n  ),\n  gaugefilter = \"409025\"\n)\n\nedges\n\n\n  \n\n\n\nThere’s also the opportunity to filter the specific nodes to include with fromfilter and tofilter. This allows things like filtering the particular nodes within those node categories (e.g. env_objs related to waterbirds). However, it is typically better to use find_related_nodes() after creation of the network, as that does network-aware filtering.\nConstruct node dataframe\nThe node dataframe defines the ‘boxes’. The simplest way to make it is to extract it from the edges. Basically, we just grab all the nodes that are in either the from or to columns of the edges df. The make_nodes() function does a bit more than just get unique node values from the edges df, it also attaches a column specifying the node order, reflecting their sequence in the causal network. There is a default sequence specified for WERP EWRs, but others can be specified with the typeorder argument.\n\nnodes &lt;- make_nodes(edges)\nnodes\n\n\n  \n\n\n\nWe can now create a minimal plot before digging back in to demo some options under the hood.",
    "crumbs": [
      "Provided data and Causal networks",
      "Causal networks",
      "Plotting causal networks"
    ]
  },
  {
    "objectID": "provided_data/causal_descriptive_plots.html#node-relevant-network",
    "href": "provided_data/causal_descriptive_plots.html#node-relevant-network",
    "title": "Causal Network Plotting",
    "section": "Node-relevant network",
    "text": "Node-relevant network\nOne common need is to ask about the connections that relate to a node or a small set of nodes. To do that, we need to be able to traverse the network upstream and downstream, using the focalnodes argument, which calls the find_related_nodes() function. This is a more complete network restructuring than just filtering a target level, as we did above, because it traces the full network and only returns nodes at any level that relate to the targets. Now we can include the 5-year targets again because we’ve reduce the nodes. Note that the focalnodes don’t have to be related to each other or at the same level- find_related_nodes() prunes the network to all connections involving all the focalnodes.\n\nmake_causal_plot(nodes, edges,\n  focalnodes = c(\"NF4\", \"Sloanes froglet\"), render = FALSE\n) |&gt;\n  DiagrammeR::render_graph()",
    "crumbs": [
      "Provided data and Causal networks",
      "Causal networks",
      "Plotting causal networks"
    ]
  },
  {
    "objectID": "provided_data/causal_descriptive_plots.html#more-options",
    "href": "provided_data/causal_descriptive_plots.html#more-options",
    "title": "Causal Network Plotting",
    "section": "More options",
    "text": "More options\nThe above is running with defaults, but there’s quite a bit more capacity to change what is plotted and the look of the graphs. A primary example is feeding the outputs of HydroBOT to the the network (e.g. shifts in the relationships or values of the nodes, such as fewer birds- see for example here and theme aggregation.\nColour can be specified differently than is done by default above, such as coloring the nodes within the node groups by outcome or assigning different node groups different color palettes, following the same idea as the generic colorgroups and colorset arguments in all the plotting functions. Making the networks interactive/dynamic is not yet possible, though tooltips work in html.\nColour to indicate a value\nEdges\nEdges can have colour in a column, as it would be if it came in as results from HydroBOT. For example, we might want to colour the edges by change between scenarios, or strength of relationships.\nAs a demonstration, we add a value column to edges as a mock-up of HydroBOT outputs and plot. We use a continuous palette here rather than the default, since this is now a continuous variable. For this demonstration, we use a smaller network to make things visible. Note that above we removed the 5-year targets from the nodes df, and here we use edges since we’re already modifying it. Either approach can drop a set of nodes, though it’s generally easier to use the nodes since nodes can appear in either the from or to of the edges.\n\nedgewithvals &lt;- edges |&gt;\n  filter(totype != \"target_5_year_2024\") |&gt;\n  mutate(value = rnorm(n()))\n\nmake_causal_plot(nodes,\n  edgewithvals,\n  focalnodes = c(\"NF4\", \"Sloanes froglet\"),\n  edge_pal = list(value = \"viridis::plasma\"),\n  edge_colorset = \"value\", render = FALSE\n) |&gt;\n  DiagrammeR::render_graph()\n\n\n\n\n\nNodes (and single-colour edges)\nWe can also colour the nodes by results (with a larger demonstration here). Here we set the edges just to a single color - feeding edge_pal or node_pal a single character value specifying a colour or a character vector of length nrow of the relevant dataframe will just insert those values and bypass the palettes. Again, we start by dummying up some ‘HydroBOT results’ in a value column. Examples where these values do come out of EWR results is in the theme aggregation notebook and the overview presentation.\n\nnodewithvals &lt;- nodes |&gt;\n  filter(NodeType != \"target_5_year_2024\") |&gt;\n  mutate(value = rnorm(n()))\n\nmake_causal_plot(nodewithvals,\n  edges,\n  focalnodes = c(\"NF4\", \"Sloanes froglet\"),\n  edge_pal = \"black\",\n  node_pal = list(value = \"scico::oslo\"),\n  node_colorset = \"value\", render = FALSE\n) |&gt;\n  DiagrammeR::render_graph()\n\n\n\n\n\nColour within node groups\nWe might want to use different colour palettes within the different node groups, but colour the nodes themselves within them, similar to other examples with bar plots and lines. To do that, we set the *_pal arguments as named lists of palettes, and also pass *_colorgroups arguments so it knows how to split the data into those palettes. This parallels the use in plot_outcomes() for other plot types, where colorgroups is the groups that get the palette, while colorset are the individual units within each group that receive colors from the respective palette, see the comparer syntax. Here, we demonstrate with nodes and plot the whole network so we can see what’s happening.\nFirst, set the list of palettes. This could be set by default for the project, along with other default colors.\n\nnode_list_c &lt;- list(\n  ewr_code = \"viridis::mako\",\n  env_obj = \"viridis::plasma\",\n  Specific_goal = \"scico::oslo\",\n  Target = \"scico::hawaii\",\n  target_5_year_2024 = \"scico::lisbon\"\n)\n\nThen, make the network\n\nmake_causal_plot(nodes, edges,\n  edge_pal = \"black\",\n  node_pal = node_list_c,\n  node_colorgroups = \"NodeType\",\n  node_colorset = \"Name\", render = FALSE\n) |&gt;\n  DiagrammeR::render_graph()\n\n\n\n\n\nGroupings within node groups\nIt’s possible but gets rapidly bespoke to break those up into more discrete chunks. There is a built in ‘werp’ default that groups EWRs and environmental objectives by their main group, and lumps all the targets by year (though only including 5-year in this example). That default can be accessed by passing 'werp' to as the node_colorset. This is an end-run that builds a new column to use as colorset, defined according to some defaults. The same thing could be done externally by creating a new column to define the colorset outside the function and then making that column colorset.\nIn that case, we might want to use a different set of palettes,\n\nnode_list_g &lt;- list(\n  ewr_code = \"viridis::mako\",\n  env_obj = \"ggthemes::excel_Green\",\n  Specific_goal = \"scico::oslo\",\n  Target = \"calecopal::superbloom3\",\n  target_5_year_2024 = \"calecopal::eschscholzia\"\n)\n\nThis lets us set the within-node_colorgroups palettes. The node_colorset = 'werp' just creates a new column in the data that gives rows values we want (e.g. the first two letters of the ewr_codes), which are then used to choose colors from that particular palette. It’s a way to have fewer colors (and more meaningful colors) within the colorgroups.\n\nmake_causal_plot(nodes, edges,\n  edge_pal = \"black\",\n  node_pal = node_list_g,\n  node_colorgroups = \"NodeType\",\n  node_colorset = \"werp\", render = FALSE\n) |&gt;\n  DiagrammeR::render_graph()",
    "crumbs": [
      "Provided data and Causal networks",
      "Causal networks",
      "Plotting causal networks"
    ]
  },
  {
    "objectID": "provided_data/causal_descriptive_plots.html#next-steps",
    "href": "provided_data/causal_descriptive_plots.html#next-steps",
    "title": "Causal Network Plotting",
    "section": "Next steps",
    "text": "Next steps\nThere’s clearly a lot more that could be done here. The nature of the output and the way its set it up are really aimed at being interactive and usable to investigate the network.\nThe setup here is actually quite similar to the other plot outputs. In all cases, there is much opportunity to adjust the look of the plots to target different uses, but also the ability to establish a consistent default (and look). These plots lend themselves to both notebooks (as here), web, and interactive interfaces (Shiny, observablejs, etc) to investigate the networks themselves or use them to plot results. That gives flexibility to get at whatever the particular question is. Though causal networks are not a typical way to present outputs, here we see that they can be incredibly powerful for not only showing the relationships, but understanding how they change under different scenarios.",
    "crumbs": [
      "Provided data and Causal networks",
      "Causal networks",
      "Plotting causal networks"
    ]
  },
  {
    "objectID": "workflows/workflow_parameters.html",
    "href": "workflows/workflow_parameters.html",
    "title": "Parameterised workflow",
    "section": "",
    "text": "This document provides a template for running HydroBOT from a parameters file, as we might do when batch processing. As such, it typically wouldn’t be run through a notebook, but be called with Rscript. That sort of setup can go a lot of different directions depending on use case, and we assume a user would be familiar with shell scripting and the particular idiosyncracies of their relevant batching system. Here, we demonstrate how to set up the parameter file and use it, and leave it to the user to build the script that gets called with Rscript from the command line or as part of an external process.\nWe have the ability to have a default params file, a second params file that tweaks those defaults, as well as include params in Quarto header yaml. These different options are all used here.\nLoad the package\nlibrary(HydroBOT)",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "From parameters"
    ]
  },
  {
    "objectID": "workflows/workflow_parameters.html#structure-of-params-files-and-arguments",
    "href": "workflows/workflow_parameters.html#structure-of-params-files-and-arguments",
    "title": "Parameterised workflow",
    "section": "Structure of params files and arguments",
    "text": "Structure of params files and arguments\nThe run_hydrobot_params function takes four arguments: yamlpath, which is a path to a yaml params file, passed_args which can come from the command line, list_args, and defaults, which is another yaml file. This two-yaml approach lets us set most of the params in common across all runs, and only modify a subset with the yamlpath file or passed_args.\nIn all cases, the arguments end up in a list with two top-level items: ewr and aggregation, within which items can be added with names matching the arguments to [prep_run_save_ewrs()] and [read_and_agg()], respectively. This gives full control over those functions.\nThe package comes with a set of default parameters in system.file('yml/default_params.yml', package = 'HydroBOT'). Users can (should) however create their own default yaml params file to set a standard set of defaults for a given project. See this file for basic structure.\nThe params.yml file (or any other name, passed to yamlpath) and passed_args and list_args then can be used to modify the default values. The idea is only a small subset of those defaults would be modified for a particular run.\nIn general, it is best to specify everything in terms of characters, logicals, or NULL. If there is a situation where that isn’t possible (bespoke spatial data, for example), it is possible to specify the aggsequence and funsequence with an R script. To do that, change the aggregation_def entry of the aggregation list to the path to that R script. For an example, see system.file('yml/params.R', package = 'HydroBOT').\nFinally, [run_hydrobot_params()] ingests paths to these files (or passed command line or lists), turns their params into R arguments, and runs HydroBOT.\n\n\n\n\n\n\nImportant\n\n\n\nThe arguments overwrite each other, so list_args has highest precedence, followed by passed_args, yamlpath, and finally defaults.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAt present we do not provide yaml param options for the comparer. This is possible, but the possibilities are a bit too wide open. It is likely the user will want to explore the output, rather than generate parameterised output, though that may change in future.",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "From parameters"
    ]
  },
  {
    "objectID": "workflows/workflow_parameters.html#additional-parameters",
    "href": "workflows/workflow_parameters.html#additional-parameters",
    "title": "Parameterised workflow",
    "section": "Additional parameters",
    "text": "Additional parameters\nSpecify the aggregation sequence in R and pass the path to that file.\naggregation:\n  # aggregation sequences (need to be defined in R)\n  aggregation_def: 'toolkit_project/agg_params.R'",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "From parameters"
    ]
  },
  {
    "objectID": "workflows/workflow_parameters.html#directories",
    "href": "workflows/workflow_parameters.html#directories",
    "title": "Parameterised workflow",
    "section": "Directories",
    "text": "Directories\nInput and output directories\newr:\n  # Outer directory for scenario\n  output_parent_dir: 'toolkit_project'\n\n  # Preexisting data\n  # Hydrographs (expected to exist already)\n  hydro_dir: NULL\n  \n\naggregation:\n  # outputs of aggregator\n  savepath: 'path/to/aggs'\nNormally output_parent_dir should point somewhere external (though keeping it inside or alongside the hydrograph data is a good idea.).\nSetting the output directories to NULL expects (in the case of hydro_dir) or builds (for savepath) a standard toolkit directory structure, with output_parent_dir as the outer directory, holding hydrographs, aggregator_output, and module_output subdirectories.",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "From parameters"
    ]
  },
  {
    "objectID": "workflows/workflow_parameters.html#module-arguments",
    "href": "workflows/workflow_parameters.html#module-arguments",
    "title": "Parameterised workflow",
    "section": "Module arguments",
    "text": "Module arguments\nCurrently, just the EWR tool. Any argument in [prep_run_save_ewrs()] can be passed. Some examples are\newr:\n  # Model type\n  model_format: 'IQQM - netcdf'\n  \n  # output and return\n  outputType:\n    - summary\n  \n  returnType: none",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "From parameters"
    ]
  },
  {
    "objectID": "workflows/workflow_parameters.html#aggregation-settings",
    "href": "workflows/workflow_parameters.html#aggregation-settings",
    "title": "Parameterised workflow",
    "section": "Aggregation settings",
    "text": "Aggregation settings\nAny arguments to read_and_agg. Some examples are\naggregation:\n  # What to aggregate\n  type: achievement\n  \n  # Aggregation settings\n  groupers: scenario\n  aggCols: ewr_achieved\n  namehistory: FALSE\n  keepAllPolys: TRUE",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "From parameters"
    ]
  },
  {
    "objectID": "workflows/workflow_parameters.html#replication",
    "href": "workflows/workflow_parameters.html#replication",
    "title": "Parameterised workflow",
    "section": "Replication",
    "text": "Replication\nThe prep_run_save_ewrs and read_and_agg functions save metadata yaml files that are fully-specified parameters files. Thus, to replicate runs, we can run from the final yaml (after the aggregator), as it has all preceding steps.\n\nrun_hydrobot_params(yamlpath = \"hydrobot_scenarios/aggregator_output/agg_metadata.yml\")",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "From parameters"
    ]
  },
  {
    "objectID": "comparer/hydrographs.html",
    "href": "comparer/hydrographs.html",
    "title": "Hydrographs",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(ggplot2)\nlibrary(dplyr)",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Hydrographs"
    ]
  },
  {
    "objectID": "comparer/hydrographs.html#demonstration-setup",
    "href": "comparer/hydrographs.html#demonstration-setup",
    "title": "Hydrographs",
    "section": "Demonstration setup",
    "text": "Demonstration setup\nAs usual, we need paths to the data, in this case the hydrographs.\n\nproject_dir &lt;- file.path(\"more_scenarios\")\nhydro_dir &lt;- file.path(project_dir, \"hydrographs\")\n\nGet scenario metadata.\n\nscenarios &lt;- yaml::read_yaml(file.path(hydro_dir, \"scenario_metadata.yml\")) |&gt;\n  tibble::as_tibble()\n\nWe have a lot of hydrographs, so for this demonstration, we will often use a subset of gauges and scenarios.\n\ngauges_to_plot &lt;- c(\"412002\", \"419001\", \"422028\", \"421001\")\nscenarios_to_plot &lt;- c(\"climatedown2adapt0\", \"climatebaseadapt0\", \"climateup2adapt0\")\n\nscenarios &lt;- scenarios |&gt;\n  filter(scenario %in% scenarios_to_plot)",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Hydrographs"
    ]
  },
  {
    "objectID": "comparer/hydrographs.html#how-it-works",
    "href": "comparer/hydrographs.html#how-it-works",
    "title": "Hydrographs",
    "section": "How it works",
    "text": "How it works\nThe baseline can be either a scenario name or a scalar. We could potentially use something else like historical daily means, but that is not currently implemented, largely because it hasn’t yet been necessary- one of the scenarios is almost always an obvious baseline.\nInternally, the plot_outcome() function calls baseline_compare(), but baseline_compare() can be used externally as well (and is actually quite useful in a number of places to baseline long data).\nFor example, we might want to calculate the difference between the modified scenarios and the base scenario. To do this, we simply pass in the data, the columns defining the groups compare_col, and the base_lev, that is, the value in compare_col that is the baseline situation. We also need to tell it which column contains the values, and the function comp_fun we want to use for the comparison. Here, we ask for the difference to the baseline provided by the base scenario.\nIn short, this gets the difference of the 0.5x and 2x scenario compared to the historical. We use difference here because these are simple multiplicative scenarios, and so the relative comparison would be trivially 0.5 or 2. In general, though, relative change is often more appropriate.\n\ndif_flow &lt;- baseline_compare(scenehydros,\n  compare_col = \"scenario\",\n  base_lev = \"climatebaseadapt0\",\n  values_col = \"flow\",\n  comp_fun = difference,\n  group_cols = c(\"Date\", \"gauge\")\n)\n\nNow we can feed the dif_flow dataframe to plot_outcomes() directly. We see that, as expected, base is now a flat line at 0, while the 2x and 0.5x go up and down, respectively.\n\nplot_outcomes(dif_flow,\n  outcome_col = \"difference_flow\",\n  outcome_lab = \"Change in flow (ML/day)\",\n  x_col = \"Date\",\n  colorset = \"climate_code\",\n  color_lab = \"Scenario\",\n  pal_list = scene_pal,\n  # facet_row = 'climate_code',\n  facet_col = \"gauge\"\n)",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Hydrographs"
    ]
  },
  {
    "objectID": "comparer/hydrographs.html#internal-to-plot_outcomes",
    "href": "comparer/hydrographs.html#internal-to-plot_outcomes",
    "title": "Hydrographs",
    "section": "Internal to plot_outcomes()",
    "text": "Internal to plot_outcomes()\nIn practice, we would typically let the baselining happen internally to the plot_outcomes() function so we aren’t carrying around and keeping track of a bunch of altered dataframes. To replicate the above plot, we only have to feed plot_outcomes() the base_lev and comp_fun arguments in base_list. If we want to do something more unusual with the baselining, we will need to do it externally. We can though specify a custom function of x and y, here the extremely contrived sqrtdif.\n\n\n\n\n\n\nNote\n\n\n\nWe would get a warning here about not being explicit about the group_cols argument to the baselining function if it were not included, which just groups by everything non-numeric if that argument is not given.\n\n\nThis automatically labels with a long but meaningful name.\n\nsqrtdif &lt;- function(x, y) {\n  sqrt(abs(x - y))\n}\n\nplot_outcomes(scenehydros,\n  outcome_col = \"flow\",\n  x_col = \"Date\",\n  colorset = \"gauge\",\n  color_lab = \"Gauge ID:\",\n  pal_list = gauge_pal,\n  base_list = list(\n    base_lev = \"climatebaseadapt0\",\n    values_col = \"flow\",\n    comp_fun = \"sqrtdif\",\n    group_cols = c(\"Date\", \"gauge\")\n  ),\n  facet_col = \"climate_code\"\n)\n\n\n\n\n\n\n\nOne way this could be very useful is for lagged or windowed operations, which we do here very simply by getting the difference to the baseline one day previous. The use in practice would need to be developed carefully to address specific questions.\n\nlagdif &lt;- function(x, y) {\n  x - lag(y)\n}\n\nWe can change the outcome_lab as well, though to make sure information isn’t lost, the baselining is appended.\n\nlagplot &lt;- plot_outcomes(scenehydros,\n  outcome_col = \"flow\",\n  outcome_lab = \"Lagged flow difference\",\n  x_col = \"Date\",\n  colorset = \"gauge\",\n  color_lab = \"Gauge ID:\",\n  pal_list = gauge_pal,\n  base_list = list(\n    base_lev = \"climatebaseadapt0\",\n    values_col = \"flow\",\n    comp_fun = \"lagdif\",\n    group_cols = c(\"Date\", \"gauge\")\n  ),\n  facet_col = \"climate_code\"\n)\n\nlagplot\n\n\n\n\n\n\n\nBecause this is a ggplot object, we can change that y-label if we really don’t want the explanation.\n\nlagplot + labs(y = \"Lag 1 flow difference\")",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Hydrographs"
    ]
  },
  {
    "objectID": "workflows/workflow_in_memory.html",
    "href": "workflows/workflow_in_memory.html",
    "title": "Run full workflow in memory",
    "section": "",
    "text": "This document provides a template for running through HydroBOT in a single document, retaining everything in-memory (no intermediate saving). In general, given the time the EWR tool (and sometimes aggregation) take to run, this approach would only be taken with small sets of input data.\nIntermediate saving is a very simple flip of a switch, demoed in its own doc.\nLoad the package\nlibrary(HydroBOT)",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "In-memory"
    ]
  },
  {
    "objectID": "workflows/workflow_in_memory.html#directories",
    "href": "workflows/workflow_in_memory.html#directories",
    "title": "Run full workflow in memory",
    "section": "Directories",
    "text": "Directories\nInput and output directories\nHere, we will use the example hydrographs that come with HydroBOT.\nNormally project_dir and hydro_dir would point somewhere external (and typically, having hydro_dir inside project_dir will make life easier).\n\nhydro_dir &lt;- system.file(\"extdata/testsmall/hydrographs\", package = \"HydroBOT\")\n\nproject_dir &lt;- file.path(\"test_dir\")\n\n# Generated data\n# EWR outputs (will be created here in controller, read from here in aggregator)\newr_results &lt;- file.path(project_dir, \"module_output\", \"EWR\")\n\n# outputs of aggregator. There may be multiple modules\nagg_results &lt;- file.path(project_dir, \"aggregator_output\")",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "In-memory"
    ]
  },
  {
    "objectID": "workflows/workflow_in_memory.html#controller",
    "href": "workflows/workflow_in_memory.html#controller",
    "title": "Run full workflow in memory",
    "section": "Controller",
    "text": "Controller\nHere, we use a simple set of default arguments, see controller for detailed treatments of arguments and their meaning.\nControl output and return\nTo determine what to save and what to return to the active session, use outputType and returnType, respectively. Each of them can take a list of any of the EWR output options (see ?prep_run_save_ewrs()). For this demonstration I’ll not save anything and return yearly to the active session.\n\noutputType &lt;- list(\"none\")\nreturnType &lt;- list(\"yearly\")",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "In-memory"
    ]
  },
  {
    "objectID": "workflows/workflow_in_memory.html#aggregator",
    "href": "workflows/workflow_in_memory.html#aggregator",
    "title": "Run full workflow in memory",
    "section": "Aggregator",
    "text": "Aggregator\nTo keep this simple, we use one aggregation list and the read_and_agg wrapper to only have to pass paths. See the more detailed documents for the different ways to specify those aggregation lists.\nWhat to aggregate\nWe need to tell it the variable to aggregate, and any grouping variables other than time, themes, and spatial groups. Typically, scenario will be a grouper, but there may be others.\n\nagg_var &lt;- \"ewr_achieved\"\nagg_groups &lt;- \"scenario\"\n\nHow to aggregate\nFundamentally, the aggregator needs paths and two lists\n\nsequence of aggregations\nsequence of aggregation functions (can be multiple per step)\n\nHere, I’m using an interleaved list of theme and spatial aggregations (see the detailed docs for more explanation), and applying only a single aggregation function at each step for simplicity. Those steps can be specified a range of different ways, see the spatial and theme docs for more examples.\n\naggseq &lt;- list(\n  all_time = \"all_time\",\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  sdl_units = sdl_units,\n  env_obj = c(\"ewr_code\", \"env_obj\"),\n  Target = c(\"env_obj\", \"Target\"),\n  mdb = basin,\n  target_5_year_2024 = c(\"Target\", \"target_5_year_2024\")\n)\n\nfunseq &lt;- list(\n  \"ArithmeticMean\",\n  \"CompensatingFactor\",\n  \"ArithmeticMean\",\n  \"ArithmeticMean\",\n  \"ArithmeticMean\",\n  \"SpatialWeightedMean\",\n  \"ArithmeticMean\"\n)",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "In-memory"
    ]
  },
  {
    "objectID": "workflows/workflow_in_memory.html#controller-1",
    "href": "workflows/workflow_in_memory.html#controller-1",
    "title": "Run full workflow in memory",
    "section": "Controller",
    "text": "Controller\nThis is not actually run here for speed.\n\newr_out &lt;- prep_run_save_ewrs(\n  hydro_dir = hydro_dir,\n  output_parent_dir = project_dir,\n  outputType = outputType,\n  returnType = returnType\n)",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "In-memory"
    ]
  },
  {
    "objectID": "workflows/workflow_in_memory.html#aggregator-1",
    "href": "workflows/workflow_in_memory.html#aggregator-1",
    "title": "Run full workflow in memory",
    "section": "Aggregator",
    "text": "Aggregator\nBecause the chunk above is not run, the needed EWR outputs are not available, but would be if it were run.\n\naggout &lt;- read_and_agg(\n  datpath = ewr_out,\n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  auto_ewr_PU = TRUE,\n  aggsequence = aggseq,\n  funsequence = funseq,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = FALSE,\n  returnList = TRUE,\n  add_max = FALSE,\n  savepath = NULL\n)\n\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`.\n\n\nℹ EWR outputs auto-grouped\n• Done automatically because `auto_ewr_PU = TRUE`\n• EWRs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\n• Rows will collapse otherwise, silently aggregating over the wrong dimension\n• Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`\n.\n! Unmatched links in causal network\n• 29 from ewr_code_timing to ewr_code\nℹ EWR gauges joined to larger units pseudo-spatially.\n• Done automatically because `auto_ewr_PU = TRUE`\n• Non-spatial join needed because gauges may inform areas they are not within\n• Best to explicitly use `pseudo_spatial = 'sdl_units'` in `multi_aggregate()` or `read_and_agg()`.\n\n! Unmatched links in causal network\n• 4 from Target to target_5_year_2024",
    "crumbs": [
      "Workflows",
      "Example workflows",
      "In-memory"
    ]
  },
  {
    "objectID": "provided_data/scenario_creation.html",
    "href": "provided_data/scenario_creation.html",
    "title": "Creating simple scenarios",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(hydrogauge)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(dplyr)",
    "crumbs": [
      "Provided data and Causal networks",
      "Example scenarios",
      "Example scenario information and creation"
    ]
  },
  {
    "objectID": "provided_data/scenario_creation.html#hydrobot-relevance",
    "href": "provided_data/scenario_creation.html#hydrobot-relevance",
    "title": "Creating simple scenarios",
    "section": "HydroBOT relevance",
    "text": "HydroBOT relevance\nThe creation of flow scenarios is not part of HydroBOT proper. Instead, HydroBOT expects to ingest hydrographs and then handles the ongoing response models, aggregation, and analyses. Thus, hydrographs are an essential input to HydroBOT. The point of this code is to generate those hydrographs.\nThis notebook generates 9 simple scenarios by multiplying and dividing hydrographs. This lets us look at a range of changes. The primary needs are multiple gauges in multiple catchments (or other spatial units), and scenarios defined by different hydrographs for the same gauge.",
    "crumbs": [
      "Provided data and Causal networks",
      "Example scenarios",
      "Example scenario information and creation"
    ]
  },
  {
    "objectID": "provided_data/scenario_creation.html#process",
    "href": "provided_data/scenario_creation.html#process",
    "title": "Creating simple scenarios",
    "section": "Process",
    "text": "Process\nWe pull a limited set of gauges for a limited time period to keep this dataset small. Primarily, we identify a set of gauges in two catchments, pull them for a short time period, and adjust them to create two simple modified scenarios, with the original data serving as the baseline scenario. Along the way, we examine the data in various ways to visualise what we’re doing and where.\nA larger and more complex set of scenarios is created in the flow scaling demonstration, without as much visualisation.",
    "crumbs": [
      "Provided data and Causal networks",
      "Example scenarios",
      "Example scenario information and creation"
    ]
  },
  {
    "objectID": "provided_data/scenario_creation.html#paths-and-other-data",
    "href": "provided_data/scenario_creation.html#paths-and-other-data",
    "title": "Creating simple scenarios",
    "section": "Paths and other data",
    "text": "Paths and other data\nThe shapefiles used to see what we’re doing and do the selecting were produced with within the HydroBOT package to keep consistency. It’s possible we’ll add more shapefile creation and move all the canonical versions and their creation to their own data package or repo.\nSet the data directory to make that easy to change. These should usually point to external shared directories. For this simple example though, we put the data inside the repo to make it self contained. The flow-scaling analyses sends them externally, which would be more typical.\n\nscenario_dir &lt;- \"more_scenarios\"\nhydro_dir &lt;- file.path(scenario_dir, \"hydrographs\")",
    "crumbs": [
      "Provided data and Causal networks",
      "Example scenarios",
      "Example scenario information and creation"
    ]
  },
  {
    "objectID": "provided_data/scenario_creation.html#language-note",
    "href": "provided_data/scenario_creation.html#language-note",
    "title": "Creating simple scenarios",
    "section": "Language note",
    "text": "Language note\nThis notebook was originally built using only python, and there is an unmaintained python-only version available by contacting the authors. I’ve moved the active version of this notebook to R, however, when HydroBOT became an R package and the flow scaling analyses ended up using R gauge pullers. There is still some remaining python in here (pulling gauges and some minor EWR functions). This notebook provides an example of how to mix R and python code chunks, which we do fairly frequently.\nWe can access python objects in R with py$objectnameand access R objects in python with r.objectname .\nIt takes -forever- to do a type translation on the DATETIME column in the gauge data. It’s unclear why (can’t replicate it with any other datetime py object). We work around that by changing it to something simple while still in python, and change it back to datetime in R.",
    "crumbs": [
      "Provided data and Causal networks",
      "Example scenarios",
      "Example scenario information and creation"
    ]
  },
  {
    "objectID": "provided_data/scenario_creation.html#spatial-datasets",
    "href": "provided_data/scenario_creation.html#spatial-datasets",
    "title": "Creating simple scenarios",
    "section": "Spatial datasets",
    "text": "Spatial datasets\nWe use spatial datasets provided by {HydroBOT}, which creates a standard set in data_creation/spatial_data_creation.qmd. These are visualised in a separate notebook. Relevant to this scenario creation, we are interested in the gauges, (HydroBOT::bom_basin_gauges) since this is what were contained in the EWR tool. We use the sdl_units dataset to obtain a subset of gauges for these simple scenarios. Relevant to the case study- the original polygon used was the Macquarie-Castlereagh in the resource_plan_areas, though we seem to use sdl units elsewhere, so I’ll use them here.",
    "crumbs": [
      "Provided data and Causal networks",
      "Example scenarios",
      "Example scenario information and creation"
    ]
  },
  {
    "objectID": "provided_data/scenario_creation.html#get-relevant-gauges",
    "href": "provided_data/scenario_creation.html#get-relevant-gauges",
    "title": "Creating simple scenarios",
    "section": "Get relevant gauges",
    "text": "Get relevant gauges\nCut the bom_basin_gauges from the whole country to just those four catchments\n\ndemo_gauges &lt;- st_intersection(bom_basin_gauges, catch_demo)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nHow many are there?\n\ndemo_gauges |&gt; nrow()\n\n[1] 295\n\n\nThat’s a fair number, but they won’t all be in the EWR.\nExtract their names\nTo feed to the gauge puller, we need their gauge numbers.\n\ngaugenums &lt;- demo_gauges$gauge\n\nFind those relevant to HydroBOT\nWe have the list of gauges, but now we need to cut the list down to those in the EWR tool. There’s not any point in pulling gauges that do not appear later in HydroBOT.\nWhich gauges are actually in the EWR tool?\n\newrs_in_pyewr &lt;- get_ewr_table()\n\nWhat are those gauges, and which are in both the ewr and the desired catchments?\nThe way that works everywhere but vscode- this seems universal though\n\newrgauges &lt;- ewrs_in_pyewr$Gauge |&gt; unique()\newr_demo_gauges &lt;- gaugenums[gaugenums %in% ewrgauges]\nlength(ewr_demo_gauges)\n\n[1] 47\n\n\n47 isn’t too many.\nNeed to categorise them so we know what to pull. Let’s cut to just flow gauges for the demo for simplicity- scaling levels is less clear.\n\ngauges_to_pull &lt;- ewrs_in_pyewr |&gt;\n  filter(Gauge %in% ewr_demo_gauges & FlowLevelVolume == \"F\") |&gt;\n  dplyr::select(Gauge) |&gt;\n  unique() |&gt;\n  pull()\n\nGet all the gauge data\nNow we have a list of gauges, we need their hydrographs. We need a reasonable time span to account for temporal variation, but not too long- this is a simple case. Let’s choose 10 years.\n\nstarttime &lt;- lubridate::ymd(20000701)\nendtime &lt;- lubridate::ymd(20210630)\n\nSince some of those are flow and some are level, let’s do two calls and bind. We could pmap, but this will be fine. It’s tempting to just use flow.\n\nif (params$REBUILD_DATA) {\n  demo_levs &lt;- hydrogauge::get_ts_traces(\n  portal = \"NSW\",\n  site_list = gauges_to_pull,\n  var_list = 141, # This is flow\n  start_time = starttime,\n  end_time = endtime\n)\n}\n\nThe azure boxes have old GDAL, which can’t read WKT2. Need to fix, but in the meantime, force with the crs number.\n\nif (grepl(\"npd-dat\", Sys.info()[\"nodename\"])) {\n  st_crs(basin) &lt;- 4283\n  st_crs(catch_demo) &lt;- 4283\n  st_crs(demo_gauges) &lt;- 4283\n}\n\nMap the gauges\nLooks reasonable. Probably overkill for testing, but can do a cut down version too.\n\n(ggplot() +\n  geom_sf(data = basin, fill = \"lightsteelblue\") +\n  geom_sf(data = catch_demo, mapping = aes(fill = SWSDLName)) +\n  geom_sf(data = demo_gauges, color = \"black\") +\n  scale_fill_brewer(type = \"qual\", palette = 8))",
    "crumbs": [
      "Provided data and Causal networks",
      "Example scenarios",
      "Example scenario information and creation"
    ]
  },
  {
    "objectID": "aggregator/read_and_agg.html",
    "href": "aggregator/read_and_agg.html",
    "title": "Read_and_agg in detail",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(dplyr)\nlibrary(ggplot2)",
    "crumbs": [
      "Aggregator",
      "Main use",
      "Using with read and agg (primary)"
    ]
  },
  {
    "objectID": "aggregator/read_and_agg.html#overview",
    "href": "aggregator/read_and_agg.html#overview",
    "title": "Read_and_agg in detail",
    "section": "Overview",
    "text": "Overview\nThe most common way to run the Aggregator is with read_and_agg(), which automates data read-in, processing, parallelisation, metadata, and saving. This requires that the output of the response models is saved out, which is almost always the case for both recordkeeping and processing purposes. This vastly simplifies automated, consistent running over many scenarios.\nTo do the same analyses as in the multi aggregate example but using read_and_agg(), we give it the path to the data instead of the data itself. If the directory contains multiple files, read_and_agg() provides capacity to operate over those files in parallel.\n\n\n\n\n\n\nImportant\n\n\n\nSubdirectories should represent scenarios, because scenario outcomes are not interdependent (should be compared, not combined). In contrast, other dimensions (e.g. location) are interdependent. If directories are not scenarios but separate units that should be aggregated, e.g. gauges which should be aggregated to basin, parallelisation will not work and many other steps will be more difficult. See more information.\n\n\nWe can return output to the active session with returnList and use savepath to save a .rds file. In most cases, we would save outputs so developing and adjusting Comparer outputs does not rely on re-running the aggregations.\nThe read_and_agg() function saves metadata files (yaml and json) that allows replication of this step with run_hydrobot_params(), see here. These files build on metadata from earlier steps if possible, including any available metadata from the Controller about module parameters and the scenarios.",
    "crumbs": [
      "Aggregator",
      "Main use",
      "Using with read and agg (primary)"
    ]
  },
  {
    "objectID": "aggregator/read_and_agg.html#demonstration",
    "href": "aggregator/read_and_agg.html#demonstration",
    "title": "Read_and_agg in detail",
    "section": "Demonstration",
    "text": "Demonstration\nHere, we perform the same set of aggregations as the primary example for multi_aggregate, but do so from the paths to the EWR outputs and note differences. See that notebook for much more detail about how aggregation itself works.\nDirectories\nFirst, we need to provide a set of paths to point to the input data, in this case the outputs from the EWR tool for the small demonstration, created by a controller notebook.\nNote that we specify a path here for the aggregator results.\n\nproject_dir &lt;- \"hydrobot_scenarios\"\nhydro_dir &lt;- file.path(project_dir, 'hydrographs')\newr_results &lt;- file.path(project_dir, \"module_output\", \"EWR\")\nagg_results &lt;- file.path(project_dir, \"aggregator_output\", \"demo\")\n\nWe can see that those outputs (the csvs) are in scenario-based subdirectories, with the yaml and json metadata for the Controller in the outer directory.\n\nlist.files(ewr_results, recursive = TRUE)\n\n[1] \"base/yearly_base.csv\"   \"down4/yearly_down4.csv\" \"ewr_metadata.json\"     \n[4] \"ewr_metadata.yml\"       \"up4/yearly_up4.csv\"    \n\n\nScenario information\nThis will be attached to metadata, typically. For this demonstration, we just use it for plot clarity and the data is simple.\n\nmultipliers &lt;- c(1.1, 1.5, 2, 3, 4)\n\nscenemults &lt;- c(1 / rev(multipliers), 1, multipliers)\n\nscenenames &lt;- c(\n  paste0(\"down\", as.character(rev(multipliers))),\n  \"base\",\n  paste0(\"up\", as.character(multipliers))\n) |&gt;\n  stringr::str_replace(\"\\\\.\", \"_\")\n\n\nscenarios &lt;- tibble::tibble(scenario = scenenames, delta = scenemults)\n\nscene_pal &lt;- make_pal(unique(scenarios$scenario), palette = \"ggsci::nrc_npg\", refvals = \"base\", refcols = \"black\")\n\nAggregation sequences\nWe use the same examples as in multi_aggregate. These cover all three dimensions. It begins temporal (all_time), then has two theme aggregations (ewr_code and env_obj), then spatial to sdl_units, two more theme-dimension (Specific_goal, Objective), a spatially-weighted aggregation to the basin, and finally to the theme level of 5-year management targets.\n\naggseq &lt;- list(\n  all_time = 'all_time',\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  env_obj = c(\"ewr_code\", \"env_obj\"),\n  sdl_units = sdl_units,\n  Specific_goal = c(\"env_obj\", \"Specific_goal\"),\n  Objective = c(\"Specific_goal\", \"Objective\"),\n  basin = basin,\n  target_5_year_2024 = c(\"Objective\", \"target_5_year_2024\")\n)\n\nfunseq &lt;- list(\n  all_time = 'ArithmeticMean',\n  ewr_code = \"CompensatingFactor\",\n  env_obj = \"ArithmeticMean\",\n  sdl_units = \"ArithmeticMean\",\n  Specific_goal = \"ArithmeticMean\",\n  Objective = \"ArithmeticMean\",\n  basin = 'SpatialWeightedMean',\n  target_5_year_2024 = \"ArithmeticMean\"\n  )\n\nDo the aggregation\nNow, we use read_and_agg() to read the data in, aggregate it, and save it out. We also return it here for making some simple example plots. Rather than using auto_ewr_PU = TRUE, we use group_until and pseudo_spatial as defined in more detail elsewhere.\n\nagged_dat &lt;- read_and_agg(\n  datpath = ewr_results,\n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  group_until = list(\n    SWSDLName = is_notpoint,\n    planning_unit_name = is_notpoint,\n    gauge = is_notpoint\n  ),\n  pseudo_spatial = \"sdl_units\",\n  aggsequence = aggseq,\n  funsequence = funseq,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = FALSE,\n  returnList = TRUE,\n  savepath = agg_results,\n  add_max = FALSE\n)\n\n! Unmatched links in causal network\n• 29 from ewr_code_timing to ewr_code\n! Unmatched links in causal network\n• 10 from env_obj to Specific_goal\n! Unmatched links in causal network\n• 7 from Objective to target_5_year_2024\n\n\nThat has the same information as the example in multi_aggregate, with 9 levels of aggregation:\n\nnames(agged_dat)\n\n[1] \"agg_input\"          \"all_time\"           \"ewr_code\"          \n[4] \"env_obj\"            \"sdl_units\"          \"Specific_goal\"     \n[7] \"Objective\"          \"basin\"              \"target_5_year_2024\"\n\n\nWe will only show one example sheet here.\n\nagged_dat$sdl_units |&gt;\n  dplyr::filter(env_obj %in% c(\"EF1\", \"WB1\", \"NF1\")) |&gt;\n  dplyr::left_join(scenarios) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    sceneorder = c(\"down4\", \"base\", \"up4\"),\n    underlay_list = list(\n      underlay = sdl_units,\n      underlay_pal = \"grey90\"\n    )\n  )",
    "crumbs": [
      "Aggregator",
      "Main use",
      "Using with read and agg (primary)"
    ]
  },
  {
    "objectID": "aggregator/read_and_agg.html#metadata",
    "href": "aggregator/read_and_agg.html#metadata",
    "title": "Read_and_agg in detail",
    "section": "Metadata",
    "text": "Metadata\nA key advantage of read_and_agg() is that it records metadata for data provenance (which is also runnable).\nFor the run here, that yaml is at \"hydrobot_scenarios/aggregator_output/demo/agg_metadata.yml\", and contains the following information:\n\nscenarios:\n  scenario_name:\n  - down4\n  - base\n  - up4\n  flow_multiplier:\n  - 0.25\n  - 1.0\n  - 4.0\newr:\n  output_parent_dir: hydrobot_scenarios\n  hydro_dir: hydrobot_scenarios/hydrographs\n  output_path: hydrobot_scenarios/module_output/EWR\n  model_format: Standard time-series\n  outputType: yearly\n  returnType: none\n  finish_time: 2025-04-04 15:44:05 AEDT\n  status: yes\n  ewr_version: py-ewr 2.3.7\n  HydroBOT_version: 0.2.2.9020\naggregation:\n  datpath: hydrobot_scenarios/module_output/EWR\n  type: achievement\n  groupers: scenario\n  group_until:\n    SWSDLName: 4\n    planning_unit_name: 4\n    gauge: 4\n  pseudo_spatial: sdl_units\n  aggCols: ewr_achieved\n  aggsequence:\n    all_time: all_time\n    ewr_code:\n    - ewr_code_timing\n    - ewr_code\n    env_obj:\n    - ewr_code\n    - env_obj\n    sdl_units: sdl_units\n    Specific_goal:\n    - env_obj\n    - Specific_goal\n    Objective:\n    - Specific_goal\n    - Objective\n    basin: basin\n    target_5_year_2024:\n    - Objective\n    - target_5_year_2024\n  funsequence:\n    all_time: ArithmeticMean\n    ewr_code: CompensatingFactor\n    env_obj: ArithmeticMean\n    sdl_units: ArithmeticMean\n    Specific_goal: ArithmeticMean\n    Objective: ArithmeticMean\n    basin: SpatialWeightedMean\n    target_5_year_2024: ArithmeticMean\n  namehistory: no\n  keepAllPolys: no\n  auto_ewr_PU: no\n  returnList: yes\n  finish_time: 2025-04-04 15:44:11 AEDT\n  status: yes\n  HydroBOT_version: 0.2.2.9020",
    "crumbs": [
      "Aggregator",
      "Main use",
      "Using with read and agg (primary)"
    ]
  },
  {
    "objectID": "aggregator/read_and_agg.html#parallelization",
    "href": "aggregator/read_and_agg.html#parallelization",
    "title": "Read_and_agg in detail",
    "section": "Parallelization",
    "text": "Parallelization\nSince aggregation should happen along the theme, space, and time dimensions, but not scenarios, we can process in parallel over scenarios. The read_and_agg() function provides this parallelisation internally and seamlessly, provided the user has the suggested package {furrr} (and its dependency, {future}). In that case, parallelising is as easy as setting a future::plan and the argument rparallel = TRUE. The exact same run as above can be done in parallel as follows:\n\nfuture::plan(future::multisession)\n\nagged_dat_p &lt;- read_and_agg(\n  datpath = ewr_results,\n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  group_until = list(\n    SWSDLName = is_notpoint,\n    planning_unit_name = is_notpoint,\n    gauge = is_notpoint\n  ),\n  pseudo_spatial = \"sdl_units\",\n  aggsequence = aggseq,\n  funsequence = funseq,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = FALSE,\n  returnList = TRUE,\n  savepath = agg_results,\n  add_max = FALSE,\n  rparallel = TRUE\n)\n\nThat output is the same as earlier, but now it’s been read-in and processed in parallel over scenarios. This toy example is only marginally faster, but parallelisation yields large speedups for larger jobs. Because scenarios run independently, massive parallelisation is possible, up to one scenario per core. Speedups can be very large, even on local machines, but are particularly useful on HPCs.",
    "crumbs": [
      "Aggregator",
      "Main use",
      "Using with read and agg (primary)"
    ]
  },
  {
    "objectID": "aggregator/read_and_agg.html#user-provided-components",
    "href": "aggregator/read_and_agg.html#user-provided-components",
    "title": "Read_and_agg in detail",
    "section": "User-provided components",
    "text": "User-provided components\nUsers can provide their own causal networks, spatial information, and aggregation functions. Moreover, aggregation can happen on arbitrary input data, possibly from unincorporated modules. See those pages for more detail using read_and_agg(), along with doing the same in multi_aggregate().",
    "crumbs": [
      "Aggregator",
      "Main use",
      "Using with read and agg (primary)"
    ]
  },
  {
    "objectID": "aggregator/read_and_agg.html#sub-directories-and-multiple-aggsequences",
    "href": "aggregator/read_and_agg.html#sub-directories-and-multiple-aggsequences",
    "title": "Read_and_agg in detail",
    "section": "Sub-directories and multiple aggsequences",
    "text": "Sub-directories and multiple aggsequences\nSometimes we might want multiple aggregation sequences to address different parts of a question, or to compare the impact of aggregation itself. Doing that is as simple as creating the multiple aggregation sequences, and typically saving them to a different directory, from which they can be read for the comparer.\nFor example, if we wanted to aggregate to Targets (Native fish, Waterbirds, etc) instead of the Objectives and long-run targets for the same EWR output as above, and also use different aggregation functions, we could set up new sequences. Here, we also call the spatial data as characters and define our own Median function, as explained in more detail here and here.\n\naggseq_new &lt;- list(\n  all_time = 'all_time',\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  env_obj = c(\"ewr_code\", \"env_obj\"),\n  sdl_units = \"sdl_units\",\n  Target = c(\"env_obj\", \"Target\"),\n  basin = \"basin\"\n)\n\nMedian &lt;- function(x) {\n  median(x, na.rm = TRUE)\n}\n\nfunseq_new &lt;- list(\n  all_time = 'ArithmeticMean',\n  ewr_code = \"LimitingFactor\",\n  env_obj = \"Median\",\n  sdl_units = \"Max\",\n  Target = \"Min\",\n  basin = 'SpatialWeightedMean'\n  )\n\nThen, to avoid overwriting the earlier version, since in this example we would want to look at both sets of aggregations, we change the savepath argument (along with aggsequence and funsequence:\n\nagged_dat_new &lt;- read_and_agg(\n  datpath = ewr_results,\n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  group_until = list(\n    SWSDLName = is_notpoint,\n    planning_unit_name = is_notpoint,\n    gauge = is_notpoint\n  ),\n  pseudo_spatial = \"sdl_units\",\n  aggsequence = aggseq_new,\n  funsequence = funseq_new,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = FALSE,\n  returnList = TRUE,\n  savepath = file.path(project_dir, 'aggregator_output', 'second'),\n  add_max = FALSE\n)\n\n! Unmatched links in causal network\n• 29 from ewr_code_timing to ewr_code",
    "crumbs": [
      "Aggregator",
      "Main use",
      "Using with read and agg (primary)"
    ]
  },
  {
    "objectID": "aggregator/read_and_agg.html#value-to-aggregate",
    "href": "aggregator/read_and_agg.html#value-to-aggregate",
    "title": "Read_and_agg in detail",
    "section": "Value to aggregate",
    "text": "Value to aggregate\nWe have demonstrated everything here by aggregating EWR data with the ewr_achieved column. However, any numeric column can be aggregated, as we show for made up module outputs. Here, we also show aggregation on a different EWR metric, the achivement of interevent requirements:\n\nagged_interevent &lt;- read_and_agg(\n  datpath = ewr_results,\n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"interevent_achieved\",\n  group_until = list(\n    SWSDLName = is_notpoint,\n    planning_unit_name = is_notpoint,\n    gauge = is_notpoint\n  ),\n  pseudo_spatial = \"sdl_units\",\n  aggsequence = aggseq,\n  funsequence = funseq,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = FALSE,\n  returnList = TRUE,\n  savepath = agg_results,\n  add_max = FALSE\n)\n\n! Unmatched links in causal network\n• 29 from ewr_code_timing to ewr_code\n! Unmatched links in causal network\n• 10 from env_obj to Specific_goal\n! Unmatched links in causal network\n• 7 from Objective to target_5_year_2024\n\n\nThat has the same sheets as above\n\nnames(agged_interevent)\n\n[1] \"agg_input\"          \"all_time\"           \"ewr_code\"          \n[4] \"env_obj\"            \"sdl_units\"          \"Specific_goal\"     \n[7] \"Objective\"          \"basin\"              \"target_5_year_2024\"\n\n\nAnd the map is similar but not identical (interevents are closely related to the frequency, so this is not surprising).\n\nagged_interevent$sdl_units |&gt;\n  dplyr::filter(env_obj %in% c(\"EF1\", \"WB1\", \"NF1\")) |&gt;\n  dplyr::left_join(scenarios) |&gt;\n  plot_outcomes(\n    outcome_col = \"interevent_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"interevent_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"scenario\",\n    facet_row = \"env_obj\",\n    sceneorder = c(\"down4\", \"base\", \"up4\"),\n    underlay_list = list(\n      underlay = sdl_units,\n      underlay_pal = \"grey90\"\n    )\n  )",
    "crumbs": [
      "Aggregator",
      "Main use",
      "Using with read and agg (primary)"
    ]
  },
  {
    "objectID": "aggregator/read_and_agg.html#gauge-and-scenario-filtering",
    "href": "aggregator/read_and_agg.html#gauge-and-scenario-filtering",
    "title": "Read_and_agg in detail",
    "section": "Gauge and scenario filtering",
    "text": "Gauge and scenario filtering\nWe usually want to apply analyses to the full set of scenarios or gauges. However, sometimes we might only want a subset. Using the gaugefilter and scenariofilter arguments can provide this functionality. Howevern this is a bit dangerous because they are simple regex on the filenames, and so are not actually specific to gauges or scenarios and are only named that way to make the regex simpler if multiple matches are desired. Moreover, if the files do not have a pattern, e.g. there are not filenames with gauge numbers in them and you try to send those numbers in gaugefilter, it will error. Since these examples do not have unique gauge files, we only use scenariofilter.\n\nsmallreadagg &lt;- read_and_agg(\n  datpath = ewr_results, \n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = causal_ewr,\n  groupers = c(\"scenario\", \"gauge\"),\n  aggCols = \"ewr_achieved\",\n  aggsequence = aggseq,\n  funsequence = funseq,\n  auto_ewr_PU = TRUE,\n    add_max = FALSE,\n  namehistory = FALSE,\n  gaugefilter = NULL,\n  scenariofilter = \"base\"\n)\n\nWarning: Causal network does not have all groupers.\n• Joining env_obj to Specific_goal\n• Groupers are scenario, gauge, polyID.\n• expect causal network to have gauge; it has planning_unit_name, SWSDLName, env_obj, Specific_goal, fromtype, totype, edgeorder.\n• Do you need to use `group_until`? Or is your network missing columns?\n\n\nWarning: Causal network does not have all groupers.\n• Joining Specific_goal to Objective\n• Groupers are scenario, gauge, polyID.\n• expect causal network to have gauge; it has planning_unit_name, SWSDLName, Specific_goal, Objective, fromtype, totype, edgeorder.\n• Do you need to use `group_until`? Or is your network missing columns?\n\n\nWarning: Causal network does not have all groupers.\n• Joining Objective to target_5_year_2024\n• Groupers are scenario, gauge, polyID.\n• expect causal network to have gauge; it has Objective, target_5_year_2024, fromtype, totype, edgeorder.\n• Do you need to use `group_until`? Or is your network missing columns?\n\ntable(smallreadagg$gauge, smallreadagg$scenario)\n\n        \n         base\n  412002   51\n  412005   39\n  421001   49\n  421004   72\n  421011   70",
    "crumbs": [
      "Aggregator",
      "Main use",
      "Using with read and agg (primary)"
    ]
  },
  {
    "objectID": "aggregator/theme_agg.html",
    "href": "aggregator/theme_agg.html",
    "title": "Theme aggregation",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(dplyr)",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Theme aggregation capability"
    ]
  },
  {
    "objectID": "aggregator/theme_agg.html#data",
    "href": "aggregator/theme_agg.html#data",
    "title": "Theme aggregation",
    "section": "Data",
    "text": "Data\nInput data to should be a dataframe (e.g. a dataframe of EWR outputs, sf object if they are spatial outcomes). If we want to pass a path instead of a dataframe (as we might for large runs), we would use read_and_agg, which wraps multi_aggregate, demonstrated in its own notebook. Thus, for the demonstration, we pull in the the EWR output produced from the HydroBOT-provided hydrographs (system.file('extdata/testsmall/hydrographs', package = 'HydroBOT'), which we have processed already here and are at the paths above.\nWe’ll pull in the data to use for demonstration so we can use theme_aggregate() and multi_aggregate() directly. If we want to feed a path instead of a dataframe, we would use read_and_agg().\nThe data comes in as a timeseries, so we do one initial level of temporal aggregation (the mean over the series) to make visualisation easier.\n\newr_out &lt;- prep_run_save_ewrs(\n  hydro_dir = hydro_dir,\n  output_parent_dir = project_dir,\n  outputType = list('none'),\n  returnType = list('yearly')\n)\n\n\n# This is just a simple prep step that is usually done internally to put the geographic coordinates on input data\newrdata &lt;- prep_ewr_output(ewr_out$yearly, type = 'achievement', add_max = FALSE)\n\n# This gets us to env_obj at the gauge over all time\npreseq &lt;- list(\n  all_time = \"all_time\"\n  )\n\nfunseq &lt;- list(\n  'ArithmeticMean'\n)\n\n# Do the aggregation to get output at each gauge averaged over time\nsimpleAgg &lt;- multi_aggregate(\n  dat = ewrdata,\n  causal_edges = causal_ewr,\n  groupers = c(\"scenario\", \"gauge\", \"ewr_code_timing\"),\n  aggCols = \"ewr_achieved\",\n  aggsequence = preseq,\n  funsequence = funseq\n)\n\nWarning: ! EWR outputs detected without `group_until`!\nℹ EWR outputs should be grouped by `SWSDLName`, `planning_unit_name`, and `gauge` until aggregated to larger spatial areas.\nℹ Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`.\nℹ Lower-level processing should include as `grouper` in `temporal_aggregate()`\n\nsimpleAgg\n\n\n  \n\n\n\nThis provides a spatially-referenced (to gauge) temporally-aggregated tibble to use to demonstrate theme aggregation. Note that this has the initial theme level of the EWR outputs (ewr_code_timing), but also two groupings that we want to preserve when we aggregate along the theme dimension- scenario and the current level of spatial grouping, the gauge locations. We have dropped the time by taking the temporal average in step one, but that would be preserved as well if present. In typical use with the EWR tool, the planning_unit_name and SWSDLName columns should be preserved for pseudo-spatial aggregation.\nWe’ll choose an example gauge to make it easier to visualise the data.\n\n# Dubbo is '421001', has 24 EWRs\n# Warren Weir is '421004', has 30 EWRs.\nexample_gauge &lt;- \"421001\"",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Theme aggregation capability"
    ]
  },
  {
    "objectID": "aggregator/theme_agg.html#causal-network",
    "href": "aggregator/theme_agg.html#causal-network",
    "title": "Theme aggregation",
    "section": "Causal network",
    "text": "Causal network\nTheme dimension aggregation must use links defined in causal_ewrs (or other causal mappings) from the ‘from’ and ‘to’ levels at each step (the from_theme and to_theme arguments, which are characters for the column names in the causal network). In other words, we can’t scale between levels with no defined relationship. However, this does not mean every level must be included, and indeed they usually are not. If a relationship exists in the causal network dataframe(s), levels can be jumped, e.g. we could go straight from env_obj to target_20_year_2039 using the links defined in causal_ewr$obj2yrtarget without connecting to Target or Objective as intermediate steps. There may not be a defined ordering of some levels, and so it is perfectly reasonable to go from env_obj to both Objective and Target, depending on the question. Here, we use the causal_ewr list of relationships provided with HydroBOT, but other lists could be supplied.",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Theme aggregation capability"
    ]
  },
  {
    "objectID": "aggregator/theme_agg.html#single-aggregation",
    "href": "aggregator/theme_agg.html#single-aggregation",
    "title": "Theme aggregation",
    "section": "Single aggregation",
    "text": "Single aggregation\nWe might just want to perform theme aggregation once. This is rare in practice, but is what multi_aggregate() uses internally for the theme steps, so is worth understanding. We can do this simply by passing the input data (in this case simpleAgg), a causal network, and providing a funlist. In this simple case, we just use a single character function name, here the custom 'ArithmeticMean', which is just a simple wrapper of mean with na.rm = TRUE. Any function can be passed this way, custom or in-built, provided it has a single argument. More complex situations are given below, and different syntax is possible.\n\n\n\n\n\n\nNote\n\n\n\nThe funlist argument here specifies the function(s) to use at a single step. It is thus not the same as the funsequence list of multi_aggregate(); instead being a single item in that list, though it may include multiple functions (e.g. the mean and max).\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe aggCols argument is ends_with(original_name) to reference the original name of the column of values- it may have a long name tracking its aggregation history, so we give it the tidyselect::ends_with to find the column. More generally, both aggCols and groupers can take any tidyselect syntax or bare names or characters, see here.\n\n\nWe demonstrate here by aggregating from the output of the EWR tool (at the ‘ewr_code_timing’ level) to the ‘env_obj’ level, which defines parts of lifecycles or components of larger Targets.\nThe theme_aggregate() function handles a single aggregation step along the theme dimension. For each step in the aggregation, we need to specify what levels we are aggregating from and to, the function to use to aggregate, and the mapping between the ‘from’ and ‘to’ levels.\nThe funlist argument works as in the spatial examples, and can be characters, bare names, or anonymous functions, with some limits. Perhaps most importantly, it can be a list of more than one function if we want to, for example, calculate the mean and maximum. Note that in theme_aggregate() itself, this list is for a single step, and would be one item in the funsequence argument to multi_aggregate().\n\ntiming_obj &lt;- theme_aggregate(\n  dat = simpleAgg,\n  from_theme = 'ewr_code_timing',\n  to_theme = 'env_obj',\n  # 'gauge' is not strictly necessary because this is an sf, but including it retains that column\n  groupers = c(\"scenario\", 'gauge'),\n  aggCols = tidyselect::ends_with(\"ewr_achieved\"),\n  funlist = 'ArithmeticMean',\n  causal_edges = causal_ewr\n)\n\nWarning: ! EWR outputs detected without `group_until`!\nℹ EWR outputs should be grouped by `SWSDLName`, `planning_unit_name` and `gauge` until aggregated to larger spatial areas.\nℹ Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`.\nℹ Lower-level processing should include as `grouper` in `theme_aggregate()`\n\ntiming_obj\n\n\n  \n\n\n\nBecause we are using theme_aggregate directly, the dimensional safety provided by multi_aggregate() is not present. However, because this is an sf object, the geometry column retains the spatial information. This should not be relied on, and would not happen if theme-aggregating a nonspatial dataset. The multi_aggregate function automatically handles this preservation, but theme_aggregate is more general, and does not make any assumptions about the grouping structure of the data. Thus, to keep spatial or temporal groupings (as we should, otherwise we can inadvertently aggregate over all of them simultaneously), we should add polyID or geometry, along with any time columns if present, to the groupers argument.\nThe resulting column name is cumbersome, but provide a record of exactly what the aggregation sequence was.\n\nnames(timing_obj)\n\n[1] \"scenario\"                                                   \n[2] \"gauge\"                                                      \n[3] \"polyID\"                                                     \n[4] \"env_obj\"                                                    \n[5] \"env_obj_ArithmeticMean_all_time_ArithmeticMean_ewr_achieved\"\n[6] \"geometry\"                                                   \n\n\nWe can clean those up into columns with agg_names_to_cols() (which happens internally in multi_aggregate() and read_and_agg() with namehistory = FALSE).\n\nto_rename &lt;- agg_names_to_cols(timing_obj, \n                              aggsequence = c(names(preseq), 'env_obj'), \n                              funsequence = c(funseq, 'ArithmeticMean'), \n                              aggCols = 'ewr_achieved')\n\nto_rename\n\n\n  \n\n\n\nA quick plot shows the outcome. For more plotting details, see the plotting section. We’ll simplify the names and choose a subset of the environmental objectives.\n\nenv_pals &lt;- list(EB = 'grDevices::Grays',\n  EF = 'grDevices::Purp',\n                NF = 'grDevices::Mint',\n                NV = 'grDevices::Burg',\n                OS = 'grDevices::Blues 2',\n                WB = 'grDevices::Peach')\n\n\nto_rename |&gt;\n  dplyr::mutate(env_group = stringr::str_extract(env_obj, '^[A-Z]+')) |&gt;\n  dplyr::filter(env_group != 'EB') |&gt; \n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = 'scenario',\n    y_lab = \"Arithmetic Mean\",\n    colorgroups = 'env_group',\n    colorset = \"env_obj\",\n    pal_list = env_pals,\n    # facet_col = \"env_obj\",\n    facet_row = \"gauge\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n  )",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Theme aggregation capability"
    ]
  },
  {
    "objectID": "aggregator/theme_agg.html#multiple-aggregation-functions",
    "href": "aggregator/theme_agg.html#multiple-aggregation-functions",
    "title": "Theme aggregation",
    "section": "Multiple aggregation functions",
    "text": "Multiple aggregation functions\nIf we give funlist more than one aggregation function, it calculates both. Here, we use the mean and minimum.\n\ntiming_obj_2 &lt;- theme_aggregate(\n  dat = simpleAgg,\n  from_theme = 'ewr_code_timing',\n  to_theme = 'env_obj',\n  groupers = c(\"scenario\", 'gauge'),\n  aggCols = tidyselect::ends_with(\"ewr_achieved\"),\n  funlist = c('ArithmeticMean', 'Min'),\n  causal_edges = causal_ewr\n)\n\nWarning: ! EWR outputs detected without `group_until`!\nℹ EWR outputs should be grouped by `SWSDLName`, `planning_unit_name` and `gauge` until aggregated to larger spatial areas.\nℹ Best to explicitly use `group_until` in `multi_aggregate()` or `read_and_agg()`.\nℹ Lower-level processing should include as `grouper` in `theme_aggregate()`\n\ntiming_obj_2 &lt;- agg_names_to_cols(timing_obj_2,\n                              aggsequence = c(names(preseq), 'env_obj'),\n                              funsequence = list(all_time = funseq,\n                                                 env_obj = list('ArithmeticMean', 'Min')),\n                              aggCols = 'ewr_achieved')\n\nWarning in funsequence[!fs] &lt;- names(unlist(funsequence[!fs])): number of items\nto replace is not a multiple of replacement length\n\n\nAnd now we can compare what we get out of the different functions\n\ntiming_obj_2 |&gt;\n  dplyr::mutate(env_group = stringr::str_extract(env_obj, '^[A-Z]+')) |&gt;\n  filter(polyID != \"r1zp2f5py7d\") |&gt; \n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = 'aggfun_2',\n    y_lab = \"Arithmetic Mean\",\n    colorgroups = 'env_group',\n    colorset = \"env_obj\",\n    pal_list = env_pals,\n    facet_col = \"scenario\",\n    facet_row = \"gauge\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n  )",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Theme aggregation capability"
    ]
  },
  {
    "objectID": "aggregator/theme_agg.html#multiple-theme-levels",
    "href": "aggregator/theme_agg.html#multiple-theme-levels",
    "title": "Theme aggregation",
    "section": "Multiple theme levels",
    "text": "Multiple theme levels\nIn general, multi_aggregate() can be used across theme, space, and temporal dimensions. But here, it is particularly helpful to allow multi-step aggregation along the theme dimension given the importance of the causal network for theme aggregation. To do that, we use the aggregations above with savehistory = TRUE and namehistory = FALSE.\nTo create the aggregation, we provide the sequence lists created above, along with the causal links, defined by the causal_edges argument. Because the make_edges() function also takes a sequence of node types, we can usually just call make_edges() on the list of relationships and the desired set of theme levels. We can also just pass in causal_edges = causal_ewr (the list with all possible links), and theme_aggregate() will auto-generate the edges it needs. That’s just a bit less efficient (the edges get generated each step instead of once).\n\naggseq &lt;- list(\n  all_time = 'all_time',\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  env_obj = c(\"ewr_code\", \"env_obj\"),\n  Specific_goal = c(\"env_obj\", \"Specific_goal\"),\n  Objective = c(\"Specific_goal\", \"Objective\"),\n  target_5_year_2024 = c(\"Objective\", \"target_5_year_2024\")\n)\n\nfunseq &lt;- list(\n  all_time = 'ArithmeticMean',\n  ewr_code = c(\"CompensatingFactor\"),\n  env_obj = c(\"ArithmeticMean\", \"LimitingFactor\"),\n  Specific_goal = c(\"ArithmeticMean\", \"LimitingFactor\"),\n  Objective = c(\"ArithmeticMean\"),\n  target_5_year_2024 = c(\"ArithmeticMean\")\n)\n\ntheme_steps &lt;- multi_aggregate(\n  dat = ewrdata,\n  causal_edges = causal_ewr,\n  groupers = c(\"scenario\", 'gauge'),\n  aggCols = \"ewr_achieved\",\n  aggsequence = aggseq,\n  funsequence = funseq,\n  namehistory = FALSE,\n  saveintermediate = TRUE,\n  auto_ewr_PU = TRUE # avoid warnings and errors\n) \n\nWarning: Causal network does not have all groupers.\n• Joining env_obj to Specific_goal\n• Groupers are scenario, gauge, polyID, planning_unit_name, SWSDLName.\n• expect causal network to have gauge, planning_unit_name, SWSDLName; it has planning_unit_name, SWSDLName, env_obj, Specific_goal, fromtype, totype, edgeorder.\n• Do you need to use `group_until`? Or is your network missing columns?\n\n\nWarning: Causal network does not have all groupers.\n• Joining Specific_goal to Objective\n• Groupers are scenario, gauge, polyID, planning_unit_name, SWSDLName.\n• expect causal network to have gauge, planning_unit_name, SWSDLName; it has planning_unit_name, SWSDLName, Specific_goal, Objective, fromtype, totype, edgeorder.\n• Do you need to use `group_until`? Or is your network missing columns?\n\n\nWarning: Causal network does not have all groupers.\n• Joining Objective to target_5_year_2024\n• Groupers are scenario, gauge, polyID.\n• expect causal network to have gauge; it has Objective, target_5_year_2024, fromtype, totype, edgeorder.\n• Do you need to use `group_until`? Or is your network missing columns?\n\n\nCausal plot\nBy returning values at each stage, we can map those to colour in a causal network. Here, we map the values of the aggregation to node color. To do this, we follow the make_causal_plot() approach of making edges and nodes, and then use a join to attach the value to each node.\nTo keep this demonstration from becoming too unwieldy, we limit the edge creation to a single gauge, and so filter the theme aggregations accordingly (or just rely on the join to drop).\nThe first step is to generate the edges and nodes for the network we want to look at.\n\nedges &lt;- make_edges(causal_ewr,\n  fromtos = aggseq[2:length(aggseq)],\n  gaugefilter = example_gauge\n)\n\nnodes &lt;- make_nodes(edges)\n\nNow, extract the values we want from the aggregation and join them to the nodes.\n\n# need to grab the right set of aggregations if there are multiple at some stages\nwhichaggs &lt;- c(\n  \"ArithmeticMean\",\n  \"CompensatingFactor\",\n  \"ArithmeticMean\",\n  \"ArithmeticMean\",\n  \"ArithmeticMean\",\n  \"ArithmeticMean\"\n)\n\n# What is the column that defines the value?\nvalcol &lt;- \"ewr_achieved\"\n\n# Get the values for each node\naggvals &lt;- extract_vals_causal(theme_steps,\n                               whichaggs = whichaggs,\n                               valcol = 'ewr_achieved',\n                               targetlevels = names(aggseq)[2:6]) # don't use the first one, it's time at _timing \n\naggvals &lt;- aggvals |&gt; \n  # the NA gauges are for levels past gauge definitions\n  filter(gauge == example_gauge | is.na(gauge)) |&gt; \n  st_drop_geometry()\n\n# join to the nodes\nnodes_with_vals &lt;- dplyr::left_join(nodes, aggvals)\n\nNow we can make the causal network plot with the nodes we chose and colour them by the values we’ve just attached to them from the aggregation. At present, it is easiest to make separate plots per scenario or other grouping ( Figure 1 , Figure 2 ). For example, in the increased watering scenario, we see more light colours, and so better performance across the range of outcomes. Further network outputs are provided in the Comparer.\n\naggNetwork_base &lt;- make_causal_plot(\n  nodes = dplyr::filter(\n    nodes_with_vals,\n    scenario == \"base\"\n  ),\n  edges = edges,\n  edge_pal = \"black\",\n  node_pal = list(value = \"scico::tokyo\"),\n  node_colorset = \"ewr_achieved\",\n  render = FALSE\n)\n\nDiagrammeR::render_graph(aggNetwork_base)\n\n\n\n\n\n\nFigure 1: Causal network for baseline scenario at example gauge, coloured by proportion passing at each node, e.g. Arithmetic Means at every step. Light yellow is 1, dark purple is 0.\n\n\n\n\naggNetwork_4 &lt;- make_causal_plot(\n  nodes = dplyr::filter(\n    nodes_with_vals,\n    scenario == \"up4\"\n  ),\n  edges = edges,\n  edge_pal = \"black\",\n  node_pal = list(value = \"scico::tokyo\"),\n  node_colorset = \"ewr_achieved\",\n  render = FALSE\n)\n\nDiagrammeR::render_graph(aggNetwork_4)\n\n\n\n\n\n\nFigure 2: Causal network for 4x scenario at example gauge, coloured by proportion passing at each node, e.g. Arithmetic Means at every step. Light yellow is 1, dark purple is 0.",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Theme aggregation capability"
    ]
  },
  {
    "objectID": "comparer/line_plots.html",
    "href": "comparer/line_plots.html",
    "title": "Line plots (quantitative x)",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Line plots (quant x)"
    ]
  },
  {
    "objectID": "comparer/line_plots.html#read-in-the-data",
    "href": "comparer/line_plots.html#read-in-the-data",
    "title": "Line plots (quantitative x)",
    "section": "Read in the data",
    "text": "Read in the data\nWe read in the example data we will use for all plots.\n\nagged_data &lt;- readRDS(file.path(agg_dir, \"achievement_aggregated.rds\"))\n\nThat has all the steps in the aggregation, but most of the plots here will only use a subset to demonstrate.\nTo make visualisation easier, the SDL units data is given a grouping column that puts the many env_obj variables in groups defined by their first two letters, e.g. EF for Ecosystem Function. These correspond to the ‘Target’ level, but it can be useful to have the two groupings together for some examples.\nIf we had used multiple aggregation functions at any step, we should filter down to the one we want here, but we only used one for this example.\nFor simplicity here, we will only look at a small selection of the scenarios (multiplicative changes of 0.5,1, and 2). Thus, we make two small dataframes for our primary examples here.\n\nscenarios_to_plot &lt;- c('climatedown2adapt0', 'climatebaseadapt0', 'climateup2adapt0')\n\nscenarios &lt;- yaml::read_yaml(file.path(hydro_dir, 'scenario_metadata.yml')) |&gt; \n  tibble::as_tibble()\n\nbasin_to_plot &lt;- agged_data$mdb |&gt; \n  dplyr::filter(scenario %in% scenarios_to_plot) |&gt; \n  dplyr::left_join(scenarios, by = 'scenario')\n\n# Create a grouping variable\nobj_sdl_to_plot &lt;- agged_data$sdl_units |&gt;\n    dplyr::filter(scenario %in% scenarios_to_plot) |&gt; \n  dplyr::mutate(env_group = stringr::str_extract(env_obj, \"^[A-Z]+\")) |&gt;\n  dplyr::arrange(env_group, env_obj) |&gt; \n  dplyr::left_join(scenarios, by = 'scenario')",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Line plots (quant x)"
    ]
  },
  {
    "objectID": "comparer/line_plots.html#lines-through-all-data",
    "href": "comparer/line_plots.html#lines-through-all-data",
    "title": "Line plots (quantitative x)",
    "section": "Lines through all data",
    "text": "Lines through all data\nA simple plot would be to look at all the outcomes, separated by color (though we ignore the added water scenarios). Even this simple plot is quite informative- we can see that the env_obj outcomes are differently sensitive to both decreases and increases in flow, and that this differs across space.\n\nsdl_line &lt;- obj_sdl_to_plot |&gt;\n  filter(adapt_code == 1) |&gt; \n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = \"flow_multiplier\",\n    colorgroups = NULL,\n    colorset = \"env_obj\",\n    pal_list = list(\"scico::berlin\"),\n    facet_row = \"SWSDLName\"\n  )\n\nsdl_line\n\n\n\n\n\n\n\nSeparate values within groups\nWe might not care so much about individual outcomes, but about their groupings, and we can plot those in color for each env_obj, according to the larger grouping it is part of by changing colorset = 'env_group'. We need to use point_group here to separate out the points for each env_obj, since several will belong to each env_group.\nThis plot also demonstrates the use of some additional arguments. We’re also using transx to log the x-axis, which is particularly appropriate for the multiplicative flow scaling in this demonstration. We also log the y-axis with transoutcome since we’re using a base_list to baseline (with comp_fun = relative to look at the multiplicative shift in each env_obj to baseline). We’re using various *_lab arguments to adjust the labelling.\n\n\n\n\n\n\nTip\n\n\n\nWhen we want to transform the y-axis and the outcome is on y, we need to use transoutcome instead of transoutcome. That is because the consistent processing of the outcome variable needs to be treated differently than y for plots like heatmaps and maps.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe group_cols argument in base_list is needed to specify unique rows for the baselining, and so must be chosen to provide appropriate groupings (unique levels) for that baselining. For example, if we are baselining scenarios, we need to include every column that might vary within scenarios. This might differ from point_group, which only determines the visual grouping with group in ggplot2::aes().\n\n\nScientifically, one important thing to note here is that the range on y (0-10) is much greater than the range on x (0.3 - 3), and so (unsurprisingly), some outcomes are disproportionately impacted by flow. Other outcome values are less than the relative shift in flow, and so there are others that are disproportionately insensitive. These disproportionate responses also depend on whether flows decrease or increase- they are not symmetric.\n\nsdl_line_options &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = \"flow_multiplier\",\n    y_lab = \"Proportion met\",\n    x_lab = \"Change in flow\",\n    transx = \"log10\",\n    transoutcome = \"log10\",\n    color_lab = \"Environmental\\ngroup\",\n    colorset = \"env_group\",\n    pal_list = list(\"scico::berlin\"),\n    point_group = \"env_obj\",\n    facet_row = \"SWSDLName\",\n    sceneorder = sceneorder,\n    base_list = list(\n          base_lev = \"climatebaseadapt0\",\n    comp_fun = \"relative\",\n    group_cols = c(\"env_obj\", \"polyID\")\n    )\n  )\n\nsdl_line_options\n\n\n\n\n\n\n\nWe can also give the groups different palettes, as demonstrated more completely in the bar plots and causal networks. Now, we don’t need point_group anymore, since the colors are assigned to the unique env_objs.\n\n# Create a palette list\ngrouplist &lt;- list(\n  EF = \"grDevices::Purp\",\n  NF = \"grDevices::Mint\",\n  NV = \"grDevices::Burg\",\n  OS = \"grDevices::Blues\",\n  WB = \"grDevices::Peach\"\n)\n\nsdl_line_groups &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = \"flow_multiplier\",\n    y_lab = \"Proportion met\",\n    x_lab = \"Change in flow\",\n    transx = \"log10\",\n    transoutcome = \"log10\",\n    color_lab = \"Environmental\\ngroup\",\n    colorgroup = \"env_group\",\n    colorset = \"env_obj\",\n    pal_list = grouplist,\n    facet_row = \"SWSDLName\",\n    base_list = list(\n          base_lev = \"climatebaseadapt0\",\n    comp_fun = \"relative\",\n    group_cols = c(\"env_obj\", \"polyID\")\n    )\n  )\n\nsdl_line_groups\n\n\n\n\n\n\nFigure 1: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Groups of environmental objectives plotted from different color palettes.\n\n\n\n\nThat’s fairly complex, so we can facet it, as we did with the bars to make the individual env_objs easier to see.\n\nsdl_line_groups_facet &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = \"flow_multiplier\",\n    y_lab = \"Proportion met\",\n    x_lab = \"Change in flow\",\n    transx = \"log10\",\n    transoutcome = \"log10\",\n    color_lab = \"Environmental\\ngroup\",\n    colorgroup = \"env_group\",\n    colorset = \"env_obj\",\n    pal_list = grouplist,\n    facet_row = \"SWSDLName\",\n    facet_col = \"env_group\",\n    base_list = list(\n          base_lev = \"climatebaseadapt0\",\n    comp_fun = \"relative\",\n    group_cols = c(\"env_obj\", \"polyID\")\n    )\n  )\n\nsdl_line_groups_facet\n\n\n\n\n\n\nFigure 2: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Groups of environmental objectives plotted from different color palettes and facetted for easier visualisation.\n\n\n\n\nThe above is typically how we would go about this facetting, but it is worth reiterating that these are just ggplots, and so we can post-hoc add facetting. Using the version with only spatial facetting (Figure 1), we can add the env_group facet on, matching Figure 2. Note that we re-build all the facets here, due to the specification of ggplot2::facet_grid.\n\nsdl_line_groups + facet_grid(SWSDLName ~ env_group)\n\n\n\n\n\n\n\nAs with the bar plots, we can color by any column we want, and the spatial units is a logical choice. We again use point_group, since multiple env_obj rows are mapped to each color (spatial unit). The overplotting gets unreadable here and so I’ve retained some facetting, but the best combination of colors, row facets, and column facets will be project dependent and also depend on the number of values being plotted. Another solution is to summarise the data with a smoother- see Section 4.2.\n\nsdl_line_sdl &lt;- obj_sdl_to_plot |&gt;\n  # filter(env_group == \"EF\") |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = \"flow_multiplier\",\n    y_lab = \"Proportion met\",\n    x_lab = \"Change in flow\",\n    transx = \"log10\",\n    transoutcome = \"log10\",\n    color_lab = \"SDL unit\",\n    colorset = \"SWSDLName\",\n    pal_list = list(\"ggsci::default_jama\"),\n    point_group = \"env_obj\",\n    facet_col = \"env_group\",\n    base_list = list(\n          base_lev = \"climatebaseadapt0\",\n    comp_fun = \"relative\",\n    group_cols = c(\"env_obj\", \"polyID\")\n    )\n  )\n\nsdl_line_sdl\n\n\n\n\n\n\nFigure 3: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Colors indicate SDL unit, each line is an env_obj.",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Line plots (quant x)"
    ]
  },
  {
    "objectID": "comparer/line_plots.html#sec-smoothers",
    "href": "comparer/line_plots.html#sec-smoothers",
    "title": "Line plots (quantitative x)",
    "section": "Smoothing (fit lines)",
    "text": "Smoothing (fit lines)\nWe can use smoothing to fit lines through multiple points, e.g. if we want to group data in some way- maybe use it to put a line through the color groups and ignore individual levels. We demonstrate here using them to illustrate unique outcomes, as well as more typical uses as lines of best fit that aggregate over a number of outcomes.\nTo get smoothed lines, we use smooth_arglist = list(). By default, (just an empty list()), that produces a loess fit (as with ggplot2::geom_smooth, but we can also pass arguments to geom_smooth() in smooth_arglist, and so allow things like lm and glm fits.\nUnique points\nFitting lines through unique points at each scenario level is a bit contrived vs. just plotting lines as above, but it can be useful if we want to accentuate nonlinear relationships. Linear fits are possible too, though these are typically less useful.\nWith unique points, this just fits a single curved line through each env_obj. Recapitulating the above, we color here from SDL unit.\n\nsdl_smooth_sdl &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = \"flow_multiplier\",\n    y_lab = \"Proportion met\",\n    x_lab = \"Change in flow\",\n    transx = \"log10\",\n    color_lab = \"Catchment\",\n    colorgroups = NULL,\n    colorset = \"SWSDLName\",\n    point_group = \"env_obj\",\n    pal_list = list(\"ggsci::default_jama\"),\n    facet_row = \"env_group\",\n    base_list = list(\n          base_lev = \"climatebaseadapt0\",\n    comp_fun = \"relative\",\n    group_cols = c(\"env_obj\", \"polyID\")\n    ),\n    smooth_arglist = list()\n  )\n\nsdl_smooth_sdl\n\n\n\n\n\n\n\nAnd we can do the same for environmental groupings. This makes much more sense as an analysis if we fit the line through the group, as we do in Section 4.2.2. We use zero_adjust = 'auto' to adjust values off zero for the log transform of y.\n\nsdl_smooth_groups &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = \"flow_multiplier\",\n    y_lab = \"Proportion met\",\n    x_lab = \"Change in flow\",\n    transx = \"log10\",\n    transoutcome = 'log10',\n    color_lab = \"Environmental grouping\",\n    colorgroups = NULL,\n    colorset = \"env_group\",\n    point_group = \"env_obj\",\n    pal_list = list(\"scico::berlin\"),\n    facet_row = \"env_group\",\n    facet_col = \"SWSDLName\",\n    base_list = list(\n          base_lev = \"climatebaseadapt0\",\n    comp_fun = \"relative\",\n    group_cols = c(\"env_obj\", \"polyID\")\n    ),\n    smooth_arglist = list(),\n    zero_adjust = 'auto'\n  )\nsdl_smooth_groups\n\n\n\n\n\n\n\nUsing method = 'lm' in smooth_arglist yields a linear fit (and any other method accepted by ggplot2::geom_smooth() should work). It does not recapitulate the simple lines above, however, because it fits the line through all the scenario data points, rather than simply joining them together. I have passed se = FALSE to ggplot2::geom_smooth() here because with unique groups the standard errors are enormous. Again, this is a bit contrived unless we want to look for a trend for each outcome at this level. A more typical way of looking at things would be to look at the trend across lots of values within some group, Section 4.2.2.\n\nobj_sdl_to_plot |&gt;\n    plot_outcomes(\n        outcome_col = \"ewr_achieved\",\n        x_col = \"flow_multiplier\",\n        y_lab = \"Proportion met\",\n        x_lab = \"Change in flow\",\n        transx = \"log10\",\n        color_lab = \"Environmental grouping\",\n        colorset = \"env_group\",\n        point_group = \"env_obj\",\n        pal_list = list(\"scico::berlin\"),\n        facet_row = \"env_group\",\n        facet_col = \"SWSDLName\",\n        base_list = list(\n            base_lev = \"climatebaseadapt0\",\n            comp_fun = \"relative\",\n            group_cols = c(\"env_obj\", \"polyID\")\n        ),\n        smooth_arglist = list(method = 'lm', se = FALSE)\n    )\n\n\n\n\n\n\n\nFit multiple points\nFitting lines is most often associated with things like regression and loess smoothing, where we use it to aggregate over a number of datapoints to find the line of best fit. We can do that here, simply by not having all points accounted for across the facetting, point_group, and colorset.\n\n\n\n\n\n\nNote\n\n\n\ngroup_cols in base_list should still include unique values, because group_cols determines the baselining (e.g. what gets compared), not the plot groupings.\n\n\nOne example would be to perform the same analysis as in Figure 3, but instead of plotting each point, fit a line to show the mean change within each SDL unit. We’ve pulled env_obj out of point_group, but left it in group_cols, because we still want each env_obj baselined with itself, not to the mean of env_group. Now, we can look at all the env_groups, because there are far fewer lines and so the overplotting isn’t an issue.\nWe use a small add_eps to avoid zeros and allow all data to be relativised and plotted.\n\nsdl_fit_sdl &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = \"flow_multiplier\",\n    y_lab = \"Proportion met\",\n    x_lab = \"Change in flow\",\n    transx = \"log10\",\n    transoutcome = \"log10\",\n    color_lab = \"SDL unit\",\n    colorset = \"SWSDLName\",\n    pal_list = list(\"ggsci::default_jama\"),\n    facet_wrapper = \"env_group\",\n        base_list = list(\n            base_lev = \"climatebaseadapt0\",\n            comp_fun = \"relative\",\n            add_eps = 'auto',\n            group_cols = c(\"env_obj\", \"polyID\")\n        ),\n        smooth_arglist = list()\n  )\n\nsdl_fit_sdl\n\n\n\n\n\n\nFigure 4: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Fits are loess smoothers. Colors indicate SDL unit, which have single lines. Each point is an env_obj.\n\n\n\n\nWe can make a very similar plot, looking at the environmental groups, a smooth fit of Figure 1 . We use a position argument (which passes to {ggplot2}, and so has the same syntax) to see overplotted points, and an add_eps to avoid zeros to relativise and plot all the data.\n\nsdl_fit_groups &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = \"flow_multiplier\",\n    y_lab = \"Proportion met\",\n    x_lab = \"Change in flow\",\n    transx = \"log10\",\n    transoutcome = \"log10\",\n    color_lab = \"Environmental\\ngroup\",\n    colorset = \"env_group\",\n    pal_list = list(\"scico::berlin\"),\n    facet_row = \"SWSDLName\",\n    facet_col = \".\",\n        base_list = list(\n            base_lev = \"climatebaseadapt0\",\n            comp_fun = \"relative\",\n            add_eps = 'auto',\n            group_cols = c(\"env_obj\", \"polyID\")\n        ),\n        smooth_arglist = list(),\n    position = position_jitter(width = 0.01, height = 0)\n  )\n\nsdl_fit_groups\n\n\n\n\n\n\nFigure 5: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Fits are loess smoothers. Colors indicate Environmental groups, which have single lines. Each point is an env_obj.\n\n\n\n\nAs we saw above, we can use method = 'lm' to plot a regression, though in general we do not expect these relationships to be linear, and mathematically characterising them will be a complex task that is not the purview of plotting.\nA linear fit of the SDL units ( Figure 6 ) is one example of how this might work. It is useful to know here that deviations from a 1:1 line on logged axes as here means that the outcomes are responding disproportionately more (steeper) or less (shallower) than the underlying changes to flow.\n\nsdl_lm_sdl &lt;- obj_sdl_to_plot |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    x_col = \"flow_multiplier\",\n    y_lab = \"Proportion met\",\n    x_lab = \"Change in flow\",\n    transx = \"log10\",\n    transoutcome = \"log10\",\n    color_lab = \"SDL unit\",\n    colorset = \"SWSDLName\",\n    pal_list = list(\"ggsci::default_jama\"),\n    facet_wrapper = \"env_group\",\n        base_list = list(\n            base_lev = \"climatebaseadapt0\",\n            comp_fun = \"relative\",\n            add_eps = 'auto',\n            group_cols = c(\"env_obj\", \"polyID\")\n        ),\n        smooth_arglist = list(method = 'lm')\n  )\n\nsdl_lm_sdl\n\n\n\n\n\n\nFigure 6: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Fits are linear regressions. Colors indicate SDL unit, which have single lines. Each point is an env_obj.",
    "crumbs": [
      "Comparer",
      "Plot types",
      "Line plots (quant x)"
    ]
  },
  {
    "objectID": "aggregator/spatial_agg.html",
    "href": "aggregator/spatial_agg.html",
    "title": "Spatial aggregation",
    "section": "",
    "text": "library(HydroBOT)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\nAggregating data to larger spatial scales is a key requirement of the aggregator component of HydroBOT. To do this spatially, we need to specify the larger units into which input data (typically in smaller spatial units) gets aggregated and the functions to use to do that aggregation. This aggregation needs to respect grouping, most obviously for scenarios, but we also need to keep theme groupings separated during spatial aggregation steps to ensure dimensional safety. Moroever, there is the special case of nonspatial aggregation of spatial data where units at one scale may be relevant to other units they are not geographically related to (e.g. gauges providing information for distant SDL units). Finally, the spatial aggregator enforces recording of the aggregations to ensure the meaning of the data isn’t lost. This results in column names like spatial_ArithmeticMean_ewr_achieved, which are cumbersome but ensure the data provenance is not lost. They can be dealt with more cleanly in multi_aggregate and with agg_names_to_cols(), which turn them into columns.\nHere, we explore the spatial aggregator, spatial_aggregate, in isolation to discuss its specific characteristics, though in practice it is typically used within multi_aggregate and read_and_agg which wrap spatial_aggregate, temporal_aggregate, and theme_aggregate to allow interleaved aggregation steps in a standardised format. See the page for multi_aggregate and read_and_agg for more details.\nWe often will want to only perform a single spatial aggregation (e.g. from gauges to sdl units), but there are instances where that isn’t true- perhaps we want to aggregate from sdl units to states or the basin. Thus, multi-step spatial aggregation is possible, including the situation where aggregation units (polygons) are not nested, as would be the case for sdl units and states, for example.\nThis document delves fairly in-depth into capabilities, including things like argument types and how they relate to other functions and permit certain tricks. Not all of these will be used or needed to understand by most users- typically there will be a set of aggregation steps fed to read_and_agg and that will be that. This sort of simpler setup is shown in the combined aggregation notebook, read-in and aggregate, and in a combined workflow. See the syntax notebook for a detailed look at argument construction for various purposes. Here, we use that syntax to demonstrate how the spatial aggregation works and the different ways it can be done.\nAs with all of the internal HydroBOT aggregations, the spatial_aggregate() function is exposed and can be used directly, but can be very ad-hoc and easy to forget theme- and temporal-axis grouping. In general, best practice is to pass a list giving the aggregation sequence to multi_aggregate.",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Spatial aggregation capability"
    ]
  },
  {
    "objectID": "aggregator/spatial_agg.html#demonstration-setup",
    "href": "aggregator/spatial_agg.html#demonstration-setup",
    "title": "Spatial aggregation",
    "section": "Demonstration setup",
    "text": "Demonstration setup\nFor this demonstration, we start with gauge-referenced data at the env_obj theme scale, and so do a bit of data pre-processing to get there.\n\nproject_dir &lt;- 'hydrobot_scenarios'\nhydro_dir &lt;- file.path(project_dir, 'hydrographs')\newr_results &lt;- file.path(project_dir, \"module_output\", \"EWR\")\n\newr_out &lt;- prep_run_save_ewrs(\n  hydro_dir = hydro_dir,\n  output_parent_dir = project_dir,\n  outputType = list('none'),\n  returnType = list('yearly')\n)\n\n\n# This is just a simple prep step that is usually done internally to put the geographic coordinates on input data\newrdata &lt;- prep_ewr_output(ewr_out$yearly, type = 'achievement')\n\n# This gets us to env_obj at the gauge over all time\npreseq &lt;- list(\n  all_time = \"all_time\",\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  env_obj = c(\"ewr_code\", \"env_obj\")\n  )\n\nfunseq &lt;- list(\n  all_time = 'ArithmeticMean',\n  ewr_code = \"CompensatingFactor\",\n  env_obj = \"ArithmeticMean\"\n)\n\n# Do the aggregation to get env_obj at each gauge\nsimpleThemeAgg &lt;- multi_aggregate(\n  dat = ewrdata,\n  causal_edges = causal_ewr,\n  groupers = c(\"scenario\", \"gauge\"),\n  aggCols = \"ewr_achieved\",\n  aggsequence = preseq,\n  funsequence = funseq\n)\n\nsimpleThemeAgg\n\n\n  \n\n\n\nThis provides a spatially-referenced (to gauge) theme-aggregated tibble to use to demonstrate spatial aggregation. Note that this has the gauge (spatial unit), but also two groupings that we want to preserve when we spatially aggregate- scenario and the current level of theme grouping, env_obj. We have dropped the time by taking the temporal average in step one, but that would be preserved as well if present.",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Spatial aggregation capability"
    ]
  },
  {
    "objectID": "aggregator/spatial_agg.html#spatial-inputs-polygons",
    "href": "aggregator/spatial_agg.html#spatial-inputs-polygons",
    "title": "Spatial aggregation",
    "section": "Spatial inputs (polygons)",
    "text": "Spatial inputs (polygons)\nSpatial aggregation requires polygons to aggregate into, and we want the capability to do that several times. The user can read in any desired polygons with sf::read_sf(path/to/polygon.shp), but here we use those provided in the standard set with {HydroBOT}. We’ll use SDL units, catchments (from CEWO), and the basin to show how the aggregation can have multiple steps with polygons that may not be nested (though care should be taken when that is the case).",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Spatial aggregation capability"
    ]
  },
  {
    "objectID": "aggregator/spatial_agg.html#single-aggregation",
    "href": "aggregator/spatial_agg.html#single-aggregation",
    "title": "Spatial aggregation",
    "section": "Single aggregation",
    "text": "Single aggregation\nWe might just want to aggregate spatially once. We can do this simply by passing the input data (anything spatial, in this case simpleThemeAgg), a set of polygons, and providing a funlist. In this simple case, we just use a bare function name, here the custom ArithmeticMean which is just a simple wrapper of mean with na.rm = TRUE. Any function can be passed this way, custom or in-built, provided it has a single argument. More complex situations are given below, and different syntax is possible.\n\n\n\n\n\n\nNote\n\n\n\nThe funlist argument here specifies the function(s) to use at a single step. It is thus not the same as the funsequence list of multi_aggregate(); instead being a single item in that list, though it may include multiple functions (e.g. the mean and max).\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe aggCols argument is ends_with(original_name) to reference the original name of the column of values- it may have a long name tracking its aggregation history, so we give it the tidyselect::ends_with to find the column. More generally, both aggCols and groupers can take any tidyselect syntax or bare names or characters, see here.\n\n\n\nobj2poly_lost &lt;- spatial_aggregate(\n  dat = simpleThemeAgg,\n  to_geo = sdl_units,\n  groupers = \"scenario\",\n  aggCols = ends_with(\"ewr_achieved\"),\n  funlist = ArithmeticMean\n)\n\nWarning: !\nEWR gauge to sdl units or planning units detected without `pseudo_spatial`!\nℹ Gauges inform multiple SDLs and PUs; this will be lost.\nℹ EWR outputs should be joined to `sdl_units` (or `planning_units`) pseudo-spatially (by column names), not with a spatial join\nℹ Best to explicitly use `pseudo_spatial = 'sdl_units'` in `multi_aggregate()` or `read_and_agg()`.\nℹ Lower-level processing should include as `joinby = 'nonspatial'` in `spatial_aggregate()`\n\nobj2poly_lost\n\n\n  \n\n\n\nBecause we are using spatial_aggregate directly, the dimensional safety provided by multi_aggregate() is not present, and this example has lost the theme levels- e.g. the different env_objs are no longer there and were all averaged together. The multi_aggregate function automatically handles this preservation, but spatial_aggregate is more general, and does not make any assumptions about the grouping structure of the data. Thus, to keep the env_obj groupings (as we should, otherwise we’re inadvertently theme-aggregating over all of them), we need to add env_obj to the groupers argument.\n\n\n\n\n\n\nTip\n\n\n\nWe use the prefix = 'sdl_units_' argument (which tracks aggregation steps) to mimic what multi_aggregate does internally. The default is spatial_, but providing the units is more informative.\n\n\n\nobj2poly &lt;- spatial_aggregate(\n  dat = simpleThemeAgg,\n  to_geo = sdl_units,\n  groupers = c(\"scenario\", \"env_obj\"),\n  aggCols = ends_with(\"ewr_achieved\"),\n  funlist = ArithmeticMean,\n  prefix = 'sdl_units_'\n)\n\nWarning: !\nEWR gauge to sdl units or planning units detected without `pseudo_spatial`!\nℹ Gauges inform multiple SDLs and PUs; this will be lost.\nℹ EWR outputs should be joined to `sdl_units` (or `planning_units`) pseudo-spatially (by column names), not with a spatial join\nℹ Best to explicitly use `pseudo_spatial = 'sdl_units'` in `multi_aggregate()` or `read_and_agg()`.\nℹ Lower-level processing should include as `joinby = 'nonspatial'` in `spatial_aggregate()`\n\nobj2poly\n\n\n  \n\n\n\nThe resulting column name is cumbersome, but provide a record of exactly what the aggregation sequence was.\n\nnames(obj2poly)\n\n[1] \"scenario\"                                                                                                        \n[2] \"env_obj\"                                                                                                         \n[3] \"polyID\"                                                                                                          \n[4] \"sdl_units_ArithmeticMean_env_obj_ArithmeticMean_ewr_code_CompensatingFactor_all_time_ArithmeticMean_ewr_achieved\"\n[5] \"SWSDLID\"                                                                                                         \n[6] \"StateID\"                                                                                                         \n[7] \"SWSDLName\"                                                                                                       \n[8] \"geometry\"                                                                                                        \n\n\nWe can clean those up into columns with agg_names_to_cols() (which happens internally in multi_aggregate() and read_and_agg() with namehistory = FALSE).\n\nobj2poly_rename &lt;- agg_names_to_cols(obj2poly, \n                              aggsequence = c(names(preseq), 'sdl_units'), \n                              funsequence = c(funseq, 'ArithmeticMean'), \n                              aggCols = 'ewr_achieved')\nobj2poly_rename\n\n\n  \n\n\n\nA quick plot shows the outcome. Because we have used the default keepAllPolys = FALSE, only the sdl units with data are here. See Section 2.4 for retaining them. For more plotting details, see the plotting section. We’ll simplify the names and choose a subset of the environmental objectives.\n\nobj2poly_rename |&gt;\n  filter(grepl(\"^EF[1-3]\", env_obj)) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    y_lab = \"Arithmetic Mean\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"env_obj\",\n    facet_row = \"scenario\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n  )",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Spatial aggregation capability"
    ]
  },
  {
    "objectID": "aggregator/spatial_agg.html#multiple-aggregations-at-a-step",
    "href": "aggregator/spatial_agg.html#multiple-aggregations-at-a-step",
    "title": "Spatial aggregation",
    "section": "Multiple aggregations at a step",
    "text": "Multiple aggregations at a step\nIf we give funlist more than one aggregation function, it calculates both. Here, we use the mean and median.\n\nmedna &lt;- function(x) {\n  median(x, na.rm = TRUE)\n}\n\n\nobj2poly_2 &lt;- spatial_aggregate(\n  dat = simpleThemeAgg,\n  to_geo = sdl_units,\n  groupers = c(\"scenario\", \"env_obj\"),\n  aggCols = ends_with(\"ewr_achieved\"),\n  funlist = c('ArithmeticMean', 'medna'),\n  prefix = 'sdl_units_'\n)\n\nWarning: !\nEWR gauge to sdl units or planning units detected without `pseudo_spatial`!\nℹ Gauges inform multiple SDLs and PUs; this will be lost.\nℹ EWR outputs should be joined to `sdl_units` (or `planning_units`) pseudo-spatially (by column names), not with a spatial join\nℹ Best to explicitly use `pseudo_spatial = 'sdl_units'` in `multi_aggregate()` or `read_and_agg()`.\nℹ Lower-level processing should include as `joinby = 'nonspatial'` in `spatial_aggregate()`\n\n# we need a full sequence\nfunseq2 &lt;- funseq\nfunseq2$sdl_units = list('ArithmeticMean', 'medna')\n\nobj2poly_2 &lt;- agg_names_to_cols(obj2poly_2, \n                              aggsequence = c(names(preseq), 'sdl_units'), \n                              funsequence = funseq2, \n                              aggCols = 'ewr_achieved')\n\nAttempting to deparse lambda function in funsequence. \nℹ It is better to use named functions than lambdas in the funsequence, because they are more clearly defined and can be more easily known later.\n\n\nWarning in funsequence[!fs] &lt;- names(unlist(funsequence[!fs])): number of items\nto replace is not a multiple of replacement length\n\n\nAnd now we can compare what we get out of the different functions (just for the baseline scenario or we run out of dimensions).\n\nobj2poly_2 |&gt;\n  filter(scenario == 'base') |&gt; \n  filter(grepl(\"^EF[1-3]\", env_obj)) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"env_obj\",\n    facet_row = \"aggfun_4\"\n  )",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Spatial aggregation capability"
    ]
  },
  {
    "objectID": "aggregator/spatial_agg.html#multiple-spatial-levels",
    "href": "aggregator/spatial_agg.html#multiple-spatial-levels",
    "title": "Spatial aggregation",
    "section": "Multiple spatial levels",
    "text": "Multiple spatial levels\nThere are a number of polygon layers we might want to aggregate into in addition to SDL units, e.g. resource plan areas, hydrological catchments, or the whole basin. We can aggregate directly into them just as we have above for SDL units. However, we might also want to have several levels of spatial aggregation, which may be nested or nearly so, e.g. from SDL units to the basin, or may be nonnested, e.g. from SDL units to catchments. Typically, this would happen with intervening theme aggregations, as in the interleaved example.\n\n\n\n\n\n\nImportant\n\n\n\nAn area column is always created for polygons, making it available for things like area-weighted means.\n\n\nNested\nA straightforward example of aggregating one set of polygons into a larger set is to move from the sdl units we’ve just used to the larger Murray-Darling Basin. Note the use of tidyselect::ends_with() to get the right column to aggregate and the weighted mean, see syntax for more detail.\n\nsimplepolypoly &lt;- spatial_aggregate(\n  dat = obj2poly,\n  to_geo = basin,\n  groupers = c(\"scenario\", \"env_obj\"),\n  aggCols = ends_with(\"ewr_achieved\"),\n  funlist = rlang::quo(list(wmna = ~ weighted.mean(., .data$area, na.rm = TRUE))),\n  na.rm = TRUE,\n  failmissing = FALSE,\n  prefix = 'basin_'\n)\nsimplepolypoly\n\n\n  \n\n\n\n\nsimplepolypoly |&gt; \n  rename('ewr_achieved' = ends_with('ewr_achieved')) |&gt;\n  filter(grepl(\"^EF[1-3]\", env_obj)) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    y_lab = \"Arithmetic Mean\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"env_obj\",\n    facet_row = \"scenario\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n  )\n\n\n\n\n\n\n\nNon-nested\nAggregating from SDL units to CEWO valleys requires addressing issues of overlaps among the various polygons. As such, it makes a good test case that catches issues with the intersection of polygons that might not happen with a simpler set of polygons Figure 1 .\n\n\n\n\n\n\nCaution\n\n\n\nThe aggregation process for multiple spatial levels works the same whether or not the smaller levels nest into the larger, but more care should be taken (and more explanation is needed) in the nonnested case.\n\n\n\noverlay_cewo_sdl &lt;- ggplot() +\n  geom_sf(data = sdl_units, aes(fill = SWSDLName), color = NA, alpha = 0.5) +\n  geom_sf(data = cewo_valleys, aes(color = ValleyName), fill = NA) +\n  theme(legend.position = \"none\") +\n  ggtitle('both')\n\nvalleys &lt;- ggplot() +\n  geom_sf(data = cewo_valleys, aes(color = ValleyName), fill = \"white\") +\n  theme(legend.position = \"none\") +\n  ggtitle('cewo_valleys')\n\nsdls &lt;- ggplot() +\n  geom_sf(data = sdl_units, aes(fill = SWSDLName), alpha = 0.5) +\n  theme(legend.position = \"none\") +\n  ggtitle('sdl_units')\n\nvalleys + sdls + overlay_cewo_sdl\n\n\n\n\n\n\nFigure 1: CEWO valleys (coloured lines) and SDL units (coloured fills) alone (a & b) and overlain (c), showing these are not nested, but instead are intersecting polygons.\n\n\n\n\nTo aggregate from one set of polygons into the other, spatial_aggregate() splits them up and aggregate in a way that respects area and borders. In other words, if a polygon lays across two of the next level up, only the bits that overlap a given polygon in that next level up are included. Under the hood, sf::st_intersection() splits the polygons to make a new set of non-overlapping polygons. Then these pieces are aggregated into the higher level. This aggregation should carefully consider area- things like means should be area-weighted, and things like sums, minima, and maxima should be thought about carefully- if the lower ‘from’ data are already sums, for example, an area weighting might make sense to get a proportion of the sum, but this is highly dependent on the particular sequence of aggregation. For other functions like minima and maxima, area-weighting may or may not be appropriate, and so careful attention should be paid to constructing the aggregation sequence and custom functions may be involved.\nTo illustrate what happens internally, the intersection of sdl_units and cewo_valleys chops up sdl_units so there are unique polygons for each sdl unit - valley combination.\n\njoinpolys &lt;- st_intersection(sdl_units, cewo_valleys)\njoinpolys\n\n\n  \n\n\n\nTo better see the many-to-many chopping we get with this particular pair of intersecting shapefiles, we can isolate an SDL unit (Victorian Murray) and see that it contains bits of 8 catchments. Likewise, the Loddon catchment contains bits of 4 SDL units Figure 2 .\n\nnvorig &lt;- ggplot() +\n  geom_sf(data = dplyr::filter(sdl_units, SWSDLName == \"Victorian Murray\"))\n\nnvpostjoin &lt;- ggplot() +\n  geom_sf(\n    data = dplyr::filter(\n      joinpolys,\n      SWSDLName == \"Victorian Murray\"\n    ),\n    aes(fill = ValleyName)\n  )\n\navorig &lt;- ggplot() +\n  geom_sf(data = dplyr::filter(cewo_valleys, ValleyName == \"Loddon\"))\n\navpostjoin &lt;- ggplot() +\n  geom_sf(\n    data = dplyr::filter(\n      joinpolys,\n      ValleyName == \"Loddon\"\n    ),\n    aes(fill = SWSDLName)\n  )\n\n(nvorig + nvpostjoin) / (avorig + avpostjoin)\n\n\n\n\n\n\nFigure 2: SDL units intersected with CEWO valleys and split to allow aggregation\n\n\n\n\nIn practice, non-nested aggregation works just like nested; a one-off aggregation from polygons to another set of polygons uses spatial_aggregate() as before. Here, we could use the obj2poly aggregation into SDL units created above as the starting point. Here, we use the pre-defined 'SpatialWeightedMean', which is the same as the anonymous function passed to the basin aggregation above.\n\nnestedpolypoly &lt;- spatial_aggregate(\n  dat = obj2poly,\n  to_geo = cewo_valleys,\n  groupers = c(\"scenario\", \"env_obj\"),\n  aggCols = ends_with(\"ewr_achieved\"),\n  funlist = \"SpatialWeightedMean\",\n  na.rm = TRUE,\n  failmissing = FALSE,\n  prefix = 'cewo_valleys_'\n)\nnestedpolypoly\n\n\n  \n\n\n\nThose are now in the CEWO valleys, with values from the intersecting parts of the sdl_units.\n\n\n\n\n\n\nWarning\n\n\n\nEven valleys with only a small part of an sdl_unit have values here, illustrating one of the dangers of such non-nested aggregations.\n\n\n\nnestedpolypoly |&gt; \n  rename('ewr_achieved' = ends_with('ewr_achieved')) |&gt;\n  filter(grepl(\"^EF[1-3]\", env_obj)) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    y_lab = \"Arithmetic Mean\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"env_obj\",\n    facet_row = \"scenario\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n  )",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Spatial aggregation capability"
    ]
  },
  {
    "objectID": "aggregator/spatial_agg.html#sec-poly-retention",
    "href": "aggregator/spatial_agg.html#sec-poly-retention",
    "title": "Spatial aggregation",
    "section": "Polygon retention with keepAllPolys\n",
    "text": "Polygon retention with keepAllPolys\n\nIn some cases, we might want to keep or drop polygons that don’t have data to have better-looking maps. By default, polygons without data are dropped, both for memory management and to avoid clutter. But if we set keepAllPolys = TRUE, we can keep them.\n\nobj2poly_keep &lt;- spatial_aggregate(\n  dat = simpleThemeAgg,\n  to_geo = sdl_units,\n  groupers = c(\"scenario\", \"env_obj\"),\n  aggCols = ends_with(\"ewr_achieved\"),\n  funlist = ArithmeticMean,\n  prefix = 'sdl_units_'\n)\nobj2poly_keep\n\n\n  \n\n\n\n\nobj2poly_rename |&gt;\n  rename('ewr_achieved' = ends_with('ewr_achieved')) |&gt;\n  filter(grepl(\"^EF[1-3]\", env_obj)) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    y_lab = \"Arithmetic Mean\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    facet_col = \"env_obj\",\n    facet_row = \"scenario\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n  )\n\n\n\n\n\n\n\nNow we have retained polygons without data. The default is keepAllPolys = FALSE. Another option for plotting though would be to use FALSE as earlier, and include an underlay. For more detail, see the plotting.\n\nobj2poly_rename |&gt;\n  rename('ewr_achieved' = ends_with('ewr_achieved')) |&gt;\n  filter(grepl(\"^EF[1-3]\", env_obj)) |&gt;\n  plot_outcomes(\n    outcome_col = \"ewr_achieved\",\n    y_lab = \"Arithmetic Mean\",\n    plot_type = \"map\",\n    colorgroups = NULL,\n    colorset = \"ewr_achieved\",\n    pal_list = list(\"scico::berlin\"),\n    pal_direction = -1,\n    underlay_list = list(underlay = 'sdl_units', pal_list = 'azure'),\n    facet_col = \"env_obj\",\n    facet_row = \"scenario\",\n    sceneorder = c(\"down4\", \"base\", \"up4\")\n  )",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Spatial aggregation capability"
    ]
  },
  {
    "objectID": "aggregator/spatial_agg.html#next-steps",
    "href": "aggregator/spatial_agg.html#next-steps",
    "title": "Spatial aggregation",
    "section": "Next steps",
    "text": "Next steps\nThe examples here are designed to dig into capability of the spatial aggregator in fairly high detail. In typical use, the capabilities here would be studied to develop an aggregation sequence that is best tailored to the study, and then use the automated and safer wrapper functions, as in the complete workflows, with more detail around those wrappers here.This document provides valuable demonstrations of capability and potential for how each spatial step in that sequence might work and could be set up.",
    "crumbs": [
      "Aggregator",
      "Dimensions",
      "Spatial aggregation capability"
    ]
  },
  {
    "objectID": "presentation/walkthrough_presentation.html#full-hydrobot-run",
    "href": "presentation/walkthrough_presentation.html#full-hydrobot-run",
    "title": "HydroBOT walkthrough",
    "section": "Full HydroBOT run",
    "text": "Full HydroBOT run\n\nlibrary(HydroBOT)\nlibrary(sf)\nlibrary(dplyr)\n\n\nMany options we could set\nSee options at github pages site for HydroBOT_website\nCan run with minimal arguments",
    "crumbs": [
      "Workflows",
      "Examples",
      "Walkthrough presentation"
    ]
  },
  {
    "objectID": "presentation/walkthrough_presentation.html#paths",
    "href": "presentation/walkthrough_presentation.html#paths",
    "title": "HydroBOT walkthrough",
    "section": "Paths",
    "text": "Paths\n\n# Outer directory for scenario\nproject_dir &lt;- file.path(\"hydrobot_scenarios\")\n\n# Hydrographs (expected to exist already)\nhydro_dir &lt;- file.path(project_dir, \"hydrographs\")\n\n# Generated data\n# EWR outputs (will be created here in controller, read from here in aggregator)\newr_results &lt;- file.path(project_dir, \"module_output\", \"EWR\")\n\n# outputs of aggregator. There may be multiple modules\nagg_results &lt;- file.path(project_dir, \"aggregator_output\")",
    "crumbs": [
      "Workflows",
      "Examples",
      "Walkthrough presentation"
    ]
  },
  {
    "objectID": "presentation/walkthrough_presentation.html#ewr-controls",
    "href": "presentation/walkthrough_presentation.html#ewr-controls",
    "title": "HydroBOT walkthrough",
    "section": "EWR controls",
    "text": "EWR controls\n\nOther modules as they are available\n\n\noutputType &lt;- list(\"yearly\")\nreturnType &lt;- list(\"none\")",
    "crumbs": [
      "Workflows",
      "Examples",
      "Walkthrough presentation"
    ]
  },
  {
    "objectID": "presentation/walkthrough_presentation.html#aggregation-sequencing",
    "href": "presentation/walkthrough_presentation.html#aggregation-sequencing",
    "title": "HydroBOT walkthrough",
    "section": "Aggregation sequencing",
    "text": "Aggregation sequencing\n\nSequence of steps\nSequence of functions\n\n\naggseq &lt;- list(\n  all_time = 'all_time',\n  ewr_code = c(\"ewr_code_timing\", \"ewr_code\"),\n  env_obj = c(\"ewr_code\", \"env_obj\"),\n  sdl_units = sdl_units,\n  Specific_goal = c(\"env_obj\", \"Specific_goal\"),\n  catchment = cewo_valleys,\n  Objective = c(\"Specific_goal\", \"Objective\"),\n  mdb = basin,\n  target_5_year_2024 = c(\"Objective\", \"target_5_year_2024\")\n)\n\n\nfunseq &lt;- list(\n  'ArithmeticMean',\n  c(\"CompensatingFactor\"),\n  c(\"ArithmeticMean\"),\n  c(\"ArithmeticMean\"),\n  c(\"ArithmeticMean\"),\n  'SpatialWeightedMean',\n  c(\"ArithmeticMean\"),\n  'SpatialWeightedMean',\n  c(\"ArithmeticMean\")\n)",
    "crumbs": [
      "Workflows",
      "Examples",
      "Walkthrough presentation"
    ]
  },
  {
    "objectID": "presentation/walkthrough_presentation.html#module",
    "href": "presentation/walkthrough_presentation.html#module",
    "title": "HydroBOT walkthrough",
    "section": "Module",
    "text": "Module\n\nCurrently just EWR\nWhere are hydrographs\nWhere to save output\n\n\newr_out &lt;- prep_run_save_ewrs(\n  hydro_dir = hydro_dir,\n  output_parent_dir = project_dir,\n  outputType = outputType,\n  returnType = returnType\n)",
    "crumbs": [
      "Workflows",
      "Examples",
      "Walkthrough presentation"
    ]
  },
  {
    "objectID": "presentation/walkthrough_presentation.html#aggregator",
    "href": "presentation/walkthrough_presentation.html#aggregator",
    "title": "HydroBOT walkthrough",
    "section": "Aggregator",
    "text": "Aggregator\nReturning instead of saving for presentation\n\nSequence (levels of each theme, space, time axis)\nAggregation functions at each step\n\n\nagged_data &lt;- read_and_agg(\n  datpath = ewr_results,\n  type = \"achievement\",\n  geopath = bom_basin_gauges,\n  causalpath = causal_ewr,\n  groupers = \"scenario\",\n  aggCols = \"ewr_achieved\",\n  aggsequence = aggseq,\n  funsequence = funseq,\n  saveintermediate = TRUE,\n  namehistory = FALSE,\n  keepAllPolys = FALSE,\n  returnList = TRUE,\n  savepath = NULL\n)",
    "crumbs": [
      "Workflows",
      "Examples",
      "Walkthrough presentation"
    ]
  },
  {
    "objectID": "presentation/walkthrough_presentation.html#input-hydrographs",
    "href": "presentation/walkthrough_presentation.html#input-hydrographs",
    "title": "HydroBOT walkthrough",
    "section": "Input hydrographs",
    "text": "Input hydrographs",
    "crumbs": [
      "Workflows",
      "Examples",
      "Walkthrough presentation"
    ]
  },
  {
    "objectID": "presentation/walkthrough_presentation.html#maps-and-spatial-scaling",
    "href": "presentation/walkthrough_presentation.html#maps-and-spatial-scaling",
    "title": "HydroBOT walkthrough",
    "section": "Maps and spatial scaling",
    "text": "Maps and spatial scaling",
    "crumbs": [
      "Workflows",
      "Examples",
      "Walkthrough presentation"
    ]
  },
  {
    "objectID": "presentation/walkthrough_presentation.html#bars--sdl-units-and-scenarios",
    "href": "presentation/walkthrough_presentation.html#bars--sdl-units-and-scenarios",
    "title": "HydroBOT walkthrough",
    "section": "Bars- SDL units and scenarios",
    "text": "Bars- SDL units and scenarios\nSDL unit differences in all environmental objectives",
    "crumbs": [
      "Workflows",
      "Examples",
      "Walkthrough presentation"
    ]
  },
  {
    "objectID": "presentation/walkthrough_presentation.html#objective-and-scenario-comparisons",
    "href": "presentation/walkthrough_presentation.html#objective-and-scenario-comparisons",
    "title": "HydroBOT walkthrough",
    "section": "Objective and scenario comparisons",
    "text": "Objective and scenario comparisons\n\nBasinSDL units",
    "crumbs": [
      "Workflows",
      "Examples",
      "Walkthrough presentation"
    ]
  },
  {
    "objectID": "presentation/walkthrough_presentation.html#lines-and-baseline",
    "href": "presentation/walkthrough_presentation.html#lines-and-baseline",
    "title": "HydroBOT walkthrough",
    "section": "Lines and baseline",
    "text": "Lines and baseline\nChange relative to baseline available to all plots\n\nDisproportionate response",
    "crumbs": [
      "Workflows",
      "Examples",
      "Walkthrough presentation"
    ]
  },
  {
    "objectID": "presentation/walkthrough_presentation.html#fits",
    "href": "presentation/walkthrough_presentation.html#fits",
    "title": "HydroBOT walkthrough",
    "section": "Fits",
    "text": "Fits\nSmoothed fit of all environmental objectives in each group\n\nRelative to baseline",
    "crumbs": [
      "Workflows",
      "Examples",
      "Walkthrough presentation"
    ]
  },
  {
    "objectID": "presentation/walkthrough_presentation.html#causal-networks",
    "href": "presentation/walkthrough_presentation.html#causal-networks",
    "title": "HydroBOT walkthrough",
    "section": "Causal networks",
    "text": "Causal networks\n\nDown 4BaselineUp 2",
    "crumbs": [
      "Workflows",
      "Examples",
      "Walkthrough presentation"
    ]
  }
]