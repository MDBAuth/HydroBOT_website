[
  {
    "objectID": "CASE_STUDY/ewr_gauges.html",
    "href": "CASE_STUDY/ewr_gauges.html",
    "title": "EWR gauges",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(sf)\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(ggplot2)\nlibrary(paletteer)\n\n\nGauges in EWR\n\n# Get the number of EWRs at each gauge in each catchment \nnewrs <- causal_ewr$ewr2obj %>% \n  group_by(gauge) %>% \n  summarise(n_ewrs = n_distinct(ewr_code))\n\nnewrs\n\n# A tibble: 133 × 2\n   gauge  n_ewrs\n   <chr>   <int>\n 1 409003      2\n 2 409017     12\n 3 409019     13\n 4 409020     13\n 5 409023     14\n 6 409024     14\n 7 409025     24\n 8 409048     14\n 9 409072      4\n10 409207     15\n# … with 123 more rows\n\nsum(newrs$n_ewrs)\n\n[1] 1442\n\n\nThat’s about half what I expect-is it because I’m looking at ewrs, not timing?\n\n# How about the timing ewrs\nnewrst <- causal_ewr$ewr2obj %>% \n  mutate(ect = stringr::str_c(ewr_code, stringr::str_replace_na(ewr_code_timing))) %>% \n  group_by(gauge) %>% \n  summarise(n_ewrs = n_distinct(ect))\n\nsum(newrst$n_ewrs)\n\n[1] 2149\n\n\nAttach a catchment?\n\n# and match them to catchment\nnewrs <- newrs %>% \n  gauge2geo(bom_basin_gauges) %>% \n  st_join(sdl_units) %>% \n  filter(!is.na(SWSDLName)) \n\n\newrhist <- newrs %>% \n  ggplot(aes(x = n_ewrs, fill = SWSDLName)) + \n  geom_histogram() +\n  labs(x = 'Number EWRs', y = 'Number gauges', fill = 'SDL unit') + \n  scale_fill_paletteer_d(palette = 'ggsci::default_igv') +\n  theme_werp_toolkit()\newrhist \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n    # plot_outcomes(newrs,\n    #               y_col = 'n_ewrs',\n    #               y_lab = 'Number EWRs',\n    #                       x_col = 'map',\n    #                       colorgroups = NULL,\n    #                       colorset = 'n_ewrs',\n    #                       pal_list = list('scico::oslo'),\n    #                       underlay_list = sdl_units)\n\newrmap <- ggplot() +\n  geom_sf(data = sdl_units, fill = 'azure', color = 'grey') +\n  geom_sf(data = newrs, mapping = aes(color = n_ewrs)) +\n  scale_color_paletteer_c('scico::bamako') +\n  theme_werp_toolkit() +\n  labs(color = 'Number\\nEWRs')\n\newrmap\n\n\n\n\n\nggsave('CASE_STUDY/ewr_hist.png', ewrhist, width = 12, height = 8, units = 'cm', dpi = 300)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\nggsave('CASE_STUDY/ewr_map.png', ewrmap, width = 10, height = 8, units = 'cm', dpi = 300)"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#full-toolkit-run",
    "href": "Presentation/walkthrough_presentation.html#full-toolkit-run",
    "title": "Toolkit walkthrough",
    "section": "Full toolkit run",
    "text": "Full toolkit run\n\nlibrary(werptoolkitr)\nlibrary(sf)\nlibrary(dplyr)\n\n\nMany options we could set\nSee options at github pages site for WERP_toolkit_demo\nCan run with minimal arguments"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#paths",
    "href": "Presentation/walkthrough_presentation.html#paths",
    "title": "Toolkit walkthrough",
    "section": "Paths",
    "text": "Paths\n\n# Outer directory for scenario\nproject_dir = file.path('scenario_example')\n\n# Hydrographs (expected to exist already)\nhydro_dir = file.path(project_dir, 'hydrographs')\n\n# Generated data\n# EWR outputs (will be created here in controller, read from here in aggregator)\newr_results <- file.path(project_dir, 'module_output', 'EWR')\n\n# outputs of aggregator. There may be multiple modules\nagg_results <- file.path(project_dir, 'aggregator_output')"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#ewr-controls",
    "href": "Presentation/walkthrough_presentation.html#ewr-controls",
    "title": "Toolkit walkthrough",
    "section": "EWR controls",
    "text": "EWR controls\n\nOther modules as they are available\n\n\noutputType <- list('summary')\nreturnType <- list('none') # list('summary', 'all')"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#aggregation-sequencing",
    "href": "Presentation/walkthrough_presentation.html#aggregation-sequencing",
    "title": "Toolkit walkthrough",
    "section": "Aggregation sequencing",
    "text": "Aggregation sequencing\n\nSequence of steps\nSequence of functions\n\n\naggseq <- list(ewr_code = c('ewr_code_timing', 'ewr_code'),\n               env_obj =  c('ewr_code', \"env_obj\"),\n               sdl_units = sdl_units,\n               Specific_goal = c('env_obj', \"Specific_goal\"),\n               catchment = cewo_valleys,\n               Objective = c('Specific_goal', 'Objective'),\n               mdb = basin,\n               target_5_year_2024 = c('Objective', 'target_5_year_2024'))\n\n\nfunseq <- list(c('CompensatingFactor'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               list(wm = ~weighted.mean(., w = area, \n                                        na.rm = TRUE)),\n               c('ArithmeticMean'),\n               \n               list(wm = ~weighted.mean(., w = area, \n                                    na.rm = TRUE)),\n               c('ArithmeticMean'))"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#module",
    "href": "Presentation/walkthrough_presentation.html#module",
    "title": "Toolkit walkthrough",
    "section": "Module",
    "text": "Module\n\nCurrently just EWR\nWhere are hydrographs\nWhere to save output\n\n\newr_out <- prep_run_save_ewrs_R(scenario_dir = hydro_dir, \n                                  output_dir = project_dir, \n                                  outputType = outputType,\n                                  returnType = returnType)"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#aggregator",
    "href": "Presentation/walkthrough_presentation.html#aggregator",
    "title": "Toolkit walkthrough",
    "section": "Aggregator",
    "text": "Aggregator\nReturning instead of saving for presentation\n\nSequence (levels of each theme, space, time axis)\nAggregation functions at each step\n\n\nagged_data <- read_and_agg(datpath = ewr_results, \n           type = 'summary',\n           geopath = bom_basin_gauges,\n           causalpath = causal_ewr,\n           groupers = 'scenario',\n           aggCols = 'ewr_achieved',\n           aggsequence = aggseq,\n           funsequence = funseq,\n           saveintermediate = TRUE,\n           namehistory = FALSE,\n           keepAllPolys = FALSE,\n           returnList = TRUE,\n           savepath = 'none')"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#input-hydrographs",
    "href": "Presentation/walkthrough_presentation.html#input-hydrographs",
    "title": "Toolkit walkthrough",
    "section": "Input hydrographs",
    "text": "Input hydrographs"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#maps-and-spatial-scaling",
    "href": "Presentation/walkthrough_presentation.html#maps-and-spatial-scaling",
    "title": "Toolkit walkthrough",
    "section": "Maps and spatial scaling",
    "text": "Maps and spatial scaling"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#bars--sdl-units-and-scenarios",
    "href": "Presentation/walkthrough_presentation.html#bars--sdl-units-and-scenarios",
    "title": "Toolkit walkthrough",
    "section": "Bars- SDL units and scenarios",
    "text": "Bars- SDL units and scenarios\nSDL unit differences in all environmental objectives"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#objective-and-scenario-comparisons",
    "href": "Presentation/walkthrough_presentation.html#objective-and-scenario-comparisons",
    "title": "Toolkit walkthrough",
    "section": "Objective and scenario comparisons",
    "text": "Objective and scenario comparisons\n\nBasinSDL units"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#lines-and-baseline",
    "href": "Presentation/walkthrough_presentation.html#lines-and-baseline",
    "title": "Toolkit walkthrough",
    "section": "Lines and baseline",
    "text": "Lines and baseline\nChange relative to baseline available to all plots\n\nDisproportionate response"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#fits",
    "href": "Presentation/walkthrough_presentation.html#fits",
    "title": "Toolkit walkthrough",
    "section": "Fits",
    "text": "Fits\nSmoothed fit of all environmental objectives in each group\n\nRelative to baseline"
  },
  {
    "objectID": "Presentation/walkthrough_presentation.html#causal-networks",
    "href": "Presentation/walkthrough_presentation.html#causal-networks",
    "title": "Toolkit walkthrough",
    "section": "Causal networks",
    "text": "Causal networks\n\nDown 4BaselineUp 4"
  },
  {
    "objectID": "aggregator/aggregation_overview.html",
    "href": "aggregator/aggregation_overview.html",
    "title": "Aggregation overview",
    "section": "",
    "text": "Incoming data from modules is typically very granular in many dimensions (as it should be if the modules are modelling data near the scale of processes). However, this means that there are thousands of different outcomes across the basin and through time. To make that useful for anything other than targeted local planning, we need to aggregate. Here, we aggregate along three dimensions- space, time, and ‘Objective’, where ‘Objective’ is the axis along increasing organisational levels, for example flow requirements for fish spawning to fish spawning to fish populations to overall environmental success.\nWe want to be able to aggregate along each of these dimensions with any number of aggregation functions (e.g. mean, min, max, more complex) to reflect the processes being aggregated. These should be factorial- if we aggregate with min and max at step 2 (generating two columns of aggregated data), then each of those columns should be aggregated according to the step-3 aggregation, and so on.\nWe want to be able to interleave the dimensions, e.g. aggregate along the Objective dimension to some intermediate level, then aggregate in space, then time, then more Objective levels, then more time and more space.\nTo achieve this, we have developed a flexible set of aggregation functions that take a list of aggregation steps, each of which can be along any dimension, and a matching list of aggregation functions to apply to that step in the aggregation. It is possible to ask for multiple functions per step."
  },
  {
    "objectID": "aggregator/aggregation_overview.html#tracking",
    "href": "aggregator/aggregation_overview.html#tracking",
    "title": "Aggregation overview",
    "section": "Tracking",
    "text": "Tracking\nIn general, aggregation over many steps can get quite complicated to track, particularly if some of the steps have multiple aggregation functions. Tracking the provenance of the final values is therefore critical to understand their meaning. By default, columns of values are named in a way that tracks their provenance, e.g. step_function_step_function_originalName. This is memory-friendly but ugly, and so we can also stack this information into columns (two for each step- one the step, the second the function) with the agg_names_to_cols function.\nFurther, in the case of a multi-step aggregation, we can either save only the final output (better for memory) or save the entire stepwise procedure, which can be very useful both for investigating results and visualisation, and it is often the case that we want to ask questions of several levels anyway."
  },
  {
    "objectID": "aggregator/aggregation_overview.html#user-inputs",
    "href": "aggregator/aggregation_overview.html#user-inputs",
    "title": "Aggregation overview",
    "section": "User inputs",
    "text": "User inputs\nThe user potentially has control over a number of decisions for the aggregation-\n\nSequencing in multiple dimensions\nAggregation function(s) to apply at each step\nData to aggregate (one or more columns)\nAny desired groupings to retain\nVarious others related primarily to data format (tracking format, saving each aggregation step, retention of NA spatial units, etc)\n\nMany of these can be specified in a number of ways. I have tried to demonstrate a number of these capabilities and options in the spatial and theme notebooks, while keeping to simpler, more typical uses in the interleaved example and the full toolkit. Some of the most useful, but also trickiest, capabilities revolve around being able to format the groupers and aggregation columns as bare names, character vectors, or tidyselect syntax, and the functions as single or multiple at each step, defined as bare names, characters, or lists with arguments. These approaches make the functions highly general and flexible to new datasets (and sequential aggregation) and custom aggregation functions.\nThe other powerful user input is the foundation of the multidimensional aggregator- the sequencing lists. These are demonstrated in all the notebooks, but are perhaps most interesting in the interleaved version. By default, only the final output is retained, but saveintermediate = TRUE saves a list of each step, which can be very useful.\nThe data itself and output data are also controlled by the user (by necessity). For inputs, multi_aggregate expects the incoming data to be in memory. There is also a wrapper read_and_agg that takes paths as arguments and does the read-in of the data internally and then runs multi_aggregate. This is often a useful approach, allowing parallelisation, better memory management, and it is far easier to use paths in a config file of arguments. Outputs from multi_aggregate are R objects- a dataframe or a list- and are returned back to the calling environment. The read_and_agg wrapper, however, can both return R objects to the session returnList = TRUE and save to a file if savepath = \"path/to/outfile\" ."
  },
  {
    "objectID": "aggregator/aggregation_overview.html#arguments-and-use-cases",
    "href": "aggregator/aggregation_overview.html#arguments-and-use-cases",
    "title": "Aggregation overview",
    "section": "Arguments and use-cases",
    "text": "Arguments and use-cases\nWe show the aggregation happening in multiple notebooks, spatial_agg.qmd, theme_agg.qmd, and theme_space_agg.qmd. These focus on different aspects, but there are valuable demonstrations of capacity in each. In addition to showing spatial aggregation, for example, the spatial aggregation notebook also works through a number of examples of how we can specify grouping, the columns to aggregate, and functions to use that are general across all the aggregation functions- we just use the spatial aggregation for an example. The different saveintermediate and namehistory options are demonstrated in each of those notebooks, showing how we can access the stepwise aggregations or only the final result, and ensure we know the exact meaning of each value in the output."
  },
  {
    "objectID": "aggregator/aggregation_overview.html#limitations",
    "href": "aggregator/aggregation_overview.html#limitations",
    "title": "Aggregation overview",
    "section": "Limitations",
    "text": "Limitations\n\nAt the time of development, there was no temporally resolved output data, and so temporal aggregation is not yet available. This is very high on the priority list, and the framework is in place to do it.\nDifferent aggregation functions for different rows in the data (e.g. mean of fish breeding, min of bird foraging) are not yet available\n\n\n-   Multiple functions are possible per step, and different functions are possible for different steps\n\n-   Different functions for different rows *within a step* requires mapping of what those functions should be, and this does not exist. Still, the capability to do this is very high on the priority list, and should not be too difficult."
  },
  {
    "objectID": "aggregator/aggregation_overview.html#development-notes",
    "href": "aggregator/aggregation_overview.html#development-notes",
    "title": "Aggregation overview",
    "section": "Development notes",
    "text": "Development notes\nI have assumed that we primarily have point data (gauges), rather than rasters, but handling rasters is a reasonably straightforward modification (and I have the code to do it elsewhere). Because of the current focus on gauges, I’m using sf primarily, but stars could be useful depending on where we get with input formats (netcdf etc) and higher-dimension data, or if we end up using rasters.\nTypically, aggregating (and some other operations) on `sf` dataframes with geometry is *much* slower than without. So I’ve put a heavy focus on safely stripping and re-adding geometry. This allows us to use dataframes that reference geometry without the geometry itself attached and only take the performance hit from geometry if it is needed.\nSpatial aggregation is inherently slow because of the geometric operations (specifically sf::st_intersection) but we’ve optimized the amount of intersecting and unpicked space where possible so nothing is slowed down by being spatial unless absolutely necessary (including the actual aggregation of the spatial data). We’re doing the absolute minimum spatially-aware processing, and doing that in a way that early spatial processing does not slow down later non-spatial processing."
  },
  {
    "objectID": "aggregator/aggregation_syntax.html",
    "href": "aggregator/aggregation_syntax.html",
    "title": "Aggregation syntax",
    "section": "",
    "text": "library(werptoolkitr)"
  },
  {
    "objectID": "aggregator/aggregation_syntax.html#argument-options-and-syntax",
    "href": "aggregator/aggregation_syntax.html#argument-options-and-syntax",
    "title": "Aggregation syntax",
    "section": "Argument options and syntax",
    "text": "Argument options and syntax\nThe aggregation functions have flexible syntax in some of their arguments, particulary for selecting grouping columns, the column(s) of values to aggregate, and the specification of aggregation functions. In general, these apply across the functions, though multi_aggregate has a bit less flexibility than the internal spatial_aggregate, theme_aggregate, and general_aggregate, largely as a consequence of passing through the call stack. Here, I use primarily the example of spatial_aggregate to illustrate the different arguments, their syntax, and why we might use it.\nI’ll create test data as in the spatial notebook.\n\nproject_dir <- file.path('scenario_example')\newr_results <- file.path(project_dir, 'module_output', 'EWR')\nsumdat <- prep_ewr_agg(ewr_results, type = 'summary', geopath = bom_basin_gauges)\n\nthemeseq <- list(c('ewr_code_timing', 'ewr_code'),\n               c('ewr_code', \"env_obj\"))\n\nfunseq <- list(c('CompensatingFactor'),\n               c('ArithmeticMean'))\n\nsimpleThemeAgg <- multi_aggregate(dat = sumdat,\n                         causal_edges = make_edges(causal_ewr, themeseq),\n                         groupers = c('scenario', 'gauge'),\n                         aggCols = 'ewr_achieved',\n                         aggsequence = themeseq,\n                         funsequence = funseq)\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\n\nJoining, by = \"gauge\"\nJoining, by = c(\"gauge\", \"ewr_code\", \"ewr_code_timing\", \"PlanningUnitID\")\nJoining, by = \"gauge\"\nJoining, by = c(\"gauge\", \"ewr_code\", \"PlanningUnitID\")\n\nsimpleThemeAgg\n\nSimple feature collection with 3444 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 144.8811 ymin: -33.86951 xmax: 151.1435 ymax: -29.92117\nGeodetic CRS:  GDA94\n# A tibble: 3,444 × 6\n   scenario gauge  polyID      env_obj env_obj_Arith…¹             geometry\n   <chr>    <chr>  <chr>       <chr>             <dbl>          <POINT [°]>\n 1 base     412002 r3cxx2uk3v7 EF1               0.714 (148.6839 -33.83301)\n 2 base     412002 r3cxx2uk3v7 EF2               0.417 (148.6839 -33.83301)\n 3 base     412002 r3cxx2uk3v7 EF3               0     (148.6839 -33.83301)\n 4 base     412002 r3cxx2uk3v7 EF3a              0.667 (148.6839 -33.83301)\n 5 base     412002 r3cxx2uk3v7 EF4               0     (148.6839 -33.83301)\n 6 base     412002 r3cxx2uk3v7 EF5               0.25  (148.6839 -33.83301)\n 7 base     412002 r3cxx2uk3v7 EF6               0     (148.6839 -33.83301)\n 8 base     412002 r3cxx2uk3v7 EF7               0     (148.6839 -33.83301)\n 9 base     412002 r3cxx2uk3v7 NF1               0.556 (148.6839 -33.83301)\n10 base     412002 r3cxx2uk3v7 NF2               0.667 (148.6839 -33.83301)\n# … with 3,434 more rows, and abbreviated variable name\n#   ¹​env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved\n\n\n\nSelecting grouping and data columns\nBoth aggCols and groupers can be character vectors, bare data-variable names, or we might want to use tidyselect syntax. For example, maybe we want to use ends_with('ewr_achieved') as above to grab pre-aggregated columns with long name histories, as in simpleThemeAgg . This is handled under the hood by selectcreator and careful parsing in the function stack. Above, we had groupers as a character vector and aggCols as tidyselect, but now we flip, and groupers is a vector of tidyselect and bare names, while aggCols is a character.\n\nNote that multi_aggregate takes advantage of this tidyselect ability under the hood to deal with the ever-lengthening column names (and sometimes expanding number of value columns if we have multiple aggregation functions at a step). This means, though, that we cannot use tidyselect to specify aggCols in multi_aggregate- it would collide with the internal tidyselect, and so we are limited to characters in that case.\nThe other restriction in multi_aggregate is that it does not accept bare function names, as they get lost for the purposes of naming the history by the time they get used in the call stack.\n\n\nobj2poly2 <- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = c(starts_with('sce'), env_obj),\n                             aggCols = \"env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved\",\n                             funlist = ArithmeticMean,\n                             keepAllPolys = TRUE)\n\nobj2poly2\n\nSimple feature collection with 3939 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 138.5685 ymin: -37.68199 xmax: 152.4821 ymax: -24.5893\nGeodetic CRS:  GDA94\n# A tibble: 3,939 × 8\n   scenario env_obj polyID      spatial_ArithmeticMean…¹ SWSDLID SWSDL…² StateID\n * <chr>    <chr>   <chr>                          <dbl> <chr>   <chr>   <chr>  \n 1 base     EF1     r602ydft049                   0.571  SS16    Lachlan NSW    \n 2 base     EF1     r6367k2uy6m                   0.217  SS20    Macqua… NSW    \n 3 base     EF1     r6d2dwp48wm                   0.238  SS21    Namoi   NSW    \n 4 base     EF2     r602ydft049                   0.416  SS16    Lachlan NSW    \n 5 base     EF2     r6367k2uy6m                   0.219  SS20    Macqua… NSW    \n 6 base     EF2     r6d2dwp48wm                   0.227  SS21    Namoi   NSW    \n 7 base     EF3     r602ydft049                   0.0417 SS16    Lachlan NSW    \n 8 base     EF3     r6367k2uy6m                   0.128  SS20    Macqua… NSW    \n 9 base     EF3     r6d2dwp48wm                   0.182  SS21    Namoi   NSW    \n10 base     EF3a    r602ydft049                   0.506  SS16    Lachlan NSW    \n# … with 3,929 more rows, 1 more variable: geometry <MULTIPOLYGON [°]>, and\n#   abbreviated variable names\n#   ¹​spatial_ArithmeticMean_env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved,\n#   ²​SWSDLName\n\n\nWe can see we get the same result as obj2poly with different ways of specifying aggCols and groupers.\nThere are times when we might want to send a vector of names, but ignore those not in the data. Most likely would be something like a set of possible grouping variables but only using them if they exist, and ignoring if not. This is a bit sloppy, but is useful sometimes to send the same set of groupers to several datasets. It fails by default, but setting failmissing = FALSE allows it to pass. Here, the ‘extra_grouper’ column doesn’t exist in the data and so is ignored.\n\nobj2polyF <- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = c('scenario', 'env_obj', 'extra_grouper'),\n                             aggCols = ends_with('ewr_achieved'),\n                             funlist = ArithmeticMean,\n                             keepAllPolys = TRUE,\n                             failmissing = FALSE)\nobj2polyF\n\nSimple feature collection with 3939 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 138.5685 ymin: -37.68199 xmax: 152.4821 ymax: -24.5893\nGeodetic CRS:  GDA94\n# A tibble: 3,939 × 8\n   scenario env_obj polyID      spatial_ArithmeticMean…¹ SWSDLID SWSDL…² StateID\n * <chr>    <chr>   <chr>                          <dbl> <chr>   <chr>   <chr>  \n 1 base     EF1     r602ydft049                   0.571  SS16    Lachlan NSW    \n 2 base     EF1     r6367k2uy6m                   0.217  SS20    Macqua… NSW    \n 3 base     EF1     r6d2dwp48wm                   0.238  SS21    Namoi   NSW    \n 4 base     EF2     r602ydft049                   0.416  SS16    Lachlan NSW    \n 5 base     EF2     r6367k2uy6m                   0.219  SS20    Macqua… NSW    \n 6 base     EF2     r6d2dwp48wm                   0.227  SS21    Namoi   NSW    \n 7 base     EF3     r602ydft049                   0.0417 SS16    Lachlan NSW    \n 8 base     EF3     r6367k2uy6m                   0.128  SS20    Macqua… NSW    \n 9 base     EF3     r6d2dwp48wm                   0.182  SS21    Namoi   NSW    \n10 base     EF3a    r602ydft049                   0.506  SS16    Lachlan NSW    \n# … with 3,929 more rows, 1 more variable: geometry <MULTIPOLYGON [°]>, and\n#   abbreviated variable names\n#   ¹​spatial_ArithmeticMean_env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved,\n#   ²​SWSDLName\n\n\n\n\nFunctions\nWe can pass single bare aggregation function names, characters, or named lists defining functions with arguments. Above, we have been specifying the function to apply as just a single bare function name. Now, we explore some other possibilities and capabilities of the aggregator.\nMost simply, we can pass character names of functions instead of bare\n\ndoublesimplechar <- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = c('scenario', 'env_obj'),\n                             aggCols = 'ewr_achieved',\n                             funlist = 'ArithmeticMean',\n                             keepAllPolys = TRUE,\n                             failmissing = FALSE)\ndoublesimplechar\n\nSimple feature collection with 3939 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 138.5685 ymin: -37.68199 xmax: 152.4821 ymax: -24.5893\nGeodetic CRS:  GDA94\n# A tibble: 3,939 × 7\n   scenario env_obj polyID     SWSDLID SWSDL…¹ StateID                  geometry\n * <chr>    <chr>   <chr>      <chr>   <chr>   <chr>          <MULTIPOLYGON [°]>\n 1 base     EF1     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 2 base     EF1     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 3 base     EF1     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n 4 base     EF2     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 5 base     EF2     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 6 base     EF2     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n 7 base     EF3     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 8 base     EF3     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 9 base     EF3     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n10 base     EF3a    r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n# … with 3,929 more rows, and abbreviated variable name ¹​SWSDLName\n\n\nIf we want to do two different aggregations on the same data, we can pass a vector of names.\n\nsimplefuns <- c('ArithmeticMean', 'GeometricMean')\n\ndoublesimplec <- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = c('scenario', 'env_obj'),\n                             aggCols = 'ewr_achieved',\n                             funlist = simplefuns,\n                             keepAllPolys = TRUE,\n                             failmissing = FALSE)\ndoublesimplec\n\nSimple feature collection with 3939 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 138.5685 ymin: -37.68199 xmax: 152.4821 ymax: -24.5893\nGeodetic CRS:  GDA94\n# A tibble: 3,939 × 7\n   scenario env_obj polyID     SWSDLID SWSDL…¹ StateID                  geometry\n * <chr>    <chr>   <chr>      <chr>   <chr>   <chr>          <MULTIPOLYGON [°]>\n 1 base     EF1     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 2 base     EF1     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 3 base     EF1     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n 4 base     EF2     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 5 base     EF2     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 6 base     EF2     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n 7 base     EF3     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 8 base     EF3     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 9 base     EF3     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n10 base     EF3a    r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n# … with 3,929 more rows, and abbreviated variable name ¹​SWSDLName\n\n\nNote- if passing multiple functions, it does not work to pass bare names. It just gets too complex to handle, and the bare names is really just a convenience shorthand.\n\nArguments to aggregation functions\nThere are three primary ways to specify function arguments- using …, writing a wrapper function with the arguments specified (e.g. see ArithmeticMean, which is just mean(x, na.rm = TRUE) ), or using anonymous functions with ~ syntax in a named list. The simplest version is to use …, but this really only works in simple cases, like passing na.rm = TRUE. It does work for multiple functions, but starts getting convoluted and unclear if they don’t share arguments or there are many arguments.\n\nsinglearg <- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = 'scenario',\n                             aggCols = 'ewr_achieved',\n                 funlist = c(mean, sd),\n                 na.rm = TRUE,\n                 keepAllPolys = TRUE,\n                 failmissing = FALSE)\nsinglearg\n\nSimple feature collection with 87 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 138.5685 ymin: -37.68199 xmax: 152.4821 ymax: -24.5893\nGeodetic CRS:  GDA94\n# A tibble: 87 × 6\n   scenario polyID      SWSDLID SWSDLName      StateID                  geometry\n * <chr>    <chr>       <chr>   <chr>          <chr>          <MULTIPOLYGON [°]>\n 1 base     r602ydft049 SS16    Lachlan        NSW     (((146.4428 -32.33613, 1…\n 2 base     r6367k2uy6m SS20    Macquarie–Cas… NSW     (((147.4064 -30.14296, 1…\n 3 base     r6d2dwp48wm SS21    Namoi          NSW     (((148.3669 -29.78047, 1…\n 4 down4    r602ydft049 SS16    Lachlan        NSW     (((146.4428 -32.33613, 1…\n 5 down4    r6367k2uy6m SS20    Macquarie–Cas… NSW     (((147.4064 -30.14296, 1…\n 6 down4    r6d2dwp48wm SS21    Namoi          NSW     (((148.3669 -29.78047, 1…\n 7 up4      r602ydft049 SS16    Lachlan        NSW     (((146.4428 -32.33613, 1…\n 8 up4      r6367k2uy6m SS20    Macquarie–Cas… NSW     (((147.4064 -30.14296, 1…\n 9 up4      r6d2dwp48wm SS21    Namoi          NSW     (((148.3669 -29.78047, 1…\n10 base     r1gkz59ex7z SS10    South Austral… SA      (((140.9917 -32.26605, 1…\n# … with 77 more rows\n\n\nWe can also pass arguments by sending a list of functions with their arguments. This is far more flexible than the … approach, as we can send any arguments to any functions this way. For clarity, we demonstrate it here for the same situation- passing the na.rm argument to mean and sd. This also lets us control the function names, because the list-names do not need to match the function names. The list-names are what get used in history-tracking (see the column names).\n\nsimplelamfuns <- list(meanna = ~mean(., na.rm = TRUE), \n                     sdna = ~sd(., na.rm = TRUE))\n\ndoublelam <- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = c('scenario', 'env_obj'),\n                             aggCols = 'ewr_achieved',\n                             funlist = simplelamfuns,\n                             keepAllPolys = TRUE,\n                             failmissing = FALSE)\ndoublelam\n\nSimple feature collection with 3939 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 138.5685 ymin: -37.68199 xmax: 152.4821 ymax: -24.5893\nGeodetic CRS:  GDA94\n# A tibble: 3,939 × 7\n   scenario env_obj polyID     SWSDLID SWSDL…¹ StateID                  geometry\n * <chr>    <chr>   <chr>      <chr>   <chr>   <chr>          <MULTIPOLYGON [°]>\n 1 base     EF1     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 2 base     EF1     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 3 base     EF1     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n 4 base     EF2     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 5 base     EF2     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 6 base     EF2     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n 7 base     EF3     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 8 base     EF3     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 9 base     EF3     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n10 base     EF3a    r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n# … with 3,929 more rows, and abbreviated variable name ¹​SWSDLName\n\n\nNote: if using anonymous functions in a list this way, they need to use rlang ~ syntax, not base \\(x) or function(x){}. That’s on the to-do list, but it’s not much of a constraint, and anything complex can be written as a standard function and called that way.\nIt’s fairly common that we’ll have vector arguments, especially for the spatial aggregations. One primary example is weightings. The most flexible approach requires these vectors to be attached to the data before it enters the function (vs. creating them automatically in-function; though that is possible it gets fragile to handle arbitrary names and maintain the correct groupings). So, here we assume that the vectors will be columns in the dataset, and demonstrate with weighted means on dummy weights.\n\nveclamfuns <- list(meanna = ~mean(., na.rm = TRUE), \n                     sdna = ~sd(., na.rm = TRUE),\n                     wmna = ~weighted.mean(., wt, na.rm = TRUE))\n\n# Not really meaningful, but weight by the number of gauges.\nwtgauge <- simpleThemeAgg %>% \n  dplyr::group_by(scenario, gauge) %>% \n  dplyr::mutate(wt = dplyr::n()) %>% \n  dplyr::ungroup()\n\ntriplevec <- spatial_aggregate(dat = wtgauge, \n                             to_geo = sdl_units,\n                             groupers = c('scenario', 'env_obj'),\n                             aggCols = 'ewr_achieved',\n                             funlist = veclamfuns,\n                             keepAllPolys = TRUE,\n                             failmissing = FALSE)\n\ntriplevec\n\nSimple feature collection with 3939 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 138.5685 ymin: -37.68199 xmax: 152.4821 ymax: -24.5893\nGeodetic CRS:  GDA94\n# A tibble: 3,939 × 7\n   scenario env_obj polyID     SWSDLID SWSDL…¹ StateID                  geometry\n * <chr>    <chr>   <chr>      <chr>   <chr>   <chr>          <MULTIPOLYGON [°]>\n 1 base     EF1     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 2 base     EF1     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 3 base     EF1     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n 4 base     EF2     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 5 base     EF2     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 6 base     EF2     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n 7 base     EF3     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 8 base     EF3     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 9 base     EF3     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n10 base     EF3a    r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n# … with 3,929 more rows, and abbreviated variable name ¹​SWSDLName\n\n\nIf we want to have custom functions with vector data arguments, we still need to use the tilde notation to point to those arguments. Making a dummy function that just adds two to the weighted mean, the wt argument doesn’t get seen if we just say funlist = wt2. Instead, we need to use a list.\n\nwt2 <- function(x, wt) {\n  2+weighted.mean(x, w = wt, na.rm = TRUE)\n}\n\nwt2list <- list(wt2 = ~wt2(., wt))\n\n\nvecnamedfun <- spatial_aggregate(dat = wtgauge, \n                             to_geo = sdl_units,\n                             groupers = c('scenario', 'env_obj'),\n                             aggCols = 'ewr_achieved',\n                             funlist = wt2list,\n                             keepAllPolys = TRUE,\n                             failmissing = FALSE)\n\nvecnamedfun\n\nSimple feature collection with 3939 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 138.5685 ymin: -37.68199 xmax: 152.4821 ymax: -24.5893\nGeodetic CRS:  GDA94\n# A tibble: 3,939 × 7\n   scenario env_obj polyID     SWSDLID SWSDL…¹ StateID                  geometry\n * <chr>    <chr>   <chr>      <chr>   <chr>   <chr>          <MULTIPOLYGON [°]>\n 1 base     EF1     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 2 base     EF1     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 3 base     EF1     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n 4 base     EF2     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 5 base     EF2     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 6 base     EF2     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n 7 base     EF3     r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n 8 base     EF3     r6367k2uy… SS20    Macqua… NSW     (((147.4064 -30.14296, 1…\n 9 base     EF3     r6d2dwp48… SS21    Namoi   NSW     (((148.3669 -29.78047, 1…\n10 base     EF3a    r602ydft0… SS16    Lachlan NSW     (((146.4428 -32.33613, 1…\n# … with 3,929 more rows, and abbreviated variable name ¹​SWSDLName\n\n\n\n\nThe ‘area’ exception\nThe only exception to attaching vector arguments are situations where the needed vector arguments depend on both sets of from and to data/polygons, and so can’t be pre-attached. The main way this comes up is with area-weighting, so spatial_joiner calculates areas so there is always an area column available for weighting.\nIn summary, we can pass single functions and their arguments in ellipses, complex lists of multiple functions using tilde-style anonymous functions, which can have vector arguments (as long as the vector is attached to the data), and lists of multiple function names. Note, however, that while we can use bare names in spatial_aggregate and theme_aggregate, we can’t use bare function names in multi_aggregate because they get lost for the namehistory creation. The only thing we can’t do currently is pass unattached vector args. I have to do such convoluted things for that to work with one function, and it’s so easy to just bind them on, I think that’s a tradeoff worth making. We can reassess if this becomes an issue later."
  },
  {
    "objectID": "aggregator/flow_scaling_aggregation.html",
    "href": "aggregator/flow_scaling_aggregation.html",
    "title": "Flow scaling aggregation",
    "section": "",
    "text": "This will be the aggregation notebook for the flow scaling demonstration"
  },
  {
    "objectID": "aggregator/spatial_agg.html",
    "href": "aggregator/spatial_agg.html",
    "title": "Spatial aggregation",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)"
  },
  {
    "objectID": "aggregator/spatial_agg.html#overview",
    "href": "aggregator/spatial_agg.html#overview",
    "title": "Spatial aggregation",
    "section": "Overview",
    "text": "Overview\nWe will often have spatial data that we want to aggregate into larger scales. We therefore want a set of functions that allow us to read in data, specify the larger units into which it gets aggregated and the functions to use to do that aggregation. Further, there is clear need to handle grouping, most obviously for scenarios, but we also need to keep theme groupings separated during spatial aggregation steps.\nThere is a standalone spatial aggregator spatial_aggregate, which I demonstrate here, along with multi_aggregate, which wraps both spatial_aggregate and theme_aggregate to allow interleaved aggregaton steps in a standardised format. This document focuses on spatial aggregation, while theme aggregation and interleaved spatial and theme aggregation are demonstrated in separate notebooks, allowing us to dig a little deeper into how each component works.\nWe often will want to only perform a single spatial aggregation (e.g. from gauges to sdl units), but there are instances where that isn’t true- perhaps we want to aggregate from sdl units to states or the basin. Thus, I demonstrate multi-step spatial aggregation, including the situation where aggregation units (polygons) are not nested, as would be the case for sdl units and states, for example. Even if some of the steps in this demonstration aren’t particularly interesting now, they allow us to develop the general process that can accept any set of polygons we want to aggregate into from any other spatial data.\nThis document delves fairly in-depth into capabilities, including things like argument types and how they relate to other functions and permit certain tricks. Not all of these will be used or needed to understand by most users- typically there will be a set of aggregation steps fed to multi_aggregate and that will be that. This sort of simpler setup is shown in the combined aggregation notebook and the full-toolkit runs. But it is helpful to document them for when they are needed. See the syntax notebook for a detailed look at argument construction for various purposes. Here, we use that syntax to demonstrate how the spatial aggregation works and the different ways it can be done."
  },
  {
    "objectID": "aggregator/spatial_agg.html#inputs",
    "href": "aggregator/spatial_agg.html#inputs",
    "title": "Spatial aggregation",
    "section": "Inputs",
    "text": "Inputs\nWe will need to be able to accept inputs at arbitrary aggregation levels (theme, spatial, or temporal). In other words, the spatial aggregation should aggregate any input spatial data into any set of spatial units, whatever that input data represents. The multi_aggregate function runs without spatial info until it reaches a step calling for spatial aggregation, at which point that data must be spatial. Beyond this requirement, multi_aggregate doesn’t care what theme or spatial scale that input data is- e.g. we could give it Objectives already at the Catchment scale, and then use it to move up.\n\nNote: in some cases, the definitions for outcomes along the ‘Objective’ axis are defined spatially; for example, the definition of Specific_objectives might vary between planning units. However, these are the scale at which definitions of outcomes change, not the scale at which those outcomes must be assessed. For example, just because Specific_objectives are differently defined between planning units, we can still scale them up in space from gauge to basin, with no reference to planning unit.\n\nFor this demonstration, we start with gauge-referenced data at the env_obj theme scale."
  },
  {
    "objectID": "aggregator/spatial_agg.html#demonstration-setup",
    "href": "aggregator/spatial_agg.html#demonstration-setup",
    "title": "Spatial aggregation",
    "section": "Demonstration setup",
    "text": "Demonstration setup\nFirst, we need to provide a set of paths to point to the input data, in this case the outputs from the EWR tool for the small demonstration, created by a controller notebook. Spatial units could be any arbitrary polygons, but we use those provided by {werptoolkitr} for consistency, which also provides the spatial locations of the gauges in bom_basin_gauges.\n\nproject_dir <- file.path('scenario_example')\newr_results <- file.path(project_dir, 'module_output', 'EWR')\n\n\nTheme aggregated inputs\nThe multi_aggregate function can combine theme and spatial aggregation, but because I want this document to demonstrate spatial aggregation, I have split up the process to be clear what is happening. First, we do a theme aggregation to get to the desired Theme level to feed to spatial.\nBefore any aggregation, we need to read the data in and make it spatial (gauge2geo pairs gauge numbers with locations provided in the geopath argument inside prep_ewr_agg. Note that this prep step is wrapped in read_and_agg, reducing user input. I show it here so we can more clearly see what is happening.\n\nsumdat <- prep_ewr_agg(ewr_results, type = 'summary', geopath = bom_basin_gauges)\n\nDefine simple theme aggregation lists to get to env_obj level, assuming that any pass on ewr_code_timing yields a pass for ewr_code and ewr_codes are averaged into env_obj. More complexity for the theme aggregations are shown in the theme notebook.\n\nthemeseq <- list(c('ewr_code_timing', 'ewr_code'),\n               c('ewr_code', \"env_obj\"))\n\nfunseq <- list(c('CompensatingFactor'),\n               c('ArithmeticMean'))\n\nPerform that simple theme aggregation so we have some test data. Since edges are only relevant for theme aggregation, make them in the call. This and everything that follows could be done with interleaved theme and spatial sequences starting with themeseq and funseq fed to multi_aggregate, but here I split them apart to better accentuate the spatial aggregation.\n\nsimpleThemeAgg <- multi_aggregate(dat = sumdat,\n                         causal_edges = make_edges(causal_ewr, themeseq),\n                         groupers = c('scenario', 'gauge'),\n                         aggCols = 'ewr_achieved',\n                         aggsequence = themeseq,\n                         funsequence = funseq)\nsimpleThemeAgg\n\n\n\n  \n\n\n\nThis provides a spatially-referenced (to gauge) theme-aggregated tibble to use to demonstrate spatial aggregation. Note that this has the gauge (spatial unit), but also two groupings that we want to preserve when we spatially aggregate- scenario and the current level of theme grouping, env_obj.\n\n\nSpatial inputs (polygons)\nSpatial aggregation requires polygons to aggregate into, and we want the capability to do that several times. The user can read in any desired polygons with sf::read_sf(path/to/polygon.shp), but here we use those provided in the standard set with {werptoolkitr}. We’ll use SDL units, catchments (from cewo), and the basin to show how the aggregation can have multiple steps with polygons that may not be nested (though care should be taken when that is the case)."
  },
  {
    "objectID": "aggregator/spatial_agg.html#single-aggregation",
    "href": "aggregator/spatial_agg.html#single-aggregation",
    "title": "Spatial aggregation",
    "section": "Single aggregation",
    "text": "Single aggregation\nWe might just want to aggregate spatially once. We can do this simply by passing the input data (anything spatial, in this case simpleThemeAgg), a set of polygons, and providing a length-one funlist. In this simple case, we just use a bare function name, here the custom ArithmeticMean which is just a simple wrapper of mean with na.rm = TRUE. Any function can be passed this way, custom or in-built, provided it has a single argument. More complex situations are given below.\nNote that the aggCols argument is ends_with(original_name) to reference the original name of the column of values- it may have a long name tracking its aggregation history, so we give it the tidyselect ends_with to find the column. More generally, both aggCols and groupers can take any tidyselect syntax or bare names or characters.\n\nobj2poly <- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = 'scenario',\n                             aggCols = ends_with('ewr_achieved'),\n                             funlist = ArithmeticMean,\n                             keepAllPolys = TRUE)\nobj2poly\n\n\n\n  \n\n\n\nNote that that has a horribly long name tracking the aggregation history, and has lost the theme levels- e.g. the different env_objs are no longer there and were all averaged together. The multi_aggregate function automatically handles this preservation, but spatial_aggregate is more general, and does not make any assumptions about the grouping structure of the data. Thus, to keep the env_obj groupings (as we should, otherwise we’re inadvertently theme-aggregating over all of them), we need to add env_obj to the groupers argument.\n\nobj2poly <- spatial_aggregate(dat = simpleThemeAgg, \n                             to_geo = sdl_units,\n                             groupers = c('scenario', 'env_obj'),\n                             aggCols = ends_with('ewr_achieved'),\n                             funlist = ArithmeticMean,\n                             keepAllPolys = TRUE)\nobj2poly\n\n\n\n  \n\n\n\nA quick plot shows what we’re dealing with. We’ll simplify the names and choose a subset of the environmental objectives.\nThere are many built-in plotting options in the toolkit, which we will use shortly. First, though a quick ggplot to see what those standardised plot functions start with.\n\n# The name is horrible, so change it.\nobj2poly %>% \n  rename(ewr_achieved = spatial_ArithmeticMean_env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved) %>% \n  filter(grepl('^EF', env_obj)) %>% \nggplot(aes(fill = ewr_achieved)) +\n  geom_sf() + \n  facet_grid(scenario~env_obj) + \n  theme(legend.position = 'bottom')\n\n\n\n\nMoving forward, we’ll use the built-in plotting functions to keep consistent with the rest of the project.\n\nscene_pal <- make_pal(unique(simpleThemeAgg$scenario), palette = 'ggsci::nrc_npg', refvals = 'base', refcols = 'black')\n\n\nobj2poly %>% \n  rename(ewr_achieved = spatial_ArithmeticMean_env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved) %>% \n  filter(grepl('^EF[1-3]', env_obj)) %>% \nplot_outcomes(y_col = 'ewr_achieved',\n                  y_lab = 'Arithmetic Mean',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'env_obj',\n                          facet_row = 'scenario',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'))"
  },
  {
    "objectID": "aggregator/spatial_agg.html#multiple-spatial-levels",
    "href": "aggregator/spatial_agg.html#multiple-spatial-levels",
    "title": "Spatial aggregation",
    "section": "Multiple spatial levels",
    "text": "Multiple spatial levels\nThere are a number of polygon layers we might want to aggregated into in addition to SDL units, e.g. resource plan areas, hydrological catchments, or the whole basin. We can aggregate directly into them just as we have here for SDL units. However, we might also want to have several levels of spatial aggregation, which may be nested or nearly so, e.g. from SDL units to the basin, or may be nonnested, e.g. from SDL units to catchments. Typically, this would happen with intervening theme aggregations, as in the interleaved example. The aggregation process for multiple spatial levels is similar whether or not the smaller levels nest into the larger, but more care should be taken (and more explanation is needed) in the nonnested case.\nNote that there is an exception to the ‘vector arguments must be attached to the data’ rule, in that an area column is always created, making it available for things like area-weighted means.\nAggregating from SDL units to cewo valleys requires addressing issues of overlaps among the various polygons. As such, it makes a good test case that catches issues with the intersection of polygons that might not happen with a simpler set of polygons Figure 1 .\n\noverlay_cewo_sdl <- ggplot() +\n  geom_sf(data = sdl_units, aes(fill = SWSDLName), color = NA, alpha = 0.5) +\n  geom_sf(data = cewo_valleys, aes(color = ValleyName), fill = NA) +\n  theme(legend.position = 'none')\n\nvalleys <- ggplot() +\n  geom_sf(data = cewo_valleys, aes(color = ValleyName), fill = 'white') + \n  theme(legend.position = 'none')\n\nsdls <- ggplot() +\n  geom_sf(data = sdl_units, aes(fill = SWSDLName), alpha = 0.5) + \n  theme(legend.position = 'none')\n\nvalleys + sdls + overlay_cewo_sdl\n\n\n\n\nFigure 1: CEWO valleys (coloured lines) and SDL units (coloured fills) alone (a & b) and overlain (c), showing these are not nested, but instead are intersecting polygons.\n\n\n\n\nTo aggregate from one set of polygons into the other, we need to split them up and aggregate in a way that respects area and borders. In other words, if we have a polygon that lays across two of the next level up, we want to only include the bits that overlap into that next level up. Under the hood, we use sf::st_intersection, which splits the polygons to make a new set of nonoverlapping polygons. Then we can use these pieces to aggregate into the higher level. This aggregation should carefully consider area- things like means should be area-weighted, and things like sums, minima, and maxima should be thought about carefully- if the lower ‘from’ data are already sums, for example, an area weighting might make sense to get a proportion of the sum, but this is highly dependent on the particular sequence of aggregation. For other functions like minima and maxima, area-weighting may or may not be appropriate, and so careful attention should be paid to constructing the aggregation sequence and custom functions may be involved.\nThe intersection of sdl_units and cewo_valleys chops up sdl_units so there are unique polygons for each sdl unit - valley combination.\n\njoinpolys <- st_intersection(sdl_units, cewo_valleys)\njoinpolys\n\n\n\n  \n\n\n\nTo better see the many-to-many chopping we get with this particular pair of intersecting shapefiles, we can isolate an SDL unit (Victorian Murray) and see that it contains bits of 8 catchments. Likewise, the Loddon catchment contains bits of 4 SDL units Figure 2 .\n\nnvorig <- ggplot() +\n  geom_sf(data = dplyr::filter(sdl_units, SWSDLName == \"Victorian Murray\"))\n\nnvpostjoin <- ggplot() +\n  geom_sf(data = dplyr::filter(joinpolys, \n                        SWSDLName == \"Victorian Murray\"), \n          aes(fill = ValleyName))\n\navorig <- ggplot() +\n  geom_sf(data = dplyr::filter(cewo_valleys, ValleyName == 'Loddon'))\n\navpostjoin <- ggplot() +\n  geom_sf(data = dplyr::filter(joinpolys, \n                        ValleyName == 'Loddon'), \n          aes(fill = SWSDLName))\n\n(nvorig + nvpostjoin)/(avorig + avpostjoin)\n\n\n\n\nFigure 2: SDL units intersected with CEWO valleys and split to allow aggregation\n\n\n\n\nAs a simple example, we could again do a one-off aggregation from a polygon to another polygon using spatial_aggregate. Here, we could use the obj2poly aggregation into SDL units created above as the starting point.\n\nsimplepolypoly <- spatial_aggregate(dat = obj2poly, \n                 to_geo = cewo_valleys,\n                 groupers = c('scenario', 'env_obj'),\n                 aggCols = 'ewr_achieved',\n                 funlist = mean,\n                 na.rm = TRUE,\n                 keepAllPolys = TRUE,\n                 failmissing = FALSE)\nsimplepolypoly\n\n\n\n  \n\n\n\nAs with all sequential aggregations, this approach works but can be very ad-hoc and easy to forget theme-axis grouping. Instead, just like with interleaved theme and space, we can pass a list giving the aggregation sequence to multi_aggregate."
  },
  {
    "objectID": "aggregator/spatial_agg.html#passing-a-list",
    "href": "aggregator/spatial_agg.html#passing-a-list",
    "title": "Spatial aggregation",
    "section": "Passing a list",
    "text": "Passing a list\nWe can use multi_aggregate to aggregate through a list of polygon sets, here sdl units to cewo valleys to the basin.\nFirst, set up a list of the spatial aggregation steps, defined by the polygon sets and aggregation functions. As usual, multiple aggregation functions can happen at each stage, and they can be characters or named lists with arguments (the weighted mean needs to be a default). Now, we’re taking advantage of the auto-calculated area of each polygon chunk for the weighted mean.\n\nglist <- list(sdl_units = sdl_units, catchment = cewo_valleys, mdb = basin)\n\nfunlist <- list(c('ArithmeticMean', 'LimitingFactor'),\n                list(wm = ~weighted.mean(., area, na.rm = TRUE)),\n                list(wm = ~weighted.mean(., area, na.rm = TRUE)))\n\n\nmultispat <- multi_aggregate(dat = simpleThemeAgg,\n                         causal_edges = themeedges,\n                         groupers = c('scenario', 'env_obj'),\n                         aggCols = 'ewr_achieved',\n                         aggsequence = glist,\n                         funsequence = funlist)\n\nBy default, that returns only the final outcome, here the basin scale and with the aggregation history in column names so we are sure of what the values represent Figure 3 . That does not make a very readable legend, but we can rename it manually and tracking the meaning is very important.\n\nmultispat %>% \n  filter(grepl('^NF[1-3]', env_obj)) %>% \nggplot() +\n  geom_sf(aes(fill = mdb_wm_catchment_wm_sdl_units_ArithmeticMean_env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved)) +\n  facet_grid(env_obj~scenario) + theme(legend.position = 'bottom')\n\n\n\n\nFigure 3: Final basin-scale results of aggregating EWR achieved with arithmetic mean to sdl units, then weighted-mean aggregation of SDL units into CEWO valleys and weighted means of CEWO valleys to the Basin.\n\n\n\n\n\nSaveintermediate and namehistory\nWe might only want the final outcome as above, but we also might want all the steps (just as we did for the theme. Making the history in columns keeps the names easier to use, though it uses more memory. I am going to do something I shouldn’t here, and change the column ‘env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved’ to ‘ewr_achieved’. That’s for readability and because here we’re using namehistory = FALSE to track the history in columns. If we were truly doing this analysis, the theme-axis aggregations in simpleThemeAgg would be included in the aggsequence, and so those steps would also be handled with aggsequence columns.\n\nsimpleclean <- simpleThemeAgg %>% \n  rename(ewr_achieved = env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved)\n\nmultispatb <- multi_aggregate(dat = simpleclean,\n                              causal_edges = themeedges,\n                              groupers = c('scenario', 'env_obj'),\n                              aggCols = 'ewr_achieved',\n                              aggsequence = glist,\n                              funsequence = funlist,\n                              saveintermediate = TRUE,\n                              namehistory = FALSE)\n\nThat final list item is scenario * aggfun1*aggfun2*aggfun3*env_obj long, e.g. it has a row for each scenario and env_obj by aggregation function, and the aggregation functions are factorial- both the aggregations at step 1 are subsequently aggregated at each of the following steps.\n\nlength(unique(multispatb$mdb$scenario)) * \nlength(unique(multispatb$mdb$aggfun_1)) * \nlength(unique(multispatb$mdb$aggfun_2)) * \nlength(unique(multispatb$mdb$aggfun_3)) *\n  length(unique(multispatb$mdb$env_obj))\n\n[1] 276\n\nnrow(multispatb$mdb)\n\n[1] 276\n\n\nNow we can more easily analyse the output, but it’s bigger. The three spatial levels plus the input can all be mapped. We’ll cut it down to a single env_obj for clarity and use the comparer standard plots, with a common set of limits. We have to choose one of the aggfun_1 values.\n\nl1 <- multispatb$simpleclean %>% \n  filter(grepl('^NF1', env_obj)) %>% \n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'map',\n                colorgroups = NULL,\n                colorset = 'ewr_achieved',\n                pal_list = list('scico::berlin'),\n                facet_col = 'scenario',\n                facet_row = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down4', 'base', 'up4'),\n                underlay_list = 'basin',\n                setLimits = c(0,1))\n\nl2 <- multispatb$sdl_units %>% \n  filter(grepl('^NF1', env_obj) &\n           aggfun_1 == 'ArithmeticMean') %>% \n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'map',\n                colorgroups = NULL,\n                colorset = 'ewr_achieved',\n                pal_list = list('scico::berlin'),\n                facet_col = 'scenario',\n                facet_row = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down4', 'base', 'up4'),\n                underlay_list = 'basin',\n                setLimits = c(0,1))\n\nl3 <- multispatb$catchment %>% \n  filter(grepl('^NF1', env_obj) &\n           aggfun_1 == 'ArithmeticMean') %>% \n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'map',\n                colorgroups = NULL,\n                colorset = 'ewr_achieved',\n                pal_list = list('scico::berlin'),\n                facet_col = 'scenario',\n                facet_row = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down4', 'base', 'up4'),\n                underlay_list = 'basin',\n                setLimits = c(0,1))\n\nl4 <- multispatb$mdb %>% \n  filter(grepl('^NF1', env_obj) &\n           aggfun_1 == 'ArithmeticMean') %>% \n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'map',\n                colorgroups = NULL,\n                colorset = 'ewr_achieved',\n                pal_list = list('scico::berlin'),\n                facet_col = 'scenario',\n                facet_row = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down4', 'base', 'up4'),\n                underlay_list = 'basin',\n                setLimits = c(0,1))\n\n\nl1 / l2 / l3 / l4 + plot_layout(guides = 'collect')\n\n\n\n\nFigure 4: Each step in spatial aggregation from gauge to SDL unit to catchment to basin, using arithmetic means of EWR achieved\n\n\n\n\n\n\nfailmissing and keepallpolys\nAs above, we might want to ignore some groupers or aggregation columns, and we might want to keep polygons that don’t have data so the maps look better.\n\nmultispatextra <- multi_aggregate(dat = simpleclean,\n                                  causal_edges = themeedges,\n                                  groupers = c('scenario', 'env_obj',\n                                               'doesnotexist'),\n                                  aggCols = c('ewr_achieved', 'notindata'),\n                                  aggsequence = glist,\n                                  funsequence = funlist,\n                                  saveintermediate = TRUE,\n                                  namehistory = FALSE,\n                                  failmissing = FALSE,\n                                  keepAllPolys = TRUE)\n\nWe can see that the missing groupers and aggcols get ignored, while we have retained polygons without data. The default is keepAllPolys = FALSE, so the original multispatb only has relevant catchments while multispatextra has all of them. I’ve taken off the basin underlay from the plots above to make this clearer.\n\nkeepfalse <- multispatb$catchment %>% \n  filter(grepl('^NF1', env_obj) &\n           aggfun_1 == 'ArithmeticMean') %>% \n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'map',\n                colorgroups = NULL,\n                colorset = 'ewr_achieved',\n                pal_list = list('scico::berlin'),\n                facet_col = 'scenario',\n                facet_row = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down4', 'base', 'up4'),\n                setLimits = c(0,1))\n\nkeeptrue <- multispatextra$catchment %>% \n  filter(grepl('^NF1', env_obj) &\n           aggfun_1 == 'ArithmeticMean') %>% \n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'map',\n                colorgroups = NULL,\n                colorset = 'ewr_achieved',\n                pal_list = list('scico::berlin'),\n                facet_col = 'scenario',\n                facet_row = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down4', 'base', 'up4'),\n                setLimits = c(0,1))\n\n\nkeepfalse / keeptrue + plot_layout(guides = 'collect')\n\n\n\n\nIt would be possible to make the same plot by using the catchment polys themselves as underlay, and while the default (underlay_list = 'cewo_valleys' ) would yield white, we also have more control over colour too (not limited to the NA grey).\n\nkeepfalse_underlay <- multispatb$catchment %>% \n  filter(grepl('^NF1', env_obj) &\n           aggfun_1 == 'ArithmeticMean') %>% \n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'map',\n                colorgroups = NULL,\n                colorset = 'ewr_achieved',\n                pal_list = list('scico::berlin'),\n                facet_col = 'scenario',\n                facet_row = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down4', 'base', 'up4'),\n                underlay_list = list(underlay = 'cewo_valleys', \n                                     underlay_pal = 'cornsilk'),\n                setLimits = c(0,1))\nkeepfalse_underlay"
  },
  {
    "objectID": "aggregator/spatial_agg.html#using-multi_aggregate-for-one-aggregation",
    "href": "aggregator/spatial_agg.html#using-multi_aggregate-for-one-aggregation",
    "title": "Spatial aggregation",
    "section": "Using multi_aggregate for one aggregation",
    "text": "Using multi_aggregate for one aggregation\nIf we’re only using one level of spatial aggregation and nothing else, there’s typically no need for the multi_aggregate wrapper. That wrapper does work even for single steps though, and becomes almost essential for multi-step. We do have a bit less flexibility with how we specify arguments- aggsequence and funsequence need to be lists or characters (funsequence cannot be bare function names). Perhaps the biggest issue is that tidyselect in aggCols runs into issues because it gets used again inside multi_aggregate, and so tidyselect in the outer call collides with that. That could all be sorted out, but seems low priority- easier to just enforce characters for aggCols and lists or characters for the sequences.\nTypically we could use namehistory = FALSE to avoid the horrible long name with all the transforms in it, but there’s no way for it to know the previous aggregation history when it’s been done in pieces (as we say in the example above where I dangerously adjusted the name of simpleThemeAgg to make multispatb. A parsing function could handle this, but it’s better to just do it all on one go anyway so this is low priority.\nAs a quick example, here is a single-step spatial aggregation using multi_aggregate, not that we have slightly more restrictive specifications for groupers, aggCols, aggsequence and funsequence.\n\nobj2polyM1 <- multi_aggregate(simpleThemeAgg,\n                            causal_edges = themeedges,\n                            groupers = c('scenario', 'env_obj'), \n                         aggCols = 'ewr_achieved',\n                         aggsequence = list(sdl_units = sdl_units),\n                         funsequence = list(list(am = ~ArithmeticMean(.))),\n                         keepAllPolys = TRUE)\n\nobj2polyM1"
  },
  {
    "objectID": "aggregator/spatial_agg.html#next-steps",
    "href": "aggregator/spatial_agg.html#next-steps",
    "title": "Spatial aggregation",
    "section": "Next steps",
    "text": "Next steps\nThe examples here are designed to dig into capability of the spatial aggregator in fairly high detail. In typical use, we’d follow something more like the interleaved notebook, but this document hopefully provides valuable demonstrations of capability and potential for how each spatial step in that sequence might work and could be set up."
  },
  {
    "objectID": "aggregator/theme_agg.html",
    "href": "aggregator/theme_agg.html",
    "title": "Theme aggregation",
    "section": "",
    "text": "library(werptoolkitr)"
  },
  {
    "objectID": "aggregator/theme_agg.html#setting-aggregation-parameters",
    "href": "aggregator/theme_agg.html#setting-aggregation-parameters",
    "title": "Theme aggregation",
    "section": "Setting aggregation parameters",
    "text": "Setting aggregation parameters\nFor each step in the aggregation, we need to specify what levels we are aggregating from and to, the function to use to aggregate, and the mapping between the ‘from’ and ‘to’ levels.\nThe aggsequence list in multi_aggregate (and theme_aggregate) needs to be nested, e.g. links must be defined in causal_ewrs (or other causal mappings) from the ‘from’ and ‘to’ levels at each step. In other words, we can’t go backwards, and we can’t scale between levels with no defined relationship. However, this does not mean we have to always include every level. If a relationship exists, levels can be jumped (e.g. we could go straight from env_obj to target_20_year_2039), and indeed there may not be a defined ordering of some levels, and so it is perfectly reasonable to go from env_obj to both Objective and Target. For EWR outputs, the aggsequence list will typically need to start with ewr_code_timing and aggregate from there into ewr_code and env_obj as everything else flows from that.\nThe funsequence is a list instead of a simple vector because multiple functions can be used at each step. When multiple functions are passed, they are factorial (each function is calculated on the results of all previous aggregations). This keeps the history clean, and allows us to easily unpick the meaning of each value in the output.\nThese aggregation sequences and their associated functions are arguments to multi_aggregate. In practice, they will typically be set to a default value in a parameter file. There is a theme_aggregate function that performs a single-level of theme aggregation, much like spatial_aggregate does for space. I do not focus on that here, because single theme aggregations tend to be less complex than single spatial aggregations, and the capability to pass different sorts of arguments is discussed in space and the syntax notebook. Instead, here we focus on the sequential aggregation provided by multi_aggregate to understand theme aggregation, which is most interesting as a multi-step process along the causal network."
  },
  {
    "objectID": "aggregator/theme_agg.html#data-overview",
    "href": "aggregator/theme_agg.html#data-overview",
    "title": "Theme aggregation",
    "section": "Data overview",
    "text": "Data overview\nThe EWR results come in three main flavours- summary over the span of the run, annual, and all, a continuous set. At the time this was written, the all didn’t exist and annual was broken, so we focus here on the summary data, with updating to handle the others very high priority. We read them in with get_ewr_output also provides some cleanup. The wrapper function read_and_agg reads them in internally and then calls multi_aggregate to avoid having all objects for each scenario in memory. That’s what we’d do in production, typically, but here the goal is to see how the theme aggregation works. The get_ewr_output function has additional arguments to filter by gauge and scenario on read-in, allowing parallelisation without overloading memory.\nWe typically would use prep_ewr_agg (and this is wrapped by multi_aggregate) but that not only calls get_ewr_output, it then makes the data geographic. The geography doesn’t need to happen here since we’re just doing theme."
  },
  {
    "objectID": "aggregator/theme_agg.html#data",
    "href": "aggregator/theme_agg.html#data",
    "title": "Theme aggregation",
    "section": "Data",
    "text": "Data\nWe’ll pull in the summary data to use for demonstration so we can use multi_aggregate directly. If we want to feed a path instead of a dataframe, we would need to use read_and_agg.\n\nsumdat <- get_ewr_output(ewr_results, type = 'summary')\n# Would make it geographic:\n# sumdat <- prep_ewr_agg(ewr_results, type = 'summary', geopath = gpath)\nsumdat\n\n\n\n  \n\n\n\nThere’s an issue with outputType = 'annual' in the version of the EWR tool this was built with. Until I update and test the new EWR tool, skip the annual data.\n\nanndat <- get_ewr_output(ewr_results, type = 'annual')\nanndat\n\n\nNote: I originally wrote get_ewr_output to automatically get both the annual and summary ewr output files, but I think given a typical workflow it will make more sense to just wrap it if we want to more than one type of output. Even if we want to use multiple output types (summary, annual, all, etc), they won’t talk to each other for any of the processing steps, so might as well run in parallel.\n\nWe’ll choose an example gauge to make it easier to visualise the data.\n\n# Dubbo is '421001', has 24 EWRs\n# Warren Weir is '421004', has 30 EWRs. \nexample_gauge <- '421001'"
  },
  {
    "objectID": "aggregator/theme_agg.html#aggregation",
    "href": "aggregator/theme_agg.html#aggregation",
    "title": "Theme aggregation",
    "section": "Aggregation",
    "text": "Aggregation\nAs a demonstration, I’ve set a range of theme levels that hit all the theme relationship dataframes from the causal networks defined in causal_ewr, and set the aggregation functions fairly simply, but with two multi-aggregation steps to illustrate how that works. For more complexity in these aggregation functions, see the spatial notebook and aggregation syntax.\nI’m using CompensatingFactor as the aggregation function for the ewr_code_timing to ewr_code step here, assuming that passing either timing sub-code means the main code passes. A similar approach could be done if we want to lump the ewr_codes themselves, e.g. put EF4a,b,c,d into EF4. I use both ArithmeticMean and LimitingFactor for the 2nd and 3rd levels to demonstrate multiple aggregations and how the outputs from those steps get carried through subsequent steps.\n\naggseq <- list(c('ewr_code_timing', 'ewr_code'),\n               c('ewr_code', \"env_obj\"), \n             c('env_obj', \"Specific_goal\"), \n             c('Specific_goal', 'Objective'), \n             c('Objective', 'target_5_year_2024'))\n\nfunseq <- list(c('CompensatingFactor'),\n               c('ArithmeticMean', 'LimitingFactor'),\n             c('ArithmeticMean', 'LimitingFactor'),\n             c('ArithmeticMean'),\n             c('ArithmeticMean'))\n\nThe groupers and aggCols arguments can take a number of different formats- character vectors, bare column names and sometimes tidyselect, though this is more true in theme_aggregate and limited for multi_aggregate as discussed in the syntax documentation. This capability gives the user quite a few options for specifying the columns to use. The use of selectcreator makes it robust to nonexistent columns with failmissing = FALSE.\nTo create the aggregation, we provide the sequence lists created above, along with the causal links, defined by the causal_edges argument. Because the make_edges function also takes a sequence of node types, we can usually just call make_edges on the list of relationships and the desired set of theme levels. We can also just pass in causal_edges = causal_ewr (the list with all possible links), and theme_aggregate will auto-generate the edges it needs. That’s just a bit less efficient.\n\nsimpleThemeAgg <- multi_aggregate(dat = sumdat,\n                         causal_edges = make_edges(causal_ewr, aggseq),\n                         groupers = c('scenario', 'gauge'),\n                         aggCols = 'ewr_achieved',\n                         aggsequence = aggseq,\n                         funsequence = funseq)\nsimpleThemeAgg\n\n\n\n  \n\n\n\nThat output has 4 columns of output values because aggregation steps are factorial in the number of aggregation functions applied. The second step found the ArithmeticMean and LimitingFactor for ewr_achieved into env_obj and then the third step found the ArithmeticMean and LimitingFactor for each of those outcomes into Specific_goal. Each subsequent step only found the ArithmeticMean for each, and so the number of output columns stopped growing.\n\nTracking aggregation steps\nTracking aggregation steps is critical for knowing the meaning of the numbers produced. We can do that in two different ways- in column headers (names) or in columns themselves.\nTracking history in column names is unweildy, but describes exactly what the numbers are and is smaller in memory. For example, the last column is\n\nnames(simpleThemeAgg)[ncol(simpleThemeAgg)]\n\n[1] \"target_5_year_2024_ArithmeticMean_Objective_ArithmeticMean_Specific_goal_LimitingFactor_env_obj_LimitingFactor_ewr_code_CompensatingFactor_ewr_achieved\"\n\n\nThis says the values in this column are the 5-year targets, calculated as the arithmetic mean of Objectives, which were the arithmetic mean of Specific goals, which were calculated from env_obj as limiting factors, which were obtained from the ewr_code as limiting factors and those were calculated from the ewr_code_timing as compensating factors.\nIt may be easier to think about the meaning of the names from the other direction- ewr_achieved were aggregated from ewr_code_timing into ewr_code as Compensating Factors, then into env_obj as limiting factors- for the env_obj to pass, all ewr_codes contributing to it must pass. Then the env_objs were aggregated into Specific_goal, again as limiting factors, so to meet a goal, all contributing env_obj must pass. Those Specific_goals were then aggregated into Objectives with the arithmetic mean, so the value for an Objective is then the average of the contributing Specific_goals. Since in this example the Specific_goals will be either 1 or 0, this average gives the proportion of Specific_goals that are met for each Objective. Similarly, the 5-year targets were obtained by averaging the Objectives contributing to them.\nA different way to track the aggregations is possible by including them in columns instead of the names. This takes more memory, but can be clearer and makes subsequent uses easier in many cases. For memory purposes, I currently parse the names into columns post-hoc. When we develop the ability to do different aggregations on different groups within a node type, we will need to make the columns as we go (and we will have to use the column method to track, since different things will happen to different values in a column). This is very high priority.\nIn the example above, we can feed the output data to agg_names_to_cols to put the history in columns instead of names.\n\nagg_names_to_cols(simpleThemeAgg, aggsequence = aggseq, funsequence = funseq, aggCols = 'ewr_achieved')\n\n\n\n  \n\n\n\nIn practice, what makes most sense is to use a switch (namehistory = FALSE) inside multi_aggregate to return this format.\n\nsimpleColHistory <-  multi_aggregate(dat = sumdat,\n                         causal_edges = make_edges(causal_ewr, aggseq),\n                         groupers = c('scenario', 'gauge'),\n                         aggCols = 'ewr_achieved',\n                         aggsequence = aggseq,\n                         funsequence = funseq,\n                         namehistory = FALSE)\nsimpleColHistory"
  },
  {
    "objectID": "aggregator/theme_agg.html#arbitrary-input-data",
    "href": "aggregator/theme_agg.html#arbitrary-input-data",
    "title": "Theme aggregation",
    "section": "Arbitrary input data",
    "text": "Arbitrary input data\nThere are three main types of input EWR data (summary, annual, and all), but we expect there will be any number of input datasets once other modules exist. Provided the causal relationships are defined for the input sets, the specific input datasets and columns of data to aggregate are general. For example, we can feed it the annual data (type = 'annual') and aggregate two different columns (feed aggCols a vector of column names). In the case of the annual data, we might also want to group by year in addition to gauge and scenario, and so we add year to the groupers vector. We’ll keep the same sequences of aggregation and functions, noting that they now are calculated for both aggCols.\nTODO turn back on once update to new EWR without annual bug.\n\nannualColHistory <-  multi_aggregate(dat = anndat,\n                                     causal_edges = make_edges(causal_ewr,\n                                                               aggseq),\n                                     groupers = c('scenario', 'gauge', 'year'),\n                                     aggCols = c('num_events', 'event_length'),\n                                     aggsequence = aggseq,\n                                     funsequence = funseq,\n                                     namehistory = FALSE)\nannualColHistory"
  },
  {
    "objectID": "aggregator/theme_agg.html#returning-every-stage",
    "href": "aggregator/theme_agg.html#returning-every-stage",
    "title": "Theme aggregation",
    "section": "Returning every stage",
    "text": "Returning every stage\nBy default, we return only the final aggregation after stepping up the full sequence set by aggsequence. But in some cases, we might want to return all of the intermediate aggregations (e.g. at each step of aggsequence). This full aggregation sequence can be useful for testing and checking, but probably more importantly, allows more complete understanding of the results. For visualization, this allows each step to be fed as colour or other attributes to the causal network. To save all steps in the aggregation sequence, we pass saveintermediate = TRUE to multi_aggregate, and it returns a list of tibbles named by the aggregation level instead of a single final tibble. We cannot just attach the stage results as columns to a flat dataframe because the aggregation is many-to-many, and so the rows do not match and are not strictly nested. Thus, each stage needs its own dataframe to avoid duplicating or deleting data. Moreover, using the additional flexibility of a list of dataframes is necessary for interleaved aggregations across theme, space, and time axes.\n\nallsteps  <-  multi_aggregate(dat = sumdat,\n                         causal_edges = make_edges(causal_ewr, aggseq),\n                         groupers = c('scenario', 'gauge'),\n                         aggCols = 'ewr_achieved',\n                         aggsequence = aggseq,\n                         funsequence = funseq,\n                         saveintermediate = TRUE,\n                         namehistory = FALSE)\n\nnames(allsteps)\n\n[1] \"ewr_code_timing\"    \"ewr_code\"           \"env_obj\"           \n[4] \"Specific_goal\"      \"Objective\"          \"target_5_year_2024\"\n\n\n\nCausal plot\nBy returning values at each stage, we can map those to colour (and later size) in a causal network. In practice, this will happen in the Comparer (and the initial setup data arrangement will be made into a function there), but we can demonstrate it quickly here. Here, we map the values of the aggregation to node color. To do this, I’ll follow the usual causal_plots approach of making edges and nodes, and then use a join to attach the value to each node.\nTo keep this demonstration from becoming too unwieldy, we limit the edge creation to a single gauge, and so will filter the theme aggregations accordingly (or just rely on the join to drop). The ewr_node_timing outcomes are likely just confusing to include here, so we cut it off.\nThe first step is to generate the edges and nodes for the network we want to look at.\n\nedges <- make_edges(causal_ewr, \n                    fromtos = aggseq[2:length(aggseq)],\n                    gaugefilter = example_gauge)\n\nnodes <- make_nodes(edges)\n\nNow, extract the values we want from the aggregation and join them to the nodes.\nTODO this will all be done in a data prep function in the comparer that processes multi-step aggregation lists. This is high priority, but needs thought for how to handle interleaved aggregation along different axes.\n\n# need to grab the right set of aggregations if there are multiple at some stages\nwhichaggs <- c('CompensatingFactor',\n               'ArithmeticMean',\n               'ArithmeticMean',\n               'ArithmeticMean',\n               'ArithmeticMean')\n\n# What is the column that defines the value?\nvalcol <- 'ewr_achieved'\n\n# Get the values for each node\ntargetlevels <- names(allsteps)\ntargetlevels[1] <- 'ewr_code_timing'\naggvals <- extract_vals_causal(allsteps, whichaggs, valcol, \n                               targetlevels = targetlevels)\n\n# Cut off the ewr_code_timing- should really have a drop_step argument to just not return it? In the extract_vals_causal\naggvals <- aggvals %>% dplyr::filter(NodeType != 'ewr_code_timing')\n# cut to relevant gauge, then remove- causes problems since node levels above env_obj aren't gauge-referenced\naggvals <- aggvals %>% dplyr::filter(gauge == example_gauge) %>% \n  dplyr::select(-gauge)\n\n# join to the nodes\nnodes_with_vals <- dplyr::left_join(nodes, aggvals)\n\nMake the causal network plot with the nodes we chose and colour by the values we’ve just attached to them from the aggregation. At present, it is easiest to make separate plots per scenario or other grouping ( Figure 1 , Figure 2 ). For example, in the increased watering scenario, we see more light colours, and so better performance across the range of outcomes. Further network outputs are provided in the Comparer.\n\naggNetwork <- make_causal_plot(nodes = dplyr::filter(nodes_with_vals, \n                                        scenario == 'base'),\n                 edges = edges,\n                 edge_pal = 'black',\n                 node_pal = list(value = 'scico::tokyo'),\n                 node_colorset = 'ewr_achieved',\n                 render = FALSE)\n\nDiagrammeR::render_graph(aggNetwork)\n\n\n\n\n\nFigure 1: Causal network for baseline scenario at example gauge, coloured by proportion passing at each node, e.g. Arithmetic Means at every step. Light yellow is 1, dark purple is 0.\n\n\n\n\naggNetwork <- make_causal_plot(nodes = dplyr::filter(nodes_with_vals, \n                                        scenario == 'up4'),\n                 edges = edges,\n                 edge_pal = 'black',\n                 node_pal = list(value = 'scico::tokyo'),\n                 node_colorset = 'ewr_achieved',\n                 render = FALSE)\n\nDiagrammeR::render_graph(aggNetwork)\n\n\n\n\n\nFigure 2: Causal network for 4x scenario at example gauge, coloured by proportion passing at each node, e.g. Arithmetic Means at every step. Light yellow is 1, dark purple is 0."
  },
  {
    "objectID": "aggregator/theme_agg.html#user-set-functions",
    "href": "aggregator/theme_agg.html#user-set-functions",
    "title": "Theme aggregation",
    "section": "User-set functions",
    "text": "User-set functions\nWe have established a simple set of default aggregation functions (ArithmeticMean, GeometricMean, LimitingFactor, and CompensatingFactor), available in default_agg_functions.R. I expect that list to grow, but it is also possible to supply user-defined functions to include in funsequence. Previously, we have used this sort of approach for things like threshold functions. For example, we might want to know the mean event length, but only for events longer than 2 days for the ewr to objective aggregation, and thereafter scale up with ArithmeticMean. We can do this by specifying a new function and including it in the list given to funsequence .\n\nevent2 <- function(x) {\n  mean(ifelse(x > 2, x, NA), na.rm = TRUE)\n}\n\nnewfuns <- list(c('CompensatingFactor'),\n                  c('event2'),\n             c('event2'),\n             c('ArithmeticMean'),\n             c('ArithmeticMean'))\n\nThis is currently turned off until we update to the new EWR without a bug in the annual results.\n\nannualEv2 <-  multi_aggregate(dat = anndat,\n                         causal_edges = make_edges(causal_ewr, aggseq),\n                         groupers = c('scenario', 'gauge', 'year'),\n                         aggCols = 'event_length',\n                         aggsequence = aggseq,\n                         funsequence = newfuns,\n                         namehistory = FALSE)\n# lots of NaN because many years and locations didn't have events > 2, so for ease of viewing, filter\nannualEv2 %>% dplyr::filter(!is.nan(event_length))"
  },
  {
    "objectID": "aggregator/theme_agg.html#gauge-and-scenario--filtering",
    "href": "aggregator/theme_agg.html#gauge-and-scenario--filtering",
    "title": "Theme aggregation",
    "section": "Gauge and scenario -filtering",
    "text": "Gauge and scenario -filtering\nReading in all of the EWR results across all gauges and scenarios could be massive, depending on the spatial scale and the number of scenarios, and so we might want to parallelise over gauges or scenarios. We also might only be interested in some subset for things like plotting. To address this, get_ewr_output has gaugefilter and scenariofilter arguments. This will is particularly useful once we have lots of data that doesn’t fit in memory or want to parallel process - if we have all the data in memory already, we can just pipe it in through a filter (or filter the first argument), but if we read in in parallel from a path, we can greatly speed up processing.\nTo only read-in the relevant data, we use the read_and_agg wrapper. The gaugefilter argument only works (currently) if there are separate files for each gauge. Once we settle on a data format, I will re-write the gaugefilter differently to only read the desired gauge from the file, though that won’t be possible with interleaved spatial aggregation.\n\nsmallreadagg  <-  read_and_agg(datpath = ewr_results, type = 'summary',\n                               geopath = bom_basin_gauges,\n                               causalpath = causal_ewr,\n                               groupers = c('scenario', 'gauge'),\n                               aggCols = 'ewr_achieved',\n                               aggsequence = aggseq,\n                               funsequence = funseq,\n                               namehistory = FALSE, \n                               gaugefilter = NULL,\n                               scenariofilter = 'base')\n\ntable(smallreadagg$gauge, smallreadagg$scenario)\n\n        \n         base\n  412002  304\n  412004  304\n  412005  304\n  412011  304\n  412012  304\n  412016  208\n  412033  304\n  412038  304\n  412039  304\n  412046  208\n  412122   76\n  412124   76\n  412163  196\n  412188  200\n  412189  280\n  419001  280\n  419006  280\n  419007  280\n  419012  280\n  419015  280\n  419016  280\n  419020  280\n  419021  280\n  419022  280\n  419026  284\n  419027  280\n  419028  280\n  419032  280\n  419039  280\n  419045  280\n  419049  280\n  419091  284\n  420020    4\n  421001  192\n  421004  288\n  421011  280\n  421012  280\n  421019  208\n  421022  276\n  421023  276\n  421090  276\n  421146    4\n  422001  292\n  422028  292\n\n\nFor a one-off that fits in memory, this is slower than filtering the data after it’s in-memory, since the read-in happens first. The advantage comes when we don’t want (or can’t fit) all of the original data in memory, such as parallelisation over scenarios.\nThe read_and_agg function is also helpful if we just want to use paths as arguments instead of reading the data in and then calling multi_aggregate. In that case, we might not use any *filter arguments, in which case it works just like multi_aggregate, but takes paths instead of objects as arguments. For example, saving all intermediate and no filtering can be done with the path to data ewr_results.\n\nreadallsteps <- read_and_agg(datpath = ewr_results, type = 'summary',\n                             geopath = bom_basin_gauges,\n                             causalpath = causal_ewr,\n                             groupers = c('scenario', 'gauge'),\n                             aggCols = 'ewr_achieved',\n                             aggsequence = aggseq,\n                             funsequence = funseq,\n                             saveintermediate = TRUE,\n                             namehistory = FALSE)\n\nnames(readallsteps)\n\n[1] \"ewr_code_timing\"    \"ewr_code\"           \"env_obj\"           \n[4] \"Specific_goal\"      \"Objective\"          \"target_5_year_2024\"\n\n\n\nParallelisation\nThe gauge and scenario filtering gives an easy way to parallelise. I haven’t written this into a function yet until we settle on how to use Azure batching with the toolkit. It will likely involve a wrapper around read_and_agg, but could be incorporated as parameters fed to read_and_agg itself. We demo how it works here. Parallelisation will not only speed up the processing, but because we can do the data reads inside the function, parallelisation over scenarios (and gauges, if not spatially-aggregating) avoids reading all the data in at once and so reduces unnecessary memory use.\nFor this demonstration, we are getting the gauge and scenario lists from previously-read data, but in typical use they would be available from scenario metadata.\n::: {#future-export .border: .2px .solid .gray; .color: .gray} Note: future is supposed to handle the .export from the calling environment, and seems to do just fine with everything except the aggregation functions. That can happen with nested foreach inside functions, but I think here it might be happening because of the way we’re using {{}} to pass an arbitrary set of functions. Easy enough to fix, by passing something to .export, but annoying. If we end up not using {future} for parallelisation on Azure, this will be moot. :::\nThe example below performs the same processing as above to produce output identical to simpleThemeAgg, but done in parallel. This is slower for this small simple demonstration because of overhead, but has the potential to be much faster for larger jobs.\nWe’re not parallelizing over gauges here because we’re unlikely to be able to do so with interleaved aggregation steps, but a nested loop would work if we are only aggregating in time or theme dimensions.\n\nlibrary(foreach)\nlibrary(doFuture)\nregisterDoFuture()\nplan(multisession)\n\nallgauges <- 'all' # unique(simpleThemeAgg$gauge)\nallscenes <- unique(simpleThemeAgg$scenario)\n\nparThemeAgg <- foreach(s = allscenes, \n                       .combine = dplyr::bind_rows) %dopar% {\n  # If parallel over gauges\n  # foreach(g = allgauges, \n  #         .combine = dplyr::bind_rows) %dopar% {\n            \n    read_and_agg(datpath = ewr_results, type = 'summary',\n                 geopath = bom_basin_gauges,\n                 causalpath = causal_ewr,\n                 groupers = c('scenario', 'gauge'),\n                 aggCols = 'ewr_achieved',\n                 aggsequence = aggseq,\n                 funsequence = funseq,\n                 namehistory = TRUE, \n                 gaugefilter = NULL,\n                 scenariofilter = s)\n  }\n\nparThemeAgg\n\n\n\n  \n\n\n\nIf we’re doing something here that is too big to return the full output (likely in practice), it would also be straightforward for the parallel loop to save the iterations and not return anything. Then we could read the output in in pieces into the comparer."
  },
  {
    "objectID": "aggregator/theme_space_agg.html#overview",
    "href": "aggregator/theme_space_agg.html#overview",
    "title": "Aggregate Theme Space",
    "section": "Overview",
    "text": "Overview\nWe have theme aggregation and spatial aggregation shown separately for in-depth looks at their meaning and capability. Here, we focus on the typical use-case of interleaved aggregation along multiple dimensions. We do not get into all the different options and syntax as they are covered in those other documents.\nAll aggregation in {werptoolkitr} operate on the same core function and use similar principles- take a list of aggregation sequences, and aggregate each step according to a list of aggregation functions. Here, we show how multi_aggregate allos us to interleave the dimensions along which we aggregate, including auto-detecting which dimension we’re operating on (though that is fragile).\nFundamentally, multi_aggregate wraps theme_aggregate and spatial_aggregate with some data organisation and tracking of what the previous level of aggregation was to maintain proper grouping as they alternate. Both of those, in turn, wrap general_aggregate with some data arrangement specific to the dimension they aggregate along, such as stripping and re-adding geometry.\nFor inputs, multi_aggregate expects the incoming data to be in memory and geographic, and prefers (but does not require) the edges defining theme relationships to be already calculated. There is also a wrapper read_and_agg that takes paths as arguments and does the read-in of the data internally, finds the edges, and then runs multi_aggregate. This is often a useful approach, allowing parallelisation, better memory management, and it is far easier to use paths in a config file of arguments."
  },
  {
    "objectID": "aggregator/theme_space_agg.html#demonstration-setup",
    "href": "aggregator/theme_space_agg.html#demonstration-setup",
    "title": "Aggregate Theme Space",
    "section": "Demonstration setup",
    "text": "Demonstration setup\nFirst, we need to provide a set of paths to point to the input data, in this case the outputs from the EWR tool for the small demonstration, created by a controller notebook. Spatial units could be any arbitrary polygons, but we use those provided by {werptoolkitr} for consistency, which also provides the spatial locations of the gauges in bom_basin_gauges.\n\nproject_dir <- file.path('scenario_example')\newr_results <- file.path(project_dir, 'module_output', 'EWR')\n\nWhere do we want the outputs to go? The multi_aggregate function we focus on here takes R objects as inputs and returns a dataframe or a list back to the session. But in practice, we will wrap that with read_and_agg, which takes paths as inputs and can both return R objects to the session returnList = TRUE and save to a file if savepath = \"path/to/outfile\" . The aggregator_output directory is not broken into module subdirectories, since it is possible we will want to aggregate across modules at the highest levels.\n\nout_path <- file.path(project_dir, 'aggregator_output')\n\nScenario information\nThis will be attached to metadata, typically. For now, I’m just using it for diagnostic plots and the demonstration data is simple, so make it here.\n\nscenarios <- tibble::tibble(scenario = c('base', 'down4', 'up4'), delta = c(1, 0.25, 4))\n\nscene_pal <- make_pal(unique(scenarios$scenario), palette = 'ggsci::nrc_npg', refvals = 'base', refcols = 'black')"
  },
  {
    "objectID": "aggregator/theme_space_agg.html#data-prep",
    "href": "aggregator/theme_space_agg.html#data-prep",
    "title": "Aggregate Theme Space",
    "section": "Data prep",
    "text": "Data prep\nTo make the actual multi-aggregate loop general and only do one thing, dataprep needs to happen first (e.g. we don’t want to do EWR-specific dataprep on econ data). That said, we can use read_and_agg, which takes paths and the aggregation lists and runs the dataprep and aggregation functions. At present, the EWR tool is the only module, so we read it in and prep the data, including making it geographic with gauge locations.\n\newrdata <- prep_ewr_agg(ewr_results, type = 'summary', geopath = bom_basin_gauges)\n\nWe use causal_ewrs for causal relationships and spatial layers provided by {werptoolkitr} to define the aggregation units."
  },
  {
    "objectID": "aggregator/theme_space_agg.html#setup",
    "href": "aggregator/theme_space_agg.html#setup",
    "title": "Aggregate Theme Space",
    "section": "Setup",
    "text": "Setup\nFirst, we specify a simple interleaved aggregation sequence with only one aggregation function applied per step for simplicity. Note that theme-axis aggregation steps are specified with a character vector c('level_from', 'level_to'), while spatial aggregation steps are specified with an sf object (polygons). Allowing specification of spatial steps by character instead of object is high on the priority list. Here, we specify the aggregation function sequence with a mixture of character names for functions and list-defined anonymous functions, as discussed in the syntax notebook.\nNaming the list by target makes tracking and interpretation much easier, and is highly recommended.\nSpatial aggregation should almost always be area-weighted after the data is in polygons (see spatial notebook), though there are some aggregation functions where it doesn’t matter (e.g. max). All polygon data has an area column calculated automatically for this reason. The first aggregation into polygons doesn’t need to be area-weighted, because the thing being aggregated (typically at the gauge scale) doesn’t have area. After that, all data is in polygons and so has area. It is likely safest to always use weighted functions like weighted.mean though, since they default to even weights if none are given.\n\naggseq <- list(ewr_code = c('ewr_code_timing', 'ewr_code'),\n               env_obj =  c('ewr_code', \"env_obj\"),\n               sdl_units = sdl_units,\n               Specific_goal = c('env_obj', \"Specific_goal\"),\n               catchment = cewo_valleys,\n               Objective = c('Specific_goal', 'Objective'),\n               mdb = basin,\n               target_5_year_2024 = c('Objective', 'target_5_year_2024'))\n\n\nfunseq <- list(c('CompensatingFactor'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               list(wm = ~weighted.mean(., w = area, \n                                        na.rm = TRUE)),\n               c('ArithmeticMean'),\n               \n               list(wm = ~weighted.mean(., w = area, \n                                    na.rm = TRUE)),\n               c('ArithmeticMean'))\n\nThe multi_aggregate function needs the edges, so calculate them just for the theme sequence (dropping the spatial steps). This can also happen automatically in multi_aggregate and, most importantly, read_and_agg, allowing us to run the code only by specifying parameters and without this sort of intermediate processing.\n\nthemeseq <- aggseq[purrr::map_lgl(aggseq, is.character)]\newr_edges <- make_edges(dflist = causal_ewr, \n                         fromtos = themeseq)"
  },
  {
    "objectID": "aggregator/theme_space_agg.html#aggregate",
    "href": "aggregator/theme_space_agg.html#aggregate",
    "title": "Aggregate Theme Space",
    "section": "Aggregate",
    "text": "Aggregate\nNow we do the aggregation. Note that we have been very aggressive in handling spatial processing and so while spatial processing is slow, we minimize it as much as possible internally.\nReturn only final\n\ntsagg <- multi_aggregate(dat = ewrdata,\n                         causal_edges = ewr_edges,\n                         groupers = 'scenario',\n                         aggCols = 'ewr_achieved',\n                         aggsequence = aggseq,\n                         funsequence = funseq)\n\nThat saves only the final outcome, which is far cheaper for memory, but doesn’t say how we got that answer.\n\ntsagg\n\n\n\n  \n\n\n\nReturn all steps\nMost often, we’ll want to save the list of outcomes at each step- it allows us to see how we got the final outcome, and it’s likely we’re interested in the outcomes at more than one step anyway. As in the theme notebook, we do this with saveintermediate = TRUE. We also set namehistory = FALSE to put the aggregation tracking in columns instead of names for ease of handling the output.\n\nallagg <- multi_aggregate(dat = ewrdata,\n                         causal_edges = ewr_edges,\n                         groupers = 'scenario',\n                         aggCols = 'ewr_achieved',\n                         aggsequence = aggseq,\n                         funsequence = funseq,\n                         saveintermediate = TRUE,\n                         namehistory = FALSE,\n                         keepAllPolys = FALSE)\n\nNow, we’ll inspect each step, both as dataframes and maps. There are many other ways of plotting the outcome data available in the comparer. The goal here is simply to visualize what happens at each step along the way, so we make some quick maps.\nSheet 1- raw data from ewr\nThis is just the input data, so we don’t bother plotting it.\n\nallagg$ewr_code_timing\n\n\n\n  \n\n\n\nSheet 2- ewr_code\nThe first aggregated level is in sheet 2, and has the code_timings aggregated to ewr_code.\n\nallagg$ewr_code\n\n\n\n  \n\n\n\nThere are many EWR codes, so just pick three haphazardly (LF1, BF1, and CF) and plot to see that this data is at the gauge scale.\n\nallagg$ewr_code |>\n  dplyr::filter(ewr_code %in% c('LF1', 'BF1', 'CF')) %>%\n  dplyr::left_join(scenarios) %>% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'ewr_code',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'),\n                          underlay_list = list(underlay = sdl_units, \n                                               underlay_pal = 'cornsilk'))\n\n\n\n\nSheet 3- env_obj\nSheet three has now been aggregated to the env_obj on the theme scale, still gauges spatially.\n\nallagg$env_obj\n\n\n\n  \n\n\n\nAgain choosing three of the first codes, we see this is still gauged.\n\nallagg$env_obj |>\n  dplyr::filter(env_obj %in% c('EF1', 'WB1', 'NF1')) %>%\n  dplyr::left_join(scenarios) %>% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'env_obj',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'),\n                          underlay_list = list(underlay = sdl_units, \n                                               underlay_pal = 'cornsilk'))\n\n\n\n\nSheet 4- sdl_units\nThe fourth step is a spatial aggregation of env_obj theme-level data into sdl_units. This stays at the env_obj theme scale but aggregates the gauges into sdl units.\n\nallagg$sdl_units\n\n\n\n  \n\n\n\nNow we have aggregated the data above into sdl polygons.\n\nallagg$sdl_units |>\n  dplyr::filter(env_obj %in% c('EF1', 'WB1', 'NF1')) %>%\n  dplyr::left_join(scenarios) %>% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'env_obj',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'),\n                          underlay_list = list(underlay = sdl_units, \n                                               underlay_pal = 'grey90'))\n\n\n\n\nSheet 5- Specific goal\nSheet 5 is back to the theme axis, aggregating env_obj to Specific goal, remaining in SDL units.\n\nallagg$Specific_goal\n\n\n\n  \n\n\n\nUsing fct_reorder. this is where info about the scenarios would come in handy as reorder cols.\n\nallagg$Specific_goal |>\n  dplyr::filter(Specific_goal %in% c('All recorded fish species', \n                                     \"Spoonbills\", \n                                     \"Decompsition\"))  %>% \n  dplyr::left_join(scenarios) %>% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'Specific_goal',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'),\n                          underlay_list = list(underlay = sdl_units, \n                                               underlay_pal = 'grey90'))\n\n\n\n\nSheet 6- Catchment\nSheet 6 (aggregation step 5) remains at the Specific goal theme scale, and aggregates spatially from SDL unit into catchment (cewo_valleys). This is a bit of a contrived aggregation, since these are at similar spatial scales but represent different spatial groupings, but it is a good test of spatial aggregation of nonnested spatial units. This is explored in more detail in the spatial notebook.\n\nallagg$catchment\n\n\n\n  \n\n\n\nNow we can see that the aggregation has occurred into a different set of polygons. In practice, this we would likely aggregate gauge-scale data into either sdl_units or cewo_valleys, depending on the target, but this demonstrates the capability and flexibility of the multistage aggregations.\n\nallagg$catchment |>\n  dplyr::filter(Specific_goal %in% c('All recorded fish species', \n                                     \"Spoonbills\", \n                                     \"Decompsition\"))  %>% \n  dplyr::left_join(scenarios) %>% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'Specific_goal',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'),\n                          underlay_list = list(underlay = cewo_valleys, \n                                               underlay_pal = 'grey90'))\n\n\n\n\nSheet 7- Objective\nWe are now back to aggregation along the theme axis (from Specific goal to Objective), remaining in cewo_valleys.\n\nallagg$Objective\n\n\n\n  \n\n\n\nWe see that these are still in the catchments, but now the values are different Objectives.\n\nallagg$Objective |>\n  dplyr::filter(Objective %in% c('No loss of native fish species',\n                                 \"Increase total waterbird abundance across all functional groups\", \n                                     \"Support instream & floodplain productivity\"))  %>% \n  dplyr::left_join(scenarios) %>% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'Objective',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'),\n                          underlay_list = list(underlay = sdl_units, \n                                               underlay_pal = 'grey90'))\n\n\n\n\nSheet 8- Basin\nThis step is a spatial aggregation to the basin scale, with theme remaining at the Objective level. The scaling to the basin is area-weighted, so larger catchments count more toward the basin-scale outcome. Recognize that for this situation with data in only a subset of the basin, aggregation to the whole basin is fraught and is likely biased by missing data.\n\nallagg$mdb\n\n\n\n  \n\n\n\nWe drop the underlay on the plots since we’re now plotting the whole basin\n\nallagg$mdb |>\n  dplyr::filter(Objective %in% c('No loss of native fish species',\n                                 \"Increase total waterbird abundance across all functional groups\", \n                                     \"Support instream & floodplain productivity\"))  %>% \n  dplyr::left_join(scenarios) %>% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'Objective',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'))\n\n\n\n\nSheet 9- 5-year targets\nFinally, we aggregate along the theme axis to 5-year targets, remaining at the basin-scale spatialy\n\nallagg$target_5_year_2024\n\n\n\n  \n\n\n\nAnd we’re still at the basin, just plotting different outcomes.\n\nallagg$target_5_year_2024 |>\n  dplyr::filter(target_5_year_2024 %in% c('All known species detected annually', \n                                          \"Establish baseline data on the number and distribution of wetlands with breeding activity of flow-dependant frog species\",\n                                          \"Rates of fall does not exceed the 5th percentile of modelled natural rates during regulated water deliveries\"))  %>% \n  dplyr::left_join(scenarios) %>% \n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'map',\n                          colorgroups = NULL,\n                          colorset = 'ewr_achieved',\n                          pal_list = list('scico::berlin'),\n                          facet_col = 'scenario',\n                          facet_row = 'target_5_year_2024',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'))"
  },
  {
    "objectID": "aggregator/theme_space_agg.html#simple-inputs-and-saving",
    "href": "aggregator/theme_space_agg.html#simple-inputs-and-saving",
    "title": "Aggregate Theme Space",
    "section": "Simple inputs and saving",
    "text": "Simple inputs and saving\nIn practice, we often won’t call multi_aggregate directly, but will use read_and_agg to run multi_aggregate, since it automates data read-in and processing and saving. To do the same analysis as above but using read_and_agg, we give it the path to the data instead of the data itself. Note also that the geopath and causalpath arguments can be objects or paths; we use objects here because they are provided with the {werptoolkitr} package. We use returnList to return the output to the active session, and savepath to save an .rds file to out_path (but only if we’re rebuilding data).\nNote- to readRDS sf objects produced here, we need to have sf loaded in the reading script.\n\nif (REBUILD_DATA) {savep <- file.path(out_path)} else {savep <- NULL}\n\nts_from_raa <- read_and_agg(datpath = ewr_results, \n                            type = 'summary',\n                 geopath = bom_basin_gauges,\n                 causalpath = causal_ewr,\n                 groupers = c('scenario'),\n                 aggCols = 'ewr_achieved',\n                 aggsequence = aggseq,\n                 funsequence = funseq,\n                 namehistory = FALSE,\n                 saveintermediate = TRUE,\n                 returnList = TRUE,\n                 savepath = savep)\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nWe can see that that produces the same list as tsagg\n\nnames(tsagg)\n\n [1] \"scenario\"                                                                                                                                                                                            \n [2] \"polyID\"                                                                                                                                                                                              \n [3] \"target_5_year_2024\"                                                                                                                                                                                  \n [4] \"target_5_year_2024_ArithmeticMean_mdb_wm_Objective_ArithmeticMean_catchment_wm_Specific_goal_ArithmeticMean_sdl_units_ArithmeticMean_env_obj_ArithmeticMean_ewr_code_CompensatingFactor_ewr_achieved\"\n [5] \"OBJECTID\"                                                                                                                                                                                            \n [6] \"DDIV_NAME\"                                                                                                                                                                                           \n [7] \"AREA_HA\"                                                                                                                                                                                             \n [8] \"SHAPE_AREA\"                                                                                                                                                                                          \n [9] \"SHAPE_LEN\"                                                                                                                                                                                           \n[10] \"geometry\"                                                                                                                                                                                            \n\nnames(ts_from_raa)\n\n[1] \"ewr_code_timing\"    \"ewr_code\"           \"env_obj\"           \n[4] \"sdl_units\"          \"Specific_goal\"      \"catchment\"         \n[7] \"Objective\"          \"mdb\"                \"target_5_year_2024\""
  },
  {
    "objectID": "aggregator/theme_space_agg.html#parallelization",
    "href": "aggregator/theme_space_agg.html#parallelization",
    "title": "Aggregate Theme Space",
    "section": "Parallelization",
    "text": "Parallelization\nThe theme notebook, demonstrates potential parallelisation over gauges and scenarios from read-in onwards, which will likely be very useful once we’re dealing with real scenarios. Once spatial aggregation occurs, parallelisation over gauges doesn’t work, since their outcomes need to be aggregated together. That means in general, we are likely to run parallelisation over just scenarios, although there is certainly scope for clever chunking to allow parallelisation over space and time as well if that becomes necessary.\n\nNote: if we want to saveintermediate = TRUE, which we often do, we can’t .combine = bind_rows, but would need to save a list of lists and then bind_rows at each list-level post-hoc with purrr. I have not established that as a function yet, but it is high priority as we settle on data formats and batching workflows.\n\n\nlibrary(foreach)\nlibrary(doFuture)\n\nregisterDoFuture()\nplan(multisession)\n# plan(sequential) # debug\n\n# get these from elsewhere, the whole point is to not read everything in. Should be able to extract from the paths (or the scenario metadata - better)\nallscenes <- list.files(ewr_results, recursive = TRUE) %>% \n  dirname() %>% \n  dirname() %>% \n  unique()\n\n# I think no longer needed now we have a package\n# passfuns <- unique(unlist(funseq))\n# passfuns <- unlist(passfuns[purrr::map_lgl(passfuns, is.character)])\n\nparAgg <- foreach(s = allscenes,\n                       .combine = dplyr::bind_rows) %dopar% {\n    \n    read_and_agg(datpath = ewr_results, type = 'summary',\n                 geopath = bom_basin_gauges,\n                 causalpath = causal_ewr,\n                 groupers = c('scenario'),\n                 aggCols = 'ewr_achieved',\n                 aggsequence = aggseq,\n                 funsequence = funseq,\n                 namehistory = TRUE,\n                 saveintermediate = FALSE,\n                 scenariofilter = s)\n  }\n\nparAgg\n\n\n\n  \n\n\n\nThat output is the same as tsagg, but now it’s been read-in and processed in parallel over scenarios. As in the theme situation, this toy example is slower, but should yield large speedups for larger jobs."
  },
  {
    "objectID": "aggregator/theme_space_agg.html#next-steps",
    "href": "aggregator/theme_space_agg.html#next-steps",
    "title": "Aggregate Theme Space",
    "section": "Next steps",
    "text": "Next steps\nWe can now proceed to the comparer. We could use the tsagg list directly if we want to do all this processing in a single session, but what is more likely is that we’ll read the data produced by read_and_agg and saved at out_path to read in to the comparer, so we do not have to re-run the aggregator every time we use the comparer."
  },
  {
    "objectID": "causal_networks/causal_manipulation.html",
    "href": "causal_networks/causal_manipulation.html",
    "title": "Causal network functions",
    "section": "",
    "text": "Purpose\nThis document will cover the functions provided by {werptoolkitr} to interact with and manipulate causal networks.\nFor example\n\nmake_nodes\nmake_edges\nThe pruning function find_realted_nodes\nothers"
  },
  {
    "objectID": "causal_networks/causal_overview.html",
    "href": "causal_networks/causal_overview.html",
    "title": "Causal networks",
    "section": "",
    "text": "This needs to be fleshed out\n\nWhat are causal networks\nWhere do we get them\nWhat are they for\n\nUsed in comms\nun-black-boxing\nThey are the theme-scaling axis\n\n\nThis links to the notebooks for creation and manipulation and causal plots."
  },
  {
    "objectID": "causal_networks/causal_plots.html",
    "href": "causal_networks/causal_plots.html",
    "title": "Causal Network Plotting",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(dplyr)"
  },
  {
    "objectID": "causal_networks/causal_plots.html#get-the-network-relationships",
    "href": "causal_networks/causal_plots.html#get-the-network-relationships",
    "title": "Causal Network Plotting",
    "section": "Get the network relationships",
    "text": "Get the network relationships\nFirst we need the relationships defining the network, which are extracted from tables in {werptoolkitr} and provided as the dataset werptoolkitr::causal_ewrs."
  },
  {
    "objectID": "causal_networks/causal_plots.html#process-data-for-network-plots",
    "href": "causal_networks/causal_plots.html#process-data-for-network-plots",
    "title": "Causal Network Plotting",
    "section": "Process data for network plots",
    "text": "Process data for network plots\nCausal networks need an edges dataframe specifying all pairwise connections, and a nodes dataframe specifying the nodes and their attributes. We make those here (and not in the data cleaning stage) for a couple reasons.\n\nThe full set of possible links is massively factorial, and so we want to choose only those useful for the needs of a given analysis.\n\nAnalyses may differ depending on network detail, spatial resolution, or use in the toolkit outside the network (e.g. theme aggregation)\n\nA key feature of the network isn’t just the existence of connections, but their directionality, and so we want to specify that explicitly.\nWe want to have the general ability to make edges and nodes available for other uses, e.g. aggregations\n\n{werptoolkitr} exports make_edges and make_nodes, along with some other network-manipulation functions\n\n\nTo build the nodes and edges for a specific plot or set of plots, we first build a dataframe of edges, and then extract nodes.\n\nConstruct edge dataframe\nThe edges dataframe contains all pairwise links between nodes in from and to columns. To get that, we need to pass it dataframes specifying links. There will usually be multiple datasets, reflecting the differing scales of the nodes (as in the causal_ewrs list provided by {werptoolkitr}). These dataframes may include many columns of potential nodes, e.g. they might provide the mapping for several steps in the network. Thus, we need to provide the node columns we actually want to map and their directionality- what are the ‘from-to’ pairings. We may want to filter to only some subset of nodes; for example we may only be interested in the environmental objectives related to waterbirds. Further, we will likely want to filter by geography, currently possible by either gauge or planning unit (deprecated).\nAs an example, we can make the relationships present at gauge 409025 linking EWRs to environmental objectives, environmental objectives to specific goals, Specific goals to Targets, and environmental objectives to 5-year targets. There are many more possible connections to include in the fromtos, which to include will depend on the questions being asked. I’ve just chosen these for a quick demo.\n\nedges <- make_edges(dflist = causal_ewr, \n               fromtos = list(c('ewr_code', 'env_obj'), \n                              c('env_obj', 'Specific_goal'), \n                              c('Specific_goal', 'Target'), \n                              c('env_obj', 'target_5_year_2024')),\n               gaugefilter = '409025')\n\nedges\n\n\n\n  \n\n\n\nThere’s also the opportunity to filter the specific nodes to include with fromfilter and tofilter. This allows things like filtering the particular nodes within those node categories (e.g. env_objs related to waterbirds). However, it is typically better to use find_related_nodes after creation of the network, as that does network-aware filtering.\nAlthough we can specify defaults, this function is also reasonably generic and so can be used far beyond whatever defaults we set- it only depends on WERP-specific things in that the spatial filtering happens on gauge (and those are cross-referenced). For a particular set of analyses, we would typically set default fromtos lists, most relevantly in the Aggregator and Comparer. If we are producing plots for illustrating the network, there may be ad-hoc adjustments to that list.\n\n\nConstruct node dataframe\nThe node dataframe defines the ‘boxes’. The simplest way to make it is to extract it from the edges. Basically, we just grab all the nodes that are in either the from or to columns of the edges df. The make_nodes function does a bit more than just get unique node values from the edges df, it also attaches a column specifying the node order, reflecting their sequence in the causal network. There is a default sequence specified for WERP EWRs, but others can be specified with the typeorder argument. We expect that new default sequences will need to be created when new sorts of relationships come online.\n\nnodes <- make_nodes(edges)\n# look at that for demo\nnodes\n\n\n\n  \n\n\n\nWe can now create a minimal plot before digging back in to demo some options under the hood."
  },
  {
    "objectID": "causal_networks/causal_plots.html#node-relevant-network",
    "href": "causal_networks/causal_plots.html#node-relevant-network",
    "title": "Causal Network Plotting",
    "section": "Node-relevant network",
    "text": "Node-relevant network\nOne thing we’re fairly likely to want to do is ask about the connections that relate to a node or a small set of nodes. To do that, we need to be able to traverse the network upstream and downstream, using the focalnodes argument, which calls the find_related_nodes function. This is a more complete network restructuring than just filtering a target level, as we did above, because it traces the full network and only returns nodes at any level that relate to the targets. Now we can include the 5-year targets again because we’ve reduce the nodes. Note that the focalnodes don’t have to be related to each other or at the same level- find_related_nodes prunes the network to all connections involving all the focalnodes.\n\nmake_causal_plot(nodes, edges, \n                 focalnodes = c('NF4', 'Sloanes froglet'), render = FALSE) %>% \n  DiagrammeR::render_graph()"
  },
  {
    "objectID": "causal_networks/causal_plots.html#plot-setup--more-options-and-next-steps",
    "href": "causal_networks/causal_plots.html#plot-setup--more-options-and-next-steps",
    "title": "Causal Network Plotting",
    "section": "Plot setup- more options and next steps",
    "text": "Plot setup- more options and next steps\nThe above is running with defaults, but there’s quite a bit more capacity to change what is plotted and the look of the graphs. Primarily, I’ve focused on development related to how we’ll want to feed the outputs of the toolkit to the the network (e.g. shifts in the relationships or values of the nodes, such as fewer birds- see for example theme aggregation and overview presentation. That could be done with colour, node size, or edge penwidth.\nColour can be specified differently than is done by default above, such as coloring the nodes within the node groups by outcome, or assigning different node groups different color palettes, following the same idea as the generic colorgroups and colorset arguments in all the plotting functions. There’s an obvious step of making it interactive/dynamic, but that hasn’t been implemented yet.\n\nColour to indicate a value\n\nEdges\nEdges we want to be able to have colour in a column, as it would be if it came in as results from the toolkit. For example, we might want to colour the edges by change between scenarios, or strength of relationships. Down the track we could similarly alter penwidth as well.\nAs a demonstration, let’s add a value column to edges as a mock-up of the toolkit outputs and plot according to that. I’ll also use a continuous palette here rather than the default, since this is now a continuous variable. I’ll use a smaller network to make things visible. Note that above we removed the 5-year targets from the nodes df, and here we use edges since we’re already modifying it. Either approach can drop a set of nodes, though it’s generally easier to use the nodes since nodes can appear in either the from or to of the edges.\n\nedgewithvals <- edges %>% \n  filter(totype != 'target_5_year_2024') %>% \n  mutate(value = rnorm(n()))\n\nmake_causal_plot(nodes,\n                 edgewithvals,\n                 focalnodes = c('NF4', 'Sloanes froglet'),\n                 edge_pal = list(value = 'viridis::plasma'),\n                 edge_colorset = 'value', render = FALSE) %>% \n  DiagrammeR::render_graph()\n\n\n\n\n\n\n\nNodes (and single-colour edges)\nWe can also colour the nodes by results. Here I’ve also set the edges just to a single color - feeding edge_pal or node_pal a single character value specifying a colour or a character vector of length nrow of the relevant dataframe will just insert those values and bypass the palettes. Again, I start by dummying up some ‘toolkit results’ in a value column. Examples where these values do come out of EWR results is in the theme aggregation notebook and the overview presentation.\n\nnodewithvals <- nodes %>% \n  filter(NodeType != 'target_5_year_2024') %>% \n  mutate(value = rnorm(n()))\n\nmake_causal_plot(nodewithvals,\n                 edges,\n                 focalnodes = c('NF4', 'Sloanes froglet'),\n                 edge_pal = 'black',\n                 node_pal = list(value = 'scico::oslo'),\n                 node_colorset = 'value', render = FALSE) %>% \n  DiagrammeR::render_graph()\n\n\n\n\n\n\n\n\nColour within node groups\nWe might want to use different colour palettes within the different node groups, but colour the nodes themselves within them. To do that, we set the *_pal arguments as named lists of palettes, and also pass *_colorgroups arguments so it knows how to split the data into those palettes. This parallels the use in plot_outcomes for other plot types, where colorgroups is the groups that get the palette, while colorset are the individual units within each group that receive colors from the respective palette. Here, we demonstrate with nodes and plot the whole network so we can see what’s happening.\nFirst, set the list of palettes. This could be set by default for the project, along with other default colors.\n\nnode_list_c = list(ewr_code = 'viridis::mako', \n                   env_obj = 'viridis::plasma', \n                   Specific_goal = 'scico::oslo', \n                   Target = 'scico::hawaii', \n                   target_5_year_2024 = 'scico::lisbon')\n\nThen, make the network\n\nmake_causal_plot(nodes, edges, \n                 edge_pal = 'black',\n                 node_pal = node_list_c,\n                 node_colorgroups = 'NodeType',\n                 node_colorset = 'Name',render = FALSE) %>% \n  DiagrammeR::render_graph()\n\n\n\n\n\n\n\nGroupings within node groups\nIt’s possible but gets rapidly bespoke to break those up into more discrete chunks. I’ve had a crude go at establishing a default though, where I grouped EWRs and environmental objectives by their main group, and lumped all the targets by year (though only including 5-year in this example). That default can be accessed by passing 'werp' to as the node_colorset. This is an end-run that builds a new column to use as colorset, defined according to some defaults. The same thing could be done externally by creating a new column to define the colorset outside the function and then making that column colorset.\nIn that case, we might want to use a different set of palettes,\n\nnode_list_g <- list(ewr_code = 'viridis::mako', \n                    env_obj = 'ggthemes::excel_Green', \n                    Specific_goal = 'scico::oslo', \n                    Target = 'calecopal::superbloom3', \n                    target_5_year_2024 = 'calecopal::eschscholzia')\n\nThis lets us set the within-node_colorgroups palettes. The node_colorset = 'werp' just creates a new column in the data that gives rows values we want (e.g. the first two letters of the ewr_codes), which are then used to choose colors from that particular palette. It’s a way to have fewer colors (and more meaningful colors) within the colorgroups.\n\nmake_causal_plot(nodes, edges, \n                 edge_pal = 'black',\n                 node_pal = node_list_g,\n                 node_colorgroups = 'NodeType',\n                 node_colorset = 'werp', render = FALSE) %>% \n  DiagrammeR::render_graph()"
  },
  {
    "objectID": "causal_networks/causal_plots.html#directions-from-here",
    "href": "causal_networks/causal_plots.html#directions-from-here",
    "title": "Causal Network Plotting",
    "section": "Directions from here",
    "text": "Directions from here\nThere’s clearly a lot more that could be done here. The nature of the output and the way I’ve set it up are really aimed at being interactive and usable to investigate the network.\nThe setup here is actually quite similar to the other plot outputs. In all cases, there is much opportunity to adjust the look of the plots to target different uses, but also the ability to establish a consistent default (and look). These plots lend themselves to both notebooks (as here), web, and interactive interfaces (Shiny, observablejs, etc) to investigate the networks themselves or use them to plot results. That gives flexibility to get at whatever the particular question is. Though causal networks are not a typical way to present outputs, here we see that they can be incredibly powerful for not only showing the relationships, but understanding how they change under different scenarios."
  },
  {
    "objectID": "comparer/bar_plots.html",
    "href": "comparer/bar_plots.html",
    "title": "Bar plots",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)"
  },
  {
    "objectID": "comparer/bar_plots.html#scenario-information",
    "href": "comparer/bar_plots.html#scenario-information",
    "title": "Bar plots",
    "section": "Scenario information",
    "text": "Scenario information\nThis will be attached to metadata, typically. For now, I’m just using it for diagnostic plots and the demonstration data is simple, so make it here.\n\nscenarios <- tibble::tibble(scenario = c('base', 'down4', 'up4'), delta = c(1, 0.25, 4))"
  },
  {
    "objectID": "comparer/bar_plots.html#subset-for-demo",
    "href": "comparer/bar_plots.html#subset-for-demo",
    "title": "Bar plots",
    "section": "Subset for demo",
    "text": "Subset for demo\nWe have a lot of hydrographs, so for this demonstration, we will often use a subset.\n\ngauges_to_plot <- c('412002', '419001', '422028', '421001')"
  },
  {
    "objectID": "comparer/bar_plots.html#choosing-example-data",
    "href": "comparer/bar_plots.html#choosing-example-data",
    "title": "Bar plots",
    "section": "Choosing example data",
    "text": "Choosing example data\nFirst, we read in the aggregated data and make a simple demonstration bar plot. There is example data provided by the toolkit (agg_theme_space and agg_theme_space_colsequence), but to continue with the demonstration, we will use the aggregations created here in the interleaved aggregation notebook.\nNote- to readRDS sf objects, we need to have sf loaded.\n\nagged_data <- readRDS(file.path(agg_dir, 'summary_aggregated.rds'))\n\nThat has all the steps in the aggregation, so we’ll choose one (the Objective theme scale at the basin spatial scale, agged_data$mdb) for the first set of plots and another (agged_data$sdl_units at the SDL unit scale and env_obj theme scale for the second set of plots. This finer scale lets us look at complicating factors like multiple spatial units and grouping outcomes.\nTo make these examples more easily, we create some slightly simpler dataframes here for those examples, but this isn’t really necessary- small data manipulations are easily piped in to plot_outcomes. The basin-scale needs a bit of cleanup because Objectives (and many of the other categories other than codes, e.g. yearly targets) are really long. We could fold them in the facet labels with ggplot2::label_wrap_gen(), but they’re so long it blocks out the plots. Ideally, we would use descriptive short names for each, but that’s a large manual job to assign them. For this demonstration, I’ll just cut them off and make them unique, but we need a better solution. The SDL units data is given a grouping column that puts the many env_obj variables in groups defined by their first two letters, e.g. EF for Ecosystem Function.\nIf we had used multiple aggregation functions at any step, we should filter down to the one we want here, but we only used one for this example.\n\n# make the super long names shorter but less useful.\nbasin_to_plot <- agged_data$mdb %>% \n  dplyr::filter(!is.na(Objective)) %>% \n  dplyr::mutate(Objective = stringr::str_trunc(Objective, 15)) %>% \n  dplyr::group_by(scenario, Objective) %>% \n  dplyr::mutate(id = as.character(row_number())) %>% \n  dplyr::ungroup() %>% \n  dplyr::mutate(Objective = stringr::str_c(Objective, '_', id)) %>% \n  dplyr::select(-id)\n\n# Create a grouping variable\nobj_sdl_to_plot <- agged_data$sdl_units |>\n  dplyr::mutate(env_group = stringr::str_extract(env_obj, '^[A-Z]+')) |>\n  dplyr::filter(!is.na(env_group)) |>\n  dplyr::arrange(env_group, env_obj)"
  },
  {
    "objectID": "comparer/bar_plots.html#scenario-fills",
    "href": "comparer/bar_plots.html#scenario-fills",
    "title": "Bar plots",
    "section": "Scenario fills",
    "text": "Scenario fills\n\nBasin scale\nWe can make plots looking at how scenarios differ for each of the outcome categories. This uses facet_wrapper to just wrap the single facet axis, looking at the basin scale and Objectives first. If we had more than one spatial unit, we would need to either filter to a target unit or facet by them. As with {ggplot2} itself, we tend to use facet_wrap for single-variable facetting and facet_row and facet_grid for specifying rows and columns, though here we use facet_row and facet_col to feed facet_grid. Plots at the basin scale are the simplest because we don’t have to worry about different bars for different spatial units.\nThe colorset argument is the column that determines color, while the pal_list defines those colors, here as a named colors object, but as we see below it can also be palette names. The sceneorder argument lets us explicitly set the order of the scenarios. This is typically easiest to have a Factor object with the scenarios and their orders, as here. But we can also just use a character vector (demonstrated later).\n\nplot_outcomes(basin_to_plot, \n                 y_col = 'ewr_achieved', \n                 facet_wrapper = 'Objective', \n                 colorset = 'scenario',\n                 pal_list = scene_pal,\n                 sceneorder = sceneorder)\n\n\n\n\nWe retain the axis names as-is from the incoming dataframe, as they provide the true meaning of each value. But we can change them, either inside the plot_outcomes function or post-hoc. We can also set the sceneorder with a character vector if that’s easier than setting up a Factor.\n\nplot_outcomes(basin_to_plot, \n                 y_col = 'ewr_achieved', \n              y_lab = 'Proportion Objectives\\nAchieved',\n              x_lab = 'Scenario',\n              color_lab = 'Scenario',\n                 facet_wrapper = 'Objective', \n                 colorset = 'scenario',\n                 pal_list = scene_pal,\n                 sceneorder = c('down4', 'base', 'up4'))\n\n\n\n\nBecause these are just ggplot objects, we can also change the labels outside the function, which can be very useful for checking that each axis is in fact what we think it is before giving it clean labels.\n\nscenebar <- plot_outcomes(basin_to_plot, \n                 y_col = 'ewr_achieved',\n                 facet_wrapper = 'Objective', \n                 colorset = 'scenario',\n                 pal_list = scene_pal,\n                 sceneorder = sceneorder)\n\nscenebar + labs(x = NULL, y = 'Proportion\\nObjectives Achieved')\n\n\n\n\nAnother approach is to put the outcomes on the x-axis, and color by scenario. This requires using the special scene_pal argument currently instead of pal_list. This is a bit of a historical holdover and will be made more general.\n\nplot_outcomes(basin_to_plot, \n                 y_col = 'ewr_achieved',\n                 x_col = 'Objective',\n                 colorset = 'scenario',\n                 scene_pal = scene_pal,\n                 sceneorder = sceneorder)\n\n\n\n\nUsing dodged bars can allow clearer comparisons, particularly accentuating the variation in sensitivity of the different outcomes to the scenarios, though there are a lot of bars to try to read here.\n\nplot_outcomes(basin_to_plot, \n                 y_col = 'ewr_achieved',\n                 x_col = 'Objective',\n                 colorset = 'scenario',\n                 scene_pal = scene_pal,\n                 sceneorder = sceneorder,\n              position = 'dodge')\n\n\n\n\n\n\nSDL units\nWe can use the aggregation step of env_obj and SDL units to demonstrate plotting that not only addresses the outcomes for scenarios, but how they differ across space. I’ll often use subsets of the env_obj codes here to keep the number of plots visible.\nFirst, we look at how the different scenarios perform for the Ecosystem Function objectives in each SDL unit\n\nobj_sdl_to_plot %>% \n  filter(grepl('^EF', env_obj)) %>% \n  plot_outcomes(y_col = 'ewr_achieved', \n                facet_col = 'env_obj',\n                facet_row = \"SWSDLName\",\n                colorset = 'scenario',\n                pal_list = scene_pal,\n                sceneorder = sceneorder)\n\n\n\n\nWe address a few ways to handle groups of outcome variables, one of the simplest is to simply facet these plots by those groups, with all the outcomes in the group getting their own bars. This puts the objectives on x and colors by scenario, with the groups accentuated by facets. These can be stacked (position = 'stack'- the default) or dodged (demonstrated here).\n\ndodgefacet <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'env_obj',\n                          colorset = 'scenario',\n                          facet_row = 'SWSDLName',\n                          facet_col = 'env_group',\n                          scales = 'free_x',\n                          scene_pal = scene_pal,\n                          sceneorder = sceneorder,\n                          position = 'dodge')\n\ndodgefacet + theme(legend.position = 'bottom') + \n  labs(x = 'Environmental Objective')"
  },
  {
    "objectID": "comparer/bar_plots.html#grouped-colors",
    "href": "comparer/bar_plots.html#grouped-colors",
    "title": "Bar plots",
    "section": "Grouped colors",
    "text": "Grouped colors\nWe have the ability to assign different color palettes to different sets of outcomes, yielding what is essentially another axis on which we can plot information. We use this same ability across a number of plot types, particularly causal networks. For example, we might categorize the env_obj outcomes into the larger scale groups (e.g. ‘NF’, ‘EF’, etc). We can then assign each of these a separate palette, and so the individual env_objs get different colors chosen from different palettes.\nAchieving this requires specifying two columns- the colorset, as above, is the column that determines color. The colorgroups column specifies the groupings of those colorset values, and so what palette to use. Thus, the pal_list needs to be either length 1 (everything gets the same palette) or length(unique(data$colorgroups)). Note also that the colorset values must be unique to colorgroups- this cannot be a one-to-many mapping because each colorset value must get a color from a single palette defined by the colorgroup it is in.\nWe demonstrate with env_obj variables mapped to larger environmental groups, making it easier to see at a glance the sorts of environmental objectives that are more or less affected, while also allowing views of the individual environmental objectives. Here we use facet_col and facet_row to ensure the SDL units don’t wrap around. We made the env_groups column when we chose the data initially.\n\n# Create a palette list\nenv_pals = list(EF = 'grDevices::Purp',\n                NF = 'grDevices::Mint',\n                NV = 'grDevices::Burg',\n                OS = 'grDevices::Blues',\n                WB = 'grDevices::Peach')\n\n# need to facet by space sdl unit and give it the colorgroup argument to take multiple palettes\nsdl_stack <- obj_sdl_to_plot |>\n  plot_outcomes(y_col = 'ewr_achieved',\n                colorgroups = 'env_group',\n                colorset = 'env_obj',\n                pal_list = env_pals,\n                facet_col = 'SWSDLName',\n                facet_row = '.',\n                sceneorder = sceneorder)\nsdl_stack\n\n\n\n\nAdding facetting by those groups can make that easier to read if the goal is to focus on changes within groups, but more plots.\n\nobj_sdl_to_plot |>\n  plot_outcomes(y_col = 'ewr_achieved',\n                colorgroups = 'env_group',\n                colorset = 'env_obj',\n                pal_list = env_pals,\n                facet_col = 'SWSDLName',\n                facet_row = 'env_group',\n                sceneorder = sceneorder)\n\n\n\n\nWe could also split those bars sideways instead of stack them, but that likely makes more sense if there are fewer categories than here. We again use position = 'dodge', but now we don’t need to sum because we’re stacking each row already. I’ve flipped the facetting and taken advantage of the fact that these are just ggplot objects to remove the legend, making it very slightly easier to read (but harder to interpret).\n\nobj_sdl_to_plot |>\n  plot_outcomes(y_col = 'ewr_achieved',\n                colorgroups = 'env_group',\n                colorset = 'env_obj',\n                pal_list = env_pals,\n                facet_col = 'env_group',\n                facet_row = 'SWSDLName',\n                sceneorder = sceneorder,\n                position = 'dodge') +\n  theme(legend.position = 'none')\n\n\n\n\nAnother approach to groups of outcomes without the colors explicitly grouped is to not use colorgroup, but instead just facet by the group and give every colorset value a color from the same palette. Depending on the palette chosen and the breaks, this can be quicker, but will not accentuate groups as well.\n\nobj_sdl_to_plot |>\n    plot_outcomes(y_col = 'ewr_achieved',\n                          colorgroups = NULL,\n                          colorset = 'env_obj',\n                          pal_list = list('scico::berlin'),\n                          facet_row = 'SWSDLName',\n                          facet_col = 'env_group',\n                          scales = 'free_x',\n                          scene_x = FALSE,\n                          scene_pal = scene_pal,\n                          sceneorder = sceneorder) \n\n\n\n\nThese plots are interesting, but in typical use, the plots above using facets for the groups or coloring by the groups themselves are likely to be easier to read, unless we really are interested in this level of granularity. Whatever approach we choose for a given plot, accentuating the differences between outcome groups can be a powerful interpretation tool."
  },
  {
    "objectID": "comparer/bar_plots.html#manual-color-definition",
    "href": "comparer/bar_plots.html#manual-color-definition",
    "title": "Bar plots",
    "section": "Manual color definition",
    "text": "Manual color definition\nThough the above examples using {paletteer} palettes are the easiest way to specify coloring, we don’t have to let the palettes auto-choose colors, and can instead pass colors objects, just as we do for scenarios. This can be particularly useful with small numbers of groups (defining too many colors is cumbersome- that’s what palettes are for) when we want to control which is which. Just as with scenarios, we use make_pal . Here, we will use ‘scico::berlin’ as the base, but define several ‘reference’ values manually. This demonstration uses includeRef = TRUE so we replace the palette values with the refs, rather than choose them from the set of values with refs removed. This tends to yield better spread of colors (and lets us sometimes ref colors and sometimes not if we also used returnUnref). For example, maybe we want to sometimes really accentuate ecosystem function and native vegetation, but not in all plots.\nFirst, we create the palettes with and without the (garish) ref values.\n\nobj_pal <- make_pal(levels = unique(obj_sdl_to_plot$env_group),\n                      palette = 'scico::lisbon',\n                    refvals = c('EF', 'NV'), refcols = c('purple', 'orange'), includeRef = TRUE, returnUnref = TRUE)\n\nThen we can create an accentuated plot sometimes, if, perhaps, we want to highlight how EF performed.\n\nplot_outcomes(obj_sdl_to_plot,\n              y_col = 'ewr_achieved',\n              colorset = 'env_group',\n              pal_list = obj_pal$refcols,\n                facet_col = 'SWSDLName',\n              facet_row = '.',\n              sceneorder = sceneorder)\n\n\n\n\nBut for other plots maybe we don’t want that accentuation and we can use the unrefcols to retain the standard coloring- note that ‘NF’, ‘OS’, and ‘WB’ colors are unchanged.\n\nplot_outcomes(obj_sdl_to_plot,\n              y_col = 'ewr_achieved',\n              colorset = 'env_group',\n              pal_list = obj_pal$unrefcols,\n                facet_col = 'SWSDLName',\n              facet_row = '.',\n              sceneorder = sceneorder)"
  },
  {
    "objectID": "comparer/causal_plots.html",
    "href": "comparer/causal_plots.html",
    "title": "Causal network outcomes",
    "section": "",
    "text": "As we showed in the theme aggregation example, we can color the nodes in a causal network by outcome values. That is perhaps the best place to see the use of causal networks for outcomes- using the interleaved example data is less clearly relevant, since spatial scale changes as well, though it can be done. Integrating these plots with the more general approach of plot_outcomes is high on the priority list.\nThere are many other causal network plots we can make but those in the theme aggregation example, target outcomes, rather than adjusting other aesthetics of the network for other reasons, such as altering color by level.\nWe should be able to change size as well. And we can color the edges, but that’s less clear what the meaning is."
  },
  {
    "objectID": "comparer/comparer_overview.html",
    "href": "comparer/comparer_overview.html",
    "title": "Comparer overview",
    "section": "",
    "text": "The Comparer really has two components- underlying functions and structure to perform comparisons and other analyses, and plotting (and other presentation types in future) capabilities to produce some standardized plots that capture important data visualisation.\nThere is quite a lot of flexibility built into all of the comparer, because different uses and different questions will require different outputs to assess, whether that means different scales of analysis, different types of plots, or different numerical comparisons. It is also likely that the desired outputs will change as we work through options.\nWhile this is called the ‘Comparer’, it also contains other functionality related to analysis generally, and can produce plots that do not include comparisons, e.g. hydrograps to simply illustrate historical flows.\nNearly all plots of outcomes (e.g not hydrographs) are made with plot_outcomes, including bars, lines, and maps. This is because at their foundation, they area all plotting a quantitative outcome with grouping of some sort. The data preparation is the same across all of them, as well as many of the arguments to ggplot. We may, though, eventually separate out the data preparation from the plots to make this function cleaner.\nNearly all plots (with the current exception of the causal networks) are made in ggplot2 and return ggplot2 objects, which can easily be further modified. The plot functions here just wrap the ggplot2 to standardise appearance and data preparation. Though it can be annoying to not use ggplot() directly to make the plots, one MAJOR advantage of the plotting function here is that any data changes that clean it for a given plot aren’t preserved, and so it’s far easier to keep the data clean, know what the data is, and avoid accidental overwriting or mislabeling of data. If we consistently do the same changes with slight modification, we can write a function, e.g. plot_prep , avoiding lots of copy-paste and its attendant errors.\nThere is clear opportunity for reactivity with nearly all plots, allowing a user to select plot types, any filtering (espcially for networks, spatial units, etc), and produces the plot."
  },
  {
    "objectID": "comparer/comparer_overview.html#theme",
    "href": "comparer/comparer_overview.html#theme",
    "title": "Comparer overview",
    "section": "Theme",
    "text": "Theme\nI have developed a theme_werp_toolkit() ggplot theme that we use to get a consistent look. We can build on this and change it as we go, as it is fairly simple at present. Additional theme arguments can be passed to it, if we want to change any of the other arguments in ggplot2::theme() on a per-plot basis. By default, theme_werp_toolkit is applied when making the plots inside plot_outcomes and plot_hydrographs, though it can be applied to any ggplot object."
  },
  {
    "objectID": "comparer/comparer_overview.html#colour",
    "href": "comparer/comparer_overview.html#colour",
    "title": "Comparer overview",
    "section": "Colour",
    "text": "Colour\nAt present, I do not enforce a standard set of colors- they’ll change between scenarios/projects and there are too many possibilities of what we might plot. I do provide default palettes for the plotting functions, but we will likely want to change them depending on what we plot. We could enforce palettes within projects, however, once the plots firm up. In general, colors can either be specified manually (usually with the help of make_pal) or with {paletteer} palettes because of the wide range of options with standard interface and ability to choose based on names. A good reference for the available palettes is here, and demonstrations of both ways of specifying colors are throughout the examples, but specifically bar plots.\nThough we do not enforce standard colors, we have established the infrastructure to set consistent colors within a project by using named color objects. This is particularly useful for scenarios, but also can be used for other qualitative categories. Quantitative outcomes (e.g. matching different palettes to outcome x vs y) is not handled automatically at present, but is left to the user. I expect that as a project proceeds and settles on desired outcomes, we will standardize color palettes for different outcome variables, scenarios, etc.\nThere is some interesting ability to set colors within a single column based on different palettes, which can be a useful way to indicate grouping variables. This is available everywhere, but is best demonstrated in the bar plots and causal plots."
  },
  {
    "objectID": "comparer/comparer_overview.html#internal-calculations-and-structure",
    "href": "comparer/comparer_overview.html#internal-calculations-and-structure",
    "title": "Comparer overview",
    "section": "Internal calculations and structure",
    "text": "Internal calculations and structure\nWhile the plots are the typical outputs of the Comparer, it has a set of useful functions for preparing data, including calculating values relative to a baseline (baseline_compare) using either default functions difference and relative, or with any user-supplied function.\nThere is an internal function plot_prep that does all the data prep, including applying baseline_compare, finding colors, and setting established scenario orders. This keeps plots and the data processing consistent, and dramatically reduces the error-prone copy-pasting of data processing with minor changes for different plots. Instead, we can almost always feed the plotting functions the same set of clean data straight out of the aggregator, and just change the arguments to the plot functions.\nBaselining is available as a standalone function (baseline_compare) and can be done automatically in the plot_* functions. This capacity is demonstrated in all the plot examples, but in most detail in the hydrographs notebook.\nOne critical issue, particularly with complex data, is being unaware of overplotting values. The plot_* functions have internal checks that the number of rows of data matches the number of axes on which the data is plotted (including facets, colors, linetype, etc). This prevents things like plotting a map of env_obj data facetted only by scenario, and so each fill represents outcomes for all env_obj, which is meaningless but very easy to do. The exception is that points are allowed to overplot, though we can use the position = 'position_jitter' to avoid even that."
  },
  {
    "objectID": "comparer/flow_scaling_comparer.html",
    "href": "comparer/flow_scaling_comparer.html",
    "title": "Flow scaling comparer",
    "section": "",
    "text": "This will be the comparer notebook for the flow scaling demonstration, including both understanding the outputs and ideally producing figures for publication."
  },
  {
    "objectID": "comparer/heatmap.html",
    "href": "comparer/heatmap.html",
    "title": "Heatmap",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "comparer/heatmap.html#demonstration-setup",
    "href": "comparer/heatmap.html#demonstration-setup",
    "title": "Heatmap",
    "section": "Demonstration setup",
    "text": "Demonstration setup\nAs usual, we need paths to the data.\n\nproject_dir <- file.path('scenario_example')\nagg_dir <- file.path(project_dir, 'aggregator_output')"
  },
  {
    "objectID": "comparer/heatmap.html#scenario-information",
    "href": "comparer/heatmap.html#scenario-information",
    "title": "Heatmap",
    "section": "Scenario information",
    "text": "Scenario information\nThis will be attached to metadata, typically. For now, I’m just using it for diagnostic plots and the demonstration data is simple, so make it here.\n\nscenarios <- tibble::tibble(scenario = c('base', 'down4', 'up4'), delta = c(1, 0.25, 4))"
  },
  {
    "objectID": "comparer/heatmap.html#subset-for-demo",
    "href": "comparer/heatmap.html#subset-for-demo",
    "title": "Heatmap",
    "section": "Subset for demo",
    "text": "Subset for demo\nWe have a lot of hydrographs, so for this demonstration, we will often use a subset.\n\ngauges_to_plot <- c('412002', '419001', '422028', '421001')"
  },
  {
    "objectID": "comparer/hydrographs.html",
    "href": "comparer/hydrographs.html",
    "title": "Hydrographs",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "comparer/hydrographs.html#demonstration-setup",
    "href": "comparer/hydrographs.html#demonstration-setup",
    "title": "Hydrographs",
    "section": "Demonstration setup",
    "text": "Demonstration setup\nAs usual, we need paths to the data, in this case the hydrographs.\n\nproject_dir <- file.path('scenario_example')\nhydro_dir = file.path(project_dir, 'hydrographs')"
  },
  {
    "objectID": "comparer/hydrographs.html#scenario-information",
    "href": "comparer/hydrographs.html#scenario-information",
    "title": "Hydrographs",
    "section": "Scenario information",
    "text": "Scenario information\nThis will be attached to metadata, typically. For now, I’m just using it for diagnostic plots and the demonstration data is simple, so make it here.\n\nscenarios <- tibble::tibble(scenario = c('base', 'down4', 'up4'), delta = c(1, 0.25, 4))"
  },
  {
    "objectID": "comparer/hydrographs.html#subset-for-demo",
    "href": "comparer/hydrographs.html#subset-for-demo",
    "title": "Hydrographs",
    "section": "Subset for demo",
    "text": "Subset for demo\nWe have a lot of hydrographs, so for this demonstration, we will often use a subset.\n\ngauges_to_plot <- c('412002', '419001', '422028', '421001')"
  },
  {
    "objectID": "comparer/line_plots.html",
    "href": "comparer/line_plots.html",
    "title": "Line plots (quantitative x)",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)"
  },
  {
    "objectID": "comparer/line_plots.html#scenario-information",
    "href": "comparer/line_plots.html#scenario-information",
    "title": "Line plots (quantitative x)",
    "section": "Scenario information",
    "text": "Scenario information\nThis will be attached to metadata, typically. For now, I’m just using it for diagnostic plots and the demonstration data is simple, so make it here.\n\nscenarios <- tibble::tibble(scenario = c('base', 'down4', 'up4'), delta = c(1, 0.25, 4))"
  },
  {
    "objectID": "comparer/line_plots.html#subset-for-demo",
    "href": "comparer/line_plots.html#subset-for-demo",
    "title": "Line plots (quantitative x)",
    "section": "Subset for demo",
    "text": "Subset for demo\nWe have a lot of hydrographs, so for this demonstration, we will often use a subset.\n\ngauges_to_plot <- c('412002', '419001', '422028', '421001')"
  },
  {
    "objectID": "comparer/line_plots.html#choosing-example-data",
    "href": "comparer/line_plots.html#choosing-example-data",
    "title": "Line plots (quantitative x)",
    "section": "Choosing example data",
    "text": "Choosing example data\nFirst, we read in the aggregated data. There is example data provided by the toolkit (agg_theme_space and agg_theme_space_colsequence), but to continue with the demonstration, we will use the aggregations created here in the interleaved aggregation notebook.\nNote- to readRDS sf objects, we need to have sf loaded.\n\nagged_data <- readRDS(file.path(agg_dir, 'summary_aggregated.rds'))\n\nThat has all the steps in the aggregation, so we’ll choose one (agged_data$sdl_units) at the SDL unit scale and env_obj theme scale, as this provides the opportunity to consider issues that arise from plottng multiple spatial units and grouped outcome levels. The same ideas would hold at any of the other levels in the aggregation.\nTo make these examples more easily, we create a slightly modified dataframe here, but this isn’t really necessary- small data manipulations are easily piped in to plot_outcomes. The SDL units data is joined to the scenarios dataframe to include the information there about the quantitative meaning of the scenarios, and is given a grouping column that puts the many env_obj variables in groups defined by their first two letters, e.g. EF for Ecosystem Function, which is then used for grouped color palettes.\nIf we had used multiple aggregation functions at any step, we should filter down to the one we want here, but we only used one for this example.\n\n# Create a grouping variable\nobj_sdl_to_plot <- agged_data$sdl_units |>\n  left_join(scenarios) |>\n  dplyr::mutate(env_group = stringr::str_extract(env_obj, '^[A-Z]+')) |>\n  dplyr::filter(!is.na(env_group)) |>\n  dplyr::arrange(env_group, env_obj)\n\nJoining, by = \"scenario\""
  },
  {
    "objectID": "comparer/line_plots.html#lines-through-all-data",
    "href": "comparer/line_plots.html#lines-through-all-data",
    "title": "Line plots (quantitative x)",
    "section": "Lines through all data",
    "text": "Lines through all data\nA simple plot would be to look at all the outcomes, separated by color. We’ve given the scenarios different shapes, but that’s not really necessary- they are different along x. Even this simple plot is quite infomative- we can see that the env_obj outcomes are differently sensitive to both decreases and increases in flow, and that this differs across space.\n\n sdl_line <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'delta',\n                          colorgroups = NULL,\n                          colorset = 'env_obj',\n                          pal_list = list('scico::berlin'),\n                          facet_row = 'SWSDLName',\n                          facet_col = '.',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'))\n \n sdl_line\n\n\n\n\nWe might not care so much about individual outcomes, but about their groupings, and we can plot those in color by changing colorset = 'env_group'. We need to use point_group here to separate out the points for each env_obj.\nThis plot also demonstrates the use of some additional arguments. We’re also using transx to log the x-axis, which is particularly appropriate for the multiplicative flow scaling in this demonstration. We also log the y-axis with transy since we’re using a comp_fun (relative) to look at the multiplicative shift in each env_obj to baseline. We’re using various *_lab arguments to adjust the labelling. We also need to use the (poorly documented) group_cols argument to specify unique rows. This is historical and only applies to baselining the data. It will be auto-found in a future update.\nScientifically, one important thing to note here is that the range on y (0-10) is much greater than the range on x (0.3 - 3), and so (unsurprisingly), some outcomes are disproportionately impacted by flow. Other outcome values are less than the relative shift in flow, and so there are others that are disproportionately insensitive. These disproportionate responses also depend on whether flows decrease or increase- they are not symmetric.\n\nsdl_line_options <- obj_sdl_to_plot |>\n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'delta',\n                y_lab = 'Proportion met',\n                x_lab = 'Change in flow',\n                transx = 'log10',\n                transy = 'log10',\n                color_lab = 'Environmental\\ngroup',\n                colorset = 'env_group',\n                pal_list = list('scico::berlin'),\n                point_group = 'env_obj',\n                facet_row = 'SWSDLName',\n                facet_col = '.',\n                scene_pal = scene_pal,\n                sceneorder = c('down4', 'base', 'up4'),\n                base_lev = 'base',\n                comp_fun = 'relative',\n                group_cols = c('env_obj', 'polyID'))\n\nsdl_line_options\n\n\n\n\nWe can also give the groups different palettes, as demonstrated more completely in the bar plots and causal networks. Now, we don’t need point_group anymore, since the colors are assigned to the unique env_objs.\n\n# Create a palette list\ngrouplist = list(EF = 'grDevices::Purp',\n                 NF = 'grDevices::Mint',\n                 NV = 'grDevices::Burg',\n                 OS = 'grDevices::Blues',\n                 WB = 'grDevices::Peach')\n\nsdl_line_groups <- obj_sdl_to_plot |>\n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'delta',\n                y_lab = 'Proportion met',\n                x_lab = 'Change in flow',\n                transx = 'log10',\n                transy = 'log10',\n                color_lab = 'Environmental\\ngroup',\n                colorgroup = 'env_group',\n                colorset = 'env_obj',\n                pal_list = grouplist,\n                facet_row = 'SWSDLName',\n                facet_col = '.',\n                scene_pal = scene_pal,\n                sceneorder = c('down4', 'base', 'up4'),\n                base_lev = 'base',\n                comp_fun = 'relative',\n                group_cols = c('env_obj', 'polyID'))\n\nsdl_line_groups\n\n\n\n\nFigure 1: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Groups of environmental objectives plotted from different color palettes.\n\n\n\n\nThat’s fairly complex, so we can facet it, as we did with the bars to make the individual env_objs easier to see.\n\nsdl_line_groups_facet <- obj_sdl_to_plot |>\n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'delta',\n                y_lab = 'Proportion met',\n                x_lab = 'Change in flow',\n                transx = 'log10',\n                transy = 'log10',\n                color_lab = 'Environmental\\ngroup',\n                colorgroup = 'env_group',\n                colorset = 'env_obj',\n                pal_list = grouplist,\n                facet_row = 'SWSDLName',\n                facet_col = 'env_group',\n                scene_pal = scene_pal,\n                sceneorder = c('down4', 'base', 'up4'),\n                base_lev = 'base',\n                comp_fun = 'relative',\n                group_cols = c('env_obj', 'polyID'))\n\nsdl_line_groups_facet\n\n\n\n\nFigure 2: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Groups of environmental objectives plotted from different color palettes and facetted for easier visualisation.\n\n\n\n\nThe above is typically how we would go about this facetting, but it is worth reiterating that these are just ggplots, and so we can post-hoc add facetting. Using the version with only spatial facetting ( Figure 1 ), we can add the env_group facet on, matching Figure 2 . Note that we re-build all the facets here, due to the specification of ggplot2::facet_grid.\n\nsdl_line_groups + facet_grid(SWSDLName ~ env_group)\n\n\n\n\nAs with the bar plots, we can color by any column we want, and the spatial units is a logical choice. We again use point_group, since multiple env_obj rows are mapped to each color. The overplotting gets unreadable here and so I’ve retained the facetting, but if we were looking at a subset, the line colors could be enough (or if we are summarising the data with a smoother- see below).\n\nsdl_line_sdl <- obj_sdl_to_plot |>\n  filter(env_group == 'EF') |>\n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'delta',\n                y_lab = 'Proportion met',\n                x_lab = 'Change in flow',\n                transx = 'log10',\n                transy = 'log10',\n                color_lab = 'SDL unit',\n                colorset = 'SWSDLName',\n                pal_list = list(\"ggsci::default_jama\"),\n                point_group = 'env_obj',\n                scene_pal = scene_pal,\n                sceneorder = c('down4', 'base', 'up4'),\n                base_lev = 'base',\n                comp_fun = 'relative',\n                group_cols = c('env_obj', 'polyID'))\n\nsdl_line_sdl\n\n\n\n\nFigure 3: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Colors indicate SDL unit, each line is an env_obj."
  },
  {
    "objectID": "comparer/line_plots.html#smoothing-fit-lines",
    "href": "comparer/line_plots.html#smoothing-fit-lines",
    "title": "Line plots (quantitative x)",
    "section": "Smoothing (fit lines)",
    "text": "Smoothing (fit lines)\nWe can use smoothing to fit lines through multiple points, e.g. if we want to group data in some way- maybe use it to put a line through the color groups and ignore individual levels. This is dangerous- it’s an aggregation. But it can also be very informative, and we can show the individual data points to avoid misleading information. We demonstrate here using them to illustrate unique outcomes, as well as more typical uses as lines of best fit that aggregate over a number of outcomes.\nTo get smoothed lines, we use smooth = TRUE. By default, that produces a loess fit (as with ggplot2::geom_smooth, but we can also pass smooth_method, which is the method argument to ggplot::geom_smooth, and so allows things like lm and glm fits.\n\nUnique points\nFitting lines through unique points at each scenario level is a bit contrived, but it can be useful if we want to accentuate nonlinear relationships. Linear fits are possible too, though these are typically less useful.\nWith unique points, this just fits a single curved line through each env_obj. Recapitulating the above, we color here from SDL unit.\n\n  sdl_smooth_sdl <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'delta',\n                          y_lab = 'Proportion met',\n                          x_lab = 'Change in flow',\n                          transx = 'log10',\n                          color_lab = 'Catchment',\n                          colorgroups = NULL,\n                          colorset = 'SWSDLName',\n                          point_group = 'env_obj',\n                          pal_list = list('ggsci::default_jama'),\n                          facet_row = 'env_group',\n                          facet_col = '.',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'),\n                          base_lev = 'base',\n                          comp_fun = 'difference',\n                          group_cols = c('env_obj', 'polyID'),\n                          smooth = TRUE)\n  \n  suppressWarnings(print(sdl_smooth_sdl))\n\n\n\n\nAnd we can do the same for environmental groupings.\n\n  sdl_smooth_groups <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'delta',\n                          y_lab = 'Proportion met',\n                          x_lab = 'Change in flow',\n                          transx = 'log10',\n                          color_lab = 'Environmental grouping',\n                          colorgroups = NULL,\n                          colorset = 'env_group',\n                          point_group = 'env_obj',\n                          pal_list = list('scico::berlin'),\n                          facet_row = 'env_group',\n                          facet_col = 'SWSDLName',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'),\n                          base_lev = 'base',\n                          comp_fun = 'difference',\n                          group_cols = c('env_obj', 'polyID'),\n                          smooth = TRUE)\nsuppressWarnings(print(sdl_smooth_groups))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nUsing smooth_method = 'lm' is a linear fit. It does not recapitulates the simple lines above, however, because it fits the line through all the scenario data points, rather than simply joining them together. I have turned smooth_se = FALSE here because with unique groups the standard errors are enormous.\n\nsdl_lm_groups <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'ewr_achieved',\n                          x_col = 'delta',\n                          y_lab = 'Proportion met',\n                          x_lab = 'Change in flow',\n                          transx = 'log10',\n                          color_lab = 'Environmental grouping',\n                          colorgroups = NULL,\n                          colorset = 'env_group',\n                          point_group = 'env_obj',\n                          pal_list = list('scico::berlin'),\n                          facet_row = 'env_group',\n                          facet_col = 'SWSDLName',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'),\n                          base_lev = 'base',\n                          comp_fun = 'relative',\n                          group_cols = c('env_obj', 'polyID'),\n                          smooth = TRUE,\n                  smooth_method = \"lm\", smooth_se = FALSE)\n\nWarning: NaN and Inf introduced in `plot_prep`, likely due to division by zero.\n108 values were lost.\n\nsuppressWarnings(print(sdl_lm_groups))\n\n\n\n\n\n\nFit multiple points\nFitting lines is most often associated with things like regression and loess smoothing, where we use it to aggregate over a number of datapoints to find the line of best fit. We can do that here, simply by not having all points accounted for across the facetting, point_group, and colorset. NOTE- group_cols should still include unique values, because group_cols determines the baselining (e.g. what gets compared), not the plot groupings.\nOne example would be to perform the same analysis as in Figure 3, but instead of plotting each point, fit a line to show the mean change within each SDL unit. We’ve pulled env_obj out of point_group, but left it in group_cols , because we still want each env_obj baselined with itself, not to the mean of env_group. Now, we can look at all the env_groups, because there are far fewer lines and so the overplotting isn’t an issue.\nWe use a small add_eps to avoid zeros and allow all data to be relativised and plotted.\n\nsdl_fit_sdl <- obj_sdl_to_plot |>\n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'delta',\n                y_lab = 'Proportion met',\n                x_lab = 'Change in flow',\n                transx = 'log10',\n                transy = 'log10',\n                color_lab = 'SDL unit',\n                colorset = 'SWSDLName',\n                pal_list = list(\"ggsci::default_jama\"),\n                facet_wrapper = 'env_group',\n                scene_pal = scene_pal,\n                sceneorder = c('down4', 'base', 'up4'),\n                base_lev = 'base',\n                comp_fun = 'relative',\n                add_eps = min(obj_sdl_to_plot$ewr_achieved[obj_sdl_to_plot$ewr_achieved > 0], \n                              na.rm = TRUE)/2,\n                group_cols = c('env_obj', 'polyID'),\n                smooth = TRUE)\n\nsuppressWarnings(print(sdl_fit_sdl))\n\n\n\n\nFigure 4: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Fits are loess smoothers. Colors indicate SDL unit, which have single lines. Each point is an env_obj.\n\n\n\n\nWe can make a very similar plot, looking at the environmental groups, a smooth fit of Figure 1 . We use a position argument (which passes to {ggplot2}, and so has the same syntax) to see overplotted points, and an add_eps to avoid zeros to relativise and plot all the data.\n\nsdl_fit_groups <- obj_sdl_to_plot |>\n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'delta',\n                y_lab = 'Proportion met',\n                x_lab = 'Change in flow',\n                transx = 'log10',\n                transy = 'log10',\n                color_lab = 'Environmental\\ngroup',\n                colorset = 'env_group',\n                pal_list = list('scico::berlin'),\n                facet_row = 'SWSDLName',\n                facet_col = '.',\n                scene_pal = scene_pal,\n                sceneorder = c('down4', 'base', 'up4'),\n                base_lev = 'base',\n                comp_fun = 'relative',\n                add_eps = min(obj_sdl_to_plot$ewr_achieved[obj_sdl_to_plot$ewr_achieved > 0], \n                              na.rm = TRUE)/2,\n                group_cols = c('env_obj', 'polyID'),\n                smooth = TRUE,\n                position = position_jitter(width = 0.01, height = 0))\n\nsdl_fit_groups\n\n\n\n\nFigure 5: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Fits are loess smoothers. Colors indicate Environmental groups, which have single lines. Each point is an env_obj.\n\n\n\n\nAs we saw above, we can use method = 'lm' to plot a regression, though in general we do not expect these relationships to be linear, and mathematically characterising them will be a complex task that is not the purview of plotting (though is in the purview of the Comparer, and will be addressed once we have more complete outputs).\nA linear fit of the SDL units ( Figure 6 ) is one example of how this might work. It is useful to know here that deviations from a 1:1 line on logged axes as here means that the outcomes are responding disproportionately more (steeper) or less (shallower) than the underlying changes to flow.\n\nsdl_lm_sdl <- obj_sdl_to_plot |>\n  plot_outcomes(y_col = 'ewr_achieved',\n                x_col = 'delta',\n                y_lab = 'Proportion met',\n                x_lab = 'Change in flow',\n                transx = 'log10',\n                transy = 'log10',\n                color_lab = 'SDL unit',\n                colorset = 'SWSDLName',\n                pal_list = list(\"ggsci::default_jama\"),\n                facet_wrapper = 'env_group',\n                scene_pal = scene_pal,\n                sceneorder = c('down4', 'base', 'up4'),\n                base_lev = 'base',\n                comp_fun = 'relative',\n                add_eps = min(obj_sdl_to_plot$ewr_achieved[obj_sdl_to_plot$ewr_achieved > 0], \n                              na.rm = TRUE)/2,\n                group_cols = c('env_obj', 'polyID'),\n                smooth = TRUE,\n                smooth_method = 'lm')\n\nsuppressWarnings(print(sdl_lm_sdl))\n\n\n\n\nFigure 6: Change in proportion of environmental objectives met in each scenario, relative to the historical baseline, dependent on the shift in flow. Fits are linear regressions. Colors indicate SDL unit, which have single lines. Each point is an env_obj."
  },
  {
    "objectID": "comparer/maps.html",
    "href": "comparer/maps.html",
    "title": "Maps",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)"
  },
  {
    "objectID": "comparer/maps.html#scenario-information",
    "href": "comparer/maps.html#scenario-information",
    "title": "Maps",
    "section": "Scenario information",
    "text": "Scenario information\nThis will be attached to metadata, typically. For now, I’m just using it for diagnostic plots and the demonstration data is simple, so make it here.\n\nscenarios <- tibble::tibble(scenario = c('base', 'down4', 'up4'), delta = c(1, 0.25, 4))"
  },
  {
    "objectID": "comparer/maps.html#subset-for-demo",
    "href": "comparer/maps.html#subset-for-demo",
    "title": "Maps",
    "section": "Subset for demo",
    "text": "Subset for demo\nWe have a lot of hydrographs, so for this demonstration, we will often use a subset.\n\ngauges_to_plot <- c('412002', '419001', '422028', '421001')"
  },
  {
    "objectID": "comparer/maps.html#choosing-example-data",
    "href": "comparer/maps.html#choosing-example-data",
    "title": "Maps",
    "section": "Choosing example data",
    "text": "Choosing example data\nFirst, we read in the aggregated data. There is example data provided by the toolkit (agged_data and agged_data_colsequence), but to continue with the demonstration, we will use the aggregations created here in the interleaved aggregation notebook.\nNote- to readRDS sf objects, we need to have sf loaded.\n\nagged_data <- readRDS(file.path(agg_dir, 'summary_aggregated.rds'))\n\nAs with line plots, we’ll make a grouping variable in the SDL-scale env_obj data for demonstrating grouped palettes, but will leave the other aggregation levels alone, and do any minor modifications there while piping into plot_outcomes.\n\n# Create a grouping variable\nobj_sdl_to_plot <- agged_data$sdl_units |>\n  left_join(scenarios) |>\n  dplyr::mutate(env_group = stringr::str_extract(env_obj, '^[A-Z]+')) |>\n  dplyr::filter(!is.na(env_group)) |>\n  dplyr::arrange(env_group, env_obj)\n\nJoining, by = \"scenario\""
  },
  {
    "objectID": "comparer/plot_creation.html",
    "href": "comparer/plot_creation.html",
    "title": "Plot demonstrations",
    "section": "",
    "text": "Then send them to the werptoolkitr package. They’re just easier to develop here where there’s data to plot.\nWhat’s the best way to do this though? just devtools::load_all(path/to/toolkit) for quick dev, anyway, I think.\nLargely the same header junk as the other qmds here to allow me to load the toolkit different ways.\n\n## GITHUB INSTALL\n# credentials::set_github_pat()\n# devtools::install_github(\"MDBAuth/WERP_toolkit\", ref = 'packaging', subdir = 'werptoolkitr', force = TRUE)\n\n## LOCAL INSTALL- easier for quick iterations, but need a path.\n# devtools::install_local(\"C:/Users/galen/Documents/WERP_toolkit/werptoolkitr\", force = TRUE)\n\n# And for very fast iteration (no building, but exposes too much, often)\ndevtools::load_all(\"C:/Users/galen/Documents/WERP_toolkit/werptoolkitr\")\n\n\n# library(werptoolkitr)\nlibrary(ggplot2)\nlibrary(dplyr) # make my life easier\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:testthat':\n\n    matches\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nscenario_dir <- file.path('scenario_example')\newr_results <- file.path(scenario_dir, 'module_output', 'EWR')\ngeo_data_dir <- system.file(\"extdata\", package = 'werptoolkitr')\n\n\n\nThis needs to come from somewhere. For now, I’m just using it for diagnostic plots, so make it here.\n\nscenarios <- tibble::tibble(scenario = c('base', 'down4', 'up4'), delta = c(1, 0.25, 4))\n\nI think I probably want to make a standard ordering of the scenarios. Can I push something through to plots\n\nsceneorder <- forcats::fct_reorder(scenarios$scenario, scenarios$delta)"
  },
  {
    "objectID": "comparer/plot_creation.html#hydrographs",
    "href": "comparer/plot_creation.html#hydrographs",
    "title": "Plot demonstrations",
    "section": "Hydrographs",
    "text": "Hydrographs\nSettings\n\nhydropath <- file.path('scenario_example', 'hydrographs')\n\nThere is code to unpack that, but it’s in python and sends them to the EWR, but here we just want to read them in. Here, make something that we can also change. With a wrapper that lets me read different hydrograph formats.\nmdba_gauge_getter seems to default to flow, and gets them with 141, ML/day. Check that all makes sense- what does EWR use? It is ML/day for most gauges (which is varto = 141), though some are stage gauges and use 100.\n\n# read_hydro <- function(hydropath, long = TRUE, format = 'csv') {\n#   if (format == 'csv') {\n#     return(read_hydro_csv(hydropath, long))\n#   }  \n# }\n# \n# read_hydro_csv <- function(hydropath, long) {\n#   all_hydros <- list.files(hydropath, recursive = TRUE)\n#   # read-in and extract the names\n#   hydros <- readr::read_csv(file.path(hydropath, all_hydros), id = 'scenario') |> \n#     dplyr::mutate(scenario = stringr::str_extract(scenario, \"(\\\\w|\\\\d)+\\\\.csv$\"),\n#            scenario = stringr::str_extract(scenario, '^[\\\\w|\\\\d]+'))\n#   \n#   if (long) {\n#     hydros <- tidyr::pivot_longer(hydros, cols = -c(scenario, Date), names_to = \"gauge\", values_to = \"flow\")\n#   }\n#   \n#   return(hydros)\n# }\n\nNow, make a plot. How do we want to organise it? There are lots of options. Particularly subsetting gauges.\n\nscenehydros <- read_hydro(hydropath, long = TRUE, format = 'csv')\n\n\ngauges_to_plot <- c('412002', '419001', '422028', '421001')\n\nforcats::fct_reorder(scenario, flow, .fun=median)\n\nscenehydros |>\n  dplyr::filter(gauge %in% gauges_to_plot) |>\nggplot2::ggplot(ggplot2::aes(x = Date, y = flow, \n                             color = forcats::fct_relevel(scenario, levels(sceneorder)))) + \n  ggplot2::geom_line() + \n  ggplot2::facet_wrap(~gauge) +\n  theme_werp_toolkit()\n\n\n\n\n\nMake that a function\nMoved it to the package\nA typical plot\nFirst, let’s establish a standard set of colours with base as a reference\n\n# rcartocolor::Antique\n# RColorBrewer::Set2\nscene_pal <- make_pal(unique(scenehydros$scenario), palette = 'ggsci::nrc_npg', refvals = 'base', refcols = 'black')\nscene_pal\n\n<colors>\nblack #E64B35FF #4DBBD5FF \n\n\n\nplot_hydrographs(scenehydros, gaugefilter = gauges_to_plot, colors = scene_pal)\n\n\n\n\nA test with scenariofilter, scales, and trans. Kinda silly, but shows it works\n\nplot_hydrographs(scenehydros, gaugefilter = gauges_to_plot, scenariofilter = c('up4', 'down4'), scales = 'free_y', transy = 'log10')\n\ncolors not specified per level. Trying to use the 'colors' argument as a palette\nname\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\n\n\n\n\n\nWhat else might we want to do?\n\nselect scenarios\ncolor/fill the backgrounds by catchment? that’s getting fairly fancy.\naverage over something? Not sure that really makes sense? I think this might be good for now.\nTheming: a set theme. and standard set of scenario colours that is consistent across figs, even when scenarios are filtered.\nI think make the theme set, and the colors we allow an argument, and make the user specify those relationships if they want to."
  },
  {
    "objectID": "comparer/plot_creation.html#setting-baseline",
    "href": "comparer/plot_creation.html#setting-baseline",
    "title": "Plot demonstrations",
    "section": "Setting baseline",
    "text": "Setting baseline\nWe want to set a baseline and the relationship to it. The baseline should be able to be either a scenario name or a scalar. Potentially something else (e.g. historical daily means), but deal with that later- relatively straightforward to add a method to pass a dataframe with date and flow cols, for example.\nDo we want to build it into the plot functions, or pre-run it? Definitely make it available for the latter, but does it just make the plot functions too messy? Would be nice for auto-labelling though.\n\ndif_flow <- baseline_compare(scenehydros, compare_col = 'scenario', base_lev = 'base', values_col = 'flow', comp_fun = difference)\n\n\nplot_hydrographs(dif_flow, gaugefilter = gauges_to_plot, y_col = 'difference_flow', colors = scene_pal)\n\n\n\n\nRelative\n\nrel_flow <- baseline_compare(scenehydros, compare_col = 'scenario', \n                             base_lev = 'base', \n                             values_col = 'flow', comp_fun = relative, add_eps = min(scenehydros$flow[scenehydros$flow > 0])/10)\n\n\nplot_hydrographs(rel_flow, gaugefilter = gauges_to_plot, y_col = 'relative_flow', colors = scene_pal)\n\n\n\n\nWell, I guess that makes sense. The scenarios are 4x and 0.24x, and so they always are flat lines at those levels (plus the add_eps) unless everything is 0. It will be more relevant for other things, I think.\nMutliplicative will usually make more sense with a logged y\n\nplot_hydrographs(rel_flow, gaugefilter = gauges_to_plot, y_col = \"relative_flow\", colors = scene_pal, transy = 'log10')\n\n\n\n\ntransy = 'log' works too, but scales::trans chooses ugly axis labels. Not that these are great, but they’re not as horrible.\nI now have incorporated the ability to pass the comparison bit directly into the plot function. It’s likely the y-labels will need to be changed post-hoc, but that’s easy with + ylab('new label'). It just ended up making more sense to label with exactly what they are, rather than all permutations of how i might want to label differences vs relatives vs. whatever down the track.\nTo re-do the above in one go, start with scenehydros directly\n\nplot_hydrographs(scenehydros, gaugefilter = gauges_to_plot, y_col = 'flow', colors = scene_pal, base_lev = 'base', comp_fun = difference)\n\n\n\n\nFor the relative, I’m not adding an eps, but letting the zeros drop out here.\n\nplot_hydrographs(scenehydros, gaugefilter = gauges_to_plot, y_col = 'flow', colors = scene_pal, base_lev = 'base', comp_fun = relative)\n\nWarning: Removed 135 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "comparer/plot_creation.html#describing-the-aggregation",
    "href": "comparer/plot_creation.html#describing-the-aggregation",
    "title": "Plot demonstrations",
    "section": "Describing the aggregation",
    "text": "Describing the aggregation\nThese are Georgia’s tile plots and maps. Need to automate how they’re done. Will need the list-saved aggregations, as well as the aggseq and funseqs.\nThough with the names and aggseqs as columns, I should be able to say what happened to the final outcome, just won’t be able to build up the plots. That’s fine, I think. Then can loop over the list if I want to show the build-up."
  },
  {
    "objectID": "comparer/plot_creation.html#davids-heatmap",
    "href": "comparer/plot_creation.html#davids-heatmap",
    "title": "Plot demonstrations",
    "section": "David’s heatmap",
    "text": "David’s heatmap\nWe can’t do this yet- all we have is flow scaling, which is one-d. That’s fine, I can make a 1-d version (e.g an x with flow scaling, y with outcome, and fit a curve/kernel/line. But even then, we don’t have anywhere like enough data. So for now, I think we just geom_line() it, but put in the ability to use a kernel density or a spline or something.\nThe maps are actually really close to it too- they have y_col mapped to fill.\nThe 1-d version will end up looking at lot like some of the figures we had in the demo doc, and for good reason- that’s what they are.\nSo, develop the 1-d version"
  },
  {
    "objectID": "comparer/plot_creation.html#scenario-vs-outcome-1d",
    "href": "comparer/plot_creation.html#scenario-vs-outcome-1d",
    "title": "Plot demonstrations",
    "section": "Scenario vs outcome 1d",
    "text": "Scenario vs outcome 1d\nThis should take tibbles of some sort, arguments for the outcome and any groupings and return a ggplot. Possibly facetted.\nTest first with just the EWR output summary\nAssume we always have a ‘scenario’ column\nFiltering is going to be trickier here- we can’t really use gaugefilter since we can’t assume gauges will exist.\nAt some level, should I just enforce the look with make_pal and theme_werp_toolkit at let us build plots ad-hoc? Yes, but let’s still provide some standard ones.\n\nBar\nThis really is a bit of a mess. We need to only plot a single value per x/color/fill/facet factorial. (and color and fill are unlikely to differ)\n\nggplot2::ggplot(summary_ewr_output, \n                ggplot2::aes(x = scenario, y = ewr_achieved, fill = scenario)) +\n  ggplot2::geom_col() # I think same as geom_bar(stat = 'identity')- plots the value\n\n\n\n\nThat’s quite clearly NOT plotting a single value. We could aggregate (e.g. over gauges), or we can split. Here, what varies within scenario? gauge, ewr_code , ewr_code_timing, anything else?\n\nsummary_ewr_output %>% \n  # just grab the first code_timing\n  dplyr::group_by(ewr_code, gauge, scenario) %>% \n  dplyr::slice(1) %>% \n  dplyr::ungroup() %>% \n  dplyr::filter(ewr_code %in% c('BF1', 'LF1', \"OB5\") & \n                  gauge %in% c(\"412002\", \"412005\", \"412038\")) %>% \nggplot2::ggplot(ggplot2::aes(x = scenario, y = ewr_achieved, fill = scenario)) +\n  ggplot2::geom_col() + # Same as geom_bar(stat = 'identity')- plots the value\n  # use `reformulate` to end run the rlang issues of characters\n  ggplot2::facet_grid(reformulate('gauge','ewr_code'))\n\n\n\n\nSo, how much handholding do we want to do? I think if we want to keep data general, as little as possible. By the time we get the data arrangement sorted out, we’ll kind of end up just wrapping the ggplot call in the arguments. I guess though we still want to do the plot_prep for baselining and colors? I have a feeling the workflow in practice will be more like\ndataprep –> plot_prep -> ggplot code -> scale_color_manual + theme_werp_toolkit\nbut we can build a function, I guess.\nIf we get to the point of making default plots, then the dataprep can be rolled up into a default_bar function or whatever.\nAnyway, let’s assume the data has been arranged in a reasonable way by the user. Then, we can pass in a fill, facet_row, facet_col, and call it a day with a little check that fill * row * col isn’t longer than nrows.\nOR, is it better to settle on what the default plots should be, and then just make them in a notebook with exposed ggplot code, since that’s more readable anyway? quite possibly. We just need to be really targetted and not fill notebooks up with 100 different copy-paste variations- at that point it SHOULD be a function. But maybe don’t write those until we bash through what we want for a given case study/report whatever. IE have an ‘experiment’ notebook that manages the run and its outputs, and use the plot infrastructure functions but not all the way to full plotting functions?\nThat would still let me enforce color palettes across scenarios, for example.\nKind of what I’m wondering is if the answer is more aggressive editing and re-writing, rather than functions all over the place. It will depend how things progress, I think.\nI’ll get through this plot, and then make more demos just in ggplot (e.g. the next couple, maps, etc). Then we can decide exactly what the functions need to do. We already have causal plot funs too.\nOne MAJOR advantage of functions is that any data changes that clean it for a given plot aren’t preserved, and so it’s way easier to keep the data clean and know what we’re working with. and if we consistently do the same changes, that can be its own function, e.g. plot_prep\nThe above can now be created with a function\nFirst, it’s the user’s responsibility to know what the data looks like. Make some\n\newr_to_bar_data <- summary_ewr_output %>% \n  # just grab the first code_timing\n  dplyr::group_by(ewr_code, gauge, scenario) %>% \n  dplyr::slice(1) %>% \n  dplyr::ungroup() %>% \n  dplyr::filter(ewr_code %in% c('BF1', 'LF1', \"OB5\") & \n                  gauge %in% c(\"412002\", \"412005\", \"412038\"))\n\nThen plot, and feed it the scenario palette scene_pal . otherwise it auto-generates with scico::oslo.\n\nplot_outcomes(ewr_to_bar_data, \n                 y_col = 'ewr_achieved', \n                 facet_row = 'gauge', \n                 facet_col = 'ewr_code',\n                 colorset = 'scenario',\n                 pal_list = scene_pal,\n                 sceneorder = levels(sceneorder))\n\ncolors not specified per level. Trying to use the 'colors' argument as a palette\nname\n\n\n\n\n\nMaybe not the prettiest color choices, but everything is working.\nDo I want a file in /R that just establishes all the default orders and palettes? Probably. Could be a function that gets called, e.g. set_plot_defaults_werp() at the head of notebooks/scripts.\n\n\nSame thing, but better data\nat the other end of the spectrum, let’s look at the basin-scaled data\nSet up the data we want to plot. There are 27 objectives. Should we just use them all? Try it.\n\nbasin_to_plot <- agg_theme_space$mdb %>% \n  rename(allArith = 4, oneLimiting = 5) %>% # for readability\n  filter(!is.na(Objective))\n\nThere’s really only one thing to facet, so should drop to facet_wrap instead of facet_grid.\n\nplot_outcomes(basin_to_plot, \n                 y_col = 'allArith',\n                 facet_wrapper = 'Objective',\n                 colorset = 'scenario',\n                 pal_list = scene_pal,\n                 sceneorder = levels(sceneorder))\n\ncolors not specified per level. Trying to use the 'colors' argument as a palette\nname\n\n\n\n\n\n\n\nStacked bar, multiple outcomes\nWhat’s x and what’s the stack? could be ewr_code or scenario in either place. Does it matter? Maybe not?\n\nggplot(basin_to_plot, aes(x = scenario, y = allArith, fill = Objective)) + \n  geom_col() + theme(legend.position = 'none')\n\n\n\n\nTHat’s basically it, once i sort out the color-grouping-\nCan I group these in some way? I have some code in causal network plots that allows specifying colors by group within a col that might work really well, e/.g. make the different Fish outcomes shades of blue, veg green, etc…\nI now have a function.\nI can use a single color palette\n\nbasin_to_plot <- agg_theme_space$mdb %>%\n    dplyr::rename(allArith = 4, oneLimiting = 5) %>% # for readability\n    dplyr::filter(!is.na(Objective))\n\n  basin_plot <- plot_outcomes(basin_to_plot,\n                                      y_col = 'allArith',\n                                      colorset = 'Objective',\n                                      pal_list = list(\"scico::oslo\"),\n                                      sceneorder = c('down4', 'base', 'up4'))\n\ncolors not specified per level. Trying to use the 'colors' argument as a palette\nname\n\n  basin_plot\n\n\n\n  # labels are terribly long\n  basin_plot + theme(legend.position = 'none')\n\n\n\n\nI can specify groups and use different palettes for each\n\nobj_sdl_to_plot <- agg_theme_space$sdl_units %>%\n    dplyr::rename(allArith = 4) # for readability\n\n  # Create a grouping variable\n  obj_sdl_to_plot <- obj_sdl_to_plot |>\n    dplyr::mutate(env_group = stringr::str_extract(env_obj, '^[A-Z]+')) |>\n    dplyr::filter(!is.na(env_group)) |>\n    dplyr::arrange(env_group, env_obj)\n\n  # Create a palette list\n  grouplist = list(EF = 'grDevices::Purp',\n                   NF = 'grDevices::Mint',\n                   NV = 'grDevices::Burg',\n                   OS = 'grDevices::Blues',\n                   WB = 'grDevices::Peach')\n\n  # need to facet by space sdl unit and create a group col to take multiple palettes\nsdl_stack <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          colorgroups = 'env_group',\n                          colorset = 'env_obj',\n                          pal_list = grouplist,\n                          facet_wrapper = 'SWSDLName',\n                          sceneorder = c('down4', 'base', 'up4'))\n\ncolors not specified per level. Trying to use the 'colors' argument as a palette\nname\n\nsdl_stack\n\n\n\n\nAnd I can specify that differently with facet_wrap and facet_rows\n\n sdl_plot_facrow <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          colorgroups = 'env_group',\n                          colorset = 'env_obj',\n                          pal_list = grouplist,\n                          facet_col = 'SWSDLName',\n                          facet_row = '.',\n                          sceneorder = c('down4', 'base', 'up4'))\n\ncolors not specified per level. Trying to use the 'colors' argument as a palette\nname\n\n sdl_plot_facrow\n\n\n\n\nAnd it might actually look best to facet on the groups\n\nsdl_plot_factgroup <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          colorgroups = 'env_group',\n                          colorset = 'env_obj',\n                          pal_list = grouplist,\n                          facet_row = 'SWSDLName',\n                          facet_col = 'env_group',\n                          sceneorder = c('down4', 'base', 'up4'))\n\ncolors not specified per level. Trying to use the 'colors' argument as a palette\nname\n\nsdl_plot_factgroup\n\n\n\n\nThis is kind of a cool plot too\n\n  obj_pal <- make_pal(levels = unique(obj_sdl_to_plot$env_group),\n                      palette = 'scico::berlin')\n  \nsdl_plot_groupblock <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          x_col = 'scenario',\n                          colorgroups = 'env_group',\n                          colorset = 'env_obj',\n                          color_lab = 'Environmental\\ngroup',\n                          pal_list = obj_pal,\n                          facet_row = 'SWSDLName',\n                          facet_col = '.',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'))\n\nsdl_plot_groupblock + theme(legend.position = 'left')\n\n\n\n\n\nFlipped axes\nDoes that actually work better with the objectives on x and scenarios as colors? Maybe coord_flipped?\nI haven’t done a plot_prep here, so the scenarios aren’t ordered properly, but that basically works. Maybe we should fct_reorder? Not sure what the natural ordering is.\n\nggplot(basin_to_plot, aes(x = forcats::fct_reorder(Objective, allArith,\n                                                   .fun = sum, \n                                                   .desc = TRUE),\n                          y = allArith, fill = scenario)) + \n  geom_col() + scale_fill_manual(values = scene_pal) \n\n\n\n\nSo, that’s one option. It might be nice to put them in groups somehow (or have that ability). ie lump by fish, veg, whatever. For some outcomes we don’t know what that looks like.\nOne option is to fct_reorder2 with a group col, another is to use the group color thing I want to do above to set color (i.e. the outline).\nAnd we need to make those names readable, but that’s going to be a job. Just flipping coords doesn’t work. stringr::str_wrap helps, but those names just need to be shortened, and that’s a manual job.\n\nggplot(basin_to_plot, aes(x = forcats::fct_reorder(\n  stringr::str_wrap(Objective, 20),\n  allArith,\n  .fun = sum, \n  .desc = TRUE),\n  y = allArith, \n  fill = scenario)) + \n  geom_col() + \n  scale_fill_manual(values = scene_pal) + \n  coord_flip()\n\n\n\n\nNow that I have a function for the other way, hopefully I can just modify it to make this stack. Will need to swap x and fill, and pass in scene_pal instead of of the big palette list (in addition to, if I want to color the bar outlines?).\nFor the same set as the function-ed stacks above (and the same as the env_group stack above, but without the grouping. this is similar to above, but now we don’t give different palettes to the groups.\n\nobj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          colorgroups = NULL,\n                          colorset = 'env_obj',\n                          pal_list = list('scico::berlin'),\n                          facet_row = 'SWSDLName',\n                          facet_col = '.',\n                          scene_x = FALSE,\n                          scene_pal = scene_pal,\n                          sceneorder = levels(sceneorder))\n\n\n\n\nI’m really close to having a grouping aesthetic, (I can assign color), but using color on bar plots is ugly. Some sort of faint background would be better, or coloring the x-labels by group. Not worth the time now, but it’s set up to do if I can figure out how to do those things.\nwhat does that look like facetted on the groups of outcomes as well? It works OK, but not as well as when the individual objectives are stacked, since each one gets its own x-tick. I hoped scales = 'free_x' would work to just give each one a subset, but it only does when the groupings are in facet-columns. That’s probably OK?\n\nstackfacet <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          colorgroups = NULL,\n                          colorset = 'env_obj',\n                          pal_list = list('scico::berlin'),\n                          facet_row = 'SWSDLName',\n                          facet_col = 'env_group',\n                          scales = 'free_x',\n                          scene_x = FALSE,\n                          scene_pal = scene_pal,\n                          sceneorder = levels(sceneorder)) \n\nstackfacet\n\n\n\n\nWe can (I think) dodge the bars. These are ggplot objects, so we can tweak them a bit after they come out of the main function.\n\ndodgefacet <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          y_lab = 'Proportion Success',\n                          colorgroups = NULL,\n                          colorset = 'env_obj',\n                          pal_list = list('scico::berlin'),\n                          facet_row = 'SWSDLName',\n                          facet_col = 'env_group',\n                          scales = 'free_x',\n                          scene_x = FALSE,\n                          scene_pal = scene_pal,\n                          sceneorder = levels(sceneorder),\n                          position = 'dodge') \n\ndodgefacet + theme(legend.position = 'bottom') + labs(x = 'Environmental Objective')\n\n\n\n\n\n\n\nscenario colors\nThose last two also work well (better) with scenario as colors and env_obj as x\n\nstackfacet <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          x_col = 'env_obj',\n                          colorgroups = NULL,\n                          colorset = 'env_obj',\n                          pal_list = list('scico::berlin'),\n                          facet_row = 'SWSDLName',\n                          facet_col = 'env_group',\n                          scales = 'free_x',\n                          scene_x = FALSE,\n                          scene_pal = scene_pal,\n                          sceneorder = levels(sceneorder)) \n\nstackfacet\n\n\n\n\nWe can (I think) dodge the bars. These are ggplot objects, so we can tweak them a bit after they come out of the main function.\n\ndodgefacet <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          x_col = 'env_obj',\n                          colorgroups = NULL,\n                          colorset = 'env_obj',\n                          pal_list = list('scico::berlin'),\n                          facet_row = 'SWSDLName',\n                          facet_col = 'env_group',\n                          scales = 'free_x',\n                          scene_x = FALSE,\n                          scene_pal = scene_pal,\n                          sceneorder = levels(sceneorder),\n                          position = 'dodge')\n\ndodgefacet + theme(legend.position = 'bottom') + labs(x = 'Environmental Objective')\n\n\n\n\n\n\n\n\n\nLines with quant x\nThis is the sort of plot we might want to make for plotting the actual values that change between scenarios. In this case, we have scenarios with flow multipliers; we add those to the data with a left_join here, but they should be attached.\n\nobj_sdl_to_plot <- obj_sdl_to_plot |>\n   left_join(scenarios)\n\nJoining, by = \"scenario\"\n\n\n\n sdl_line <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          x_col = 'delta',\n                          colorgroups = NULL,\n                          colorset = 'env_obj',\n                          pal_list = list('scico::berlin'),\n                          facet_row = 'SWSDLName',\n                          facet_col = '.',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'))\n \n sdl_line # + theme(legend.position = 'bottom')\n\n\n\n\nAnd we can do the same sorts of things here as elsewhere- make values relative, group the colors, etc\n\n  sdl_line_options <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          x_col = 'delta',\n                          y_lab = 'Proportion met',\n                          x_lab = 'Change in flow',\n                          transx = 'log10',\n                          color_lab = 'Environmental\\ngroup',\n                          colorgroups = 'env_group',\n                          colorset = 'env_obj',\n                          pal_list = obj_pal,\n                          facet_row = 'SWSDLName',\n                          facet_col = '.',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'),\n                          base_lev = 'base',\n                          comp_fun = 'relative',\n                          group_cols = c('env_obj', 'polyID'))\n\nWarning: NaN and Inf introduced in `plot_prep`, likely due to division by zero.\n45 values were lost.\n\n  sdl_line_options\n\nWarning: Removed 30 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n\n\n\n\n\nIn the demo doc, Georgia had the lines as states. This should be flexible enough to do that (well, not states, necessarily, but arbitrary groups).\n\n  sdl_line_catchment <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          x_col = 'delta',\n                          y_lab = 'Proportion met',\n                          x_lab = 'Change in flow',\n                          transx = 'log10',\n                          color_lab = 'Catchment',\n                          colorgroups = NULL,\n                          colorset = 'SWSDLName',\n                          point_group = 'env_obj',\n                          pal_list = list('RColorBrewer::Dark2'),\n                          facet_row = 'env_group',\n                          facet_col = '.',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'),\n                          base_lev = 'base',\n                          comp_fun = 'difference',\n                          group_cols = c('env_obj', 'polyID'),\n                          smooth = FALSE)\n  \n  sdl_line_catchment\n\n\n\n\nAnd we can smooth- if we do it for unique groupings (e.g. same as above), we just get smooth curves, though the loess really hates the sample size.\n\nsdl_line_catchment_smooth <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          x_col = 'delta',\n                          y_lab = 'Proportion met',\n                          x_lab = 'Change in flow',\n                          transx = 'log10',\n                          color_lab = 'Catchment',\n                          colorgroups = NULL,\n                          colorset = 'SWSDLName',\n                          point_group = 'env_obj',\n                          pal_list = list('RColorBrewer::Dark2'),\n                          facet_row = 'env_group',\n                          facet_col = '.',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'),\n                          base_lev = 'base',\n                          comp_fun = 'difference',\n                          group_cols = c('env_obj', 'polyID'),\n                          smooth = TRUE)\n  \n  suppressWarnings(print(sdl_line_catchment_smooth))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nWe can also use smoothing with multiple points, e.g. if we want to group data in some way- maybe use it to put a line through the color groups and ignore individual levels. This is dangerous- it’s an aggregation. So it’s usually better to do the aggregation explicitly, but if you’re OK with a mean aggregation within the implied groups here, it works, and shows the data points, which is nice.\n\n  sdl_smooth_groups <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          x_col = 'delta',\n                          y_lab = 'Proportion met',\n                          x_lab = 'Change in flow',\n                          transx = 'log10',\n                          color_lab = 'Environmental\\ngroup',\n                          colorgroups = 'env_group',\n                          colorset = 'env_obj',\n                          pal_list = obj_pal,\n                          facet_row = 'SWSDLName',\n                          facet_col = '.',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'),\n                          base_lev = 'base',\n                          comp_fun = 'difference',\n                          group_cols = c('polyID'),\n                          smooth = TRUE)\n  \n  suppressWarnings(print(sdl_smooth_groups))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nAnd if we use method = 'lm', we’re just taking a mean aggregation and plotting it with the data points. Jittering should help too.\n\n  sdl_smooth_mean <- obj_sdl_to_plot |>\n    plot_outcomes(y_col = 'allArith',\n                          x_col = 'delta',\n                          y_lab = 'Proportion met',\n                          x_lab = 'Change in flow',\n                          transx = 'log10',\n                          color_lab = 'Environmental\\ngroup',\n                          colorgroups = 'env_group',\n                          colorset = 'env_obj',\n                          pal_list = obj_pal,\n                          facet_row = 'SWSDLName',\n                          facet_col = '.',\n                          scene_pal = scene_pal,\n                          sceneorder = c('down4', 'base', 'up4'),\n                          base_lev = 'base',\n                          comp_fun = 'difference',\n                          group_cols = c('polyID'),\n                          smooth = TRUE,\n                          smooth_method = 'lm',\n                          position = position_jitter(width = 0.01, height = 0))\n  \n  suppressWarnings(print(sdl_smooth_mean))\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "controller/controller_ewr_stepthrough_R.html",
    "href": "controller/controller_ewr_stepthrough_R.html",
    "title": "Scenario controller in detail",
    "section": "",
    "text": "Load the package\nThe controller primarily sets the paths to scenarios, calls the modules, and saves the output and metadata. In normal use, the series of steps below is wrapped. Once we have the directory and set any other needed parameters (e.g. point at a config file), we should just click go, and auto-generate the folder structure, run the ewr, and output the results. I’m stepping through that here so we can see what’s happening and where we need to tweak as formats change; this document is intended to expose some of the inner workings of the black box. Wrapped versions of the controller alone and the whole toolkit are available to illustrate this more normal use."
  },
  {
    "objectID": "controller/controller_ewr_stepthrough_R.html#set-paths",
    "href": "controller/controller_ewr_stepthrough_R.html#set-paths",
    "title": "Scenario controller in detail",
    "section": "Set paths",
    "text": "Set paths\nWe need to set the path to this demonstration. This should all be in a single outer directory project_dir, and there should be an inner directory with the input data /hydrographs. These would typically point to external shared directories. For this simple example though, we put the data inside the repo to make it self contained. The saved data goes to project_dir/module_output automatically. The /hydrographs subdirectory could be made automatic as well, but I’m waiting for the input data format to firm up.\n\nproject_dir = file.path('scenario_example')\nhydro_dir = file.path(project_dir, 'hydrographs')"
  },
  {
    "objectID": "controller/controller_ewr_stepthrough_R.html#format",
    "href": "controller/controller_ewr_stepthrough_R.html#format",
    "title": "Scenario controller in detail",
    "section": "Format",
    "text": "Format\nWe need to pass the data format to the downstream modules so they can parse the data. Currently the demo csvs are created in a format that parses like IQQM, and the netcdf will be. The EWR tool (the only current module) has three options currently 1) 'Bigmod - MDBA', 2) 'IQQM - NSW 10,000 years', and 3) 'Source - NSW (res.csv)'. I’m exposing this for the example, but we can auto-set this to the IQQM default in normal use.\n\n# Options\n# 'Bigmod - MDBA'\n# 'IQQM - NSW 10,000 years'\n# 'Source - NSW (res.csv)'\n\nmodel_format = 'IQQM - NSW 10,000 years'"
  },
  {
    "objectID": "controller/controller_ewr_stepthrough_R.html#climate-info",
    "href": "controller/controller_ewr_stepthrough_R.html#climate-info",
    "title": "Scenario controller in detail",
    "section": "Climate info",
    "text": "Climate info\nLike the format, allowance and climate are arguments to the EWR tool, so I set them here to be clear what we’re doing, but in general they would be set by default.\n\nMINT <- (100 - 0)/100\nMAXT <- (100 + 0 )/100\nDUR <- (100 - 0 )/100\nDRAW <- (100 -0 )/100\n\n# A named list in R becomes a dict in python\nallowance <- list('minThreshold' = MINT, 'maxThreshold' = MAXT, 'duration' = DUR, 'drawdown' = DRAW)\n\nclimate <- 'Standard - 1911 to 2018 climate categorisation'"
  },
  {
    "objectID": "controller/controller_ewr_stepthrough_R.html#set-up-output-directories",
    "href": "controller/controller_ewr_stepthrough_R.html#set-up-output-directories",
    "title": "Scenario controller in detail",
    "section": "Set up output directories",
    "text": "Set up output directories\nWe get the information about the gauges and filepaths project_dir as a dict with make_scenario_info. The scenarios need to be in separate directories inside /hydro_dir, but the files in those directories could come in multiple arrangements. Currently, we allow multiple csvs of single-gauge hydrographs or single csvs of multiple-gauge hydrographs. Once the netcdf format settles down, we will include parsing that. If there are multiple gauges within each csv, they enter the dict as a ‘gauge’ value. If not, the ‘gauge’ value is just the filename, so these need to be gauge numbers. For this example, we have single csvs with multiple gauges.\nThe output directory and subdirs for scenarios is created by make_output_dir, which also returns that outer directory location. The EWR tool needs the paths to the gauge data as a list, so paths_gauges just unfolds the dict to give that. There will likely be continued changes to these functions as we settle on standard directory structures and data formats. These functions are currently all python wrapped by R for historical reasons, but as they are just directory creation and querying could be easily re-written in R to tighten up the package.\n\n# Gives file locations as a dict- \nsceneinfodict <- make_scenario_info_R(hydro_dir)\n# make the output directory structure\noutpath <- make_output_dir_R(project_dir, sceneinfodict)\n# unfold the sceneinfodict to make it easy to get the lists of paths and gauges\n# everyhydro = paths_gauges(sceneinfodict)[0]\n\n# The inner level turned into characters in R and needs to stay as a list. This is annoying but needed *only in R*\nfor (i in 1:length(sceneinfodict)) {\n  for (j in 1:2) {\n    sceneinfodict[[i]][[j]] <- as.list(sceneinfodict[[i]][[j]])\n  }\n}\n\n# more list-making to work from R-Python\neveryhydro <- as.list(paths_gauges(sceneinfodict)[[1]])"
  },
  {
    "objectID": "controller/controller_ewr_stepthrough_R.html#run-the-ewr-tool",
    "href": "controller/controller_ewr_stepthrough_R.html#run-the-ewr-tool",
    "title": "Scenario controller in detail",
    "section": "Run the ewr tool",
    "text": "Run the ewr tool\nNow we run the ewr tool with the parameters given and save the output.\nThere’s an issue with outputType = 'annual' in the version of the EWR tool this was built with. Until I update and test the new EWR tool, skip the annual data. There are still a number of messages printed by the EWR code, about pulling the data and gauge dependencies. Those are useful when using the code, but I’ll suppress printing them here since there are so many.\n\n# To make a list in python, need to have unnamed lists in R\nif (!params$REBUILD_DATA) {\n  outputType <- list('none')\n}\nif (params$REBUILD_DATA) {\n  outputType <- list('summary', 'all')\n}\n\n\newr_out <- run_save_ewrs_R(everyhydro, outpath, model_format = model_format, allowance = allowance, climate = climate, outputType = outputType, datesuffix = FALSE, returnType = list('summary', 'all'))\n\nBriefly, we can see that that has returned dataframes from the EWR (with some leftover pandas datetime environments that we could clean up if we wanted to use this in-memory). Typically, though, we just save this out.\n\nnames(ewr_out)\n\n[1] \"summary\" \"all\"    \n\nstr(ewr_out$summary)\n\n'data.frame':   2736 obs. of  19 variables:\n $ Scenario          : chr  \"hydrographs\\\\base\\\\base\" \"hydrographs\\\\base\\\\base\" \"hydrographs\\\\base\\\\base\" \"hydrographs\\\\base\\\\base\" ...\n $ Gauge             : chr  \"412002\" \"412002\" \"412002\" \"412002\" ...\n $ PlanningUnit      : chr  \"Upper Lachlan River\" \"Upper Lachlan River\" \"Upper Lachlan River\" \"Upper Lachlan River\" ...\n $ EwrCode           : chr  \"BF1_a\" \"BF1_b\" \"BF2_a\" \"BF2_b\" ...\n $ Multigauge        : chr  \"\" \"\" \"\" \"\" ...\n $ EventYears        : num  9 10 9 11 1 1 0 0 1 3 ...\n $ Frequency         : num  82 91 82 100 9 9 0 0 9 27 ...\n $ TargetFrequency   : chr  \"50\" \"100\" \"50\" \"100\" ...\n $ AchievementCount  : num  9 11 9 13 1 1 0 0 1 4 ...\n $ AchievementPerYear: num  0.8182 1 0.8182 1.1818 0.0909 ...\n $ EventCount        : num  9 11 9 13 1 1 0 0 1 4 ...\n $ EventCountAll     : num  43 43 19 19 3 6 0 0 10 21 ...\n $ EventsPerYear     : num  0.8182 1 0.8182 1.1818 0.0909 ...\n $ EventsPerYearAll  : num  3.909 3.909 1.727 1.727 0.273 ...\n $ AverageEventLength: num  78.47 78.47 108.42 108.42 2.67 ...\n $ ThresholdDays     : num  3374 3374 2060 2060 8 ...\n $ MaxInterEventYears: chr  \"1\" \"1\" \"2\" \"2\" ...\n $ NoDataDays        : num  0 0 0 0 0 0 0 0 0 0 ...\n $ TotalDays         : num  3652 3652 3652 3652 3652 ...\n - attr(*, \"pandas.index\")=Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n            ...\n            2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735],\n           dtype='int64', length=2736)\n\nstr(ewr_out$all)\n\n'data.frame':   60050 obs. of  10 variables:\n $ scenario     : chr  \"hydrographs\\\\base\\\\base\" \"hydrographs\\\\base\\\\base\" \"hydrographs\\\\base\\\\base\" \"hydrographs\\\\base\\\\base\" ...\n $ gauge        : chr  \"412033\" \"412033\" \"412033\" \"412033\" ...\n $ pu           : chr  \"Belubula River\" \"Belubula River\" \"Belubula River\" \"Belubula River\" ...\n $ ewr          : chr  \"CF1_a\" \"CF1_a\" \"CF1_a\" \"CF1_a\" ...\n $ waterYear    : num  2009 2009 2009 2010 2012 ...\n $ startDate    :List of 60050\n  ..$ :2010-01-01\n  ..$ :2010-02-23\n  ..$ :2010-03-17\n  ..$ :2010-04-09\n  ..$ :2012-10-16\n  ..$ :2014-11-10\n  ..$ :2014-11-28\n  ..$ :2014-12-20\n  ..$ :2015-01-07\n  ..$ :2015-02-08\n  ..$ :2015-02-23\n  ..$ :2015-02-27\n  ..$ :2015-03-07\n  ..$ :2016-01-16\n  ..$ :2016-02-18\n  ..$ :2016-03-16\n  ..$ :2016-04-02\n  ..$ :2019-03-05\n  ..$ :2019-05-29\n  ..$ :2019-08-09\n  ..$ :2019-09-01\n  ..$ :2010-01-01\n  ..$ :2010-02-23\n  ..$ :2010-03-17\n  ..$ :2010-04-09\n  ..$ :2012-10-16\n  ..$ :2014-11-10\n  ..$ :2014-11-28\n  ..$ :2014-12-20\n  ..$ :2015-01-07\n  ..$ :2015-02-08\n  ..$ :2015-02-23\n  ..$ :2015-02-27\n  ..$ :2015-03-07\n  ..$ :2016-01-16\n  ..$ :2016-02-18\n  ..$ :2016-03-16\n  ..$ :2016-04-02\n  ..$ :2019-03-05\n  ..$ :2019-05-29\n  ..$ :2019-08-09\n  ..$ :2019-09-01\n  ..$ :2010-01-01\n  ..$ :2010-02-23\n  ..$ :2010-03-17\n  ..$ :2010-04-09\n  ..$ :2012-10-16\n  ..$ :2014-11-10\n  ..$ :2014-11-28\n  ..$ :2014-12-20\n  ..$ :2015-01-07\n  ..$ :2015-02-08\n  ..$ :2015-02-23\n  ..$ :2015-02-27\n  ..$ :2015-03-07\n  ..$ :2016-01-16\n  ..$ :2016-02-18\n  ..$ :2016-03-16\n  ..$ :2016-04-02\n  ..$ :2019-03-05\n  ..$ :2019-05-29\n  ..$ :2019-08-09\n  ..$ :2019-09-01\n  ..$ :2010-02-14\n  ..$ :2010-03-08\n  ..$ :2010-07-17\n  ..$ :2010-07-30\n  ..$ :2011-07-01\n  ..$ :2011-11-11\n  ..$ :2011-11-23\n  ..$ :2012-07-01\n  ..$ :2012-10-17\n  ..$ :2013-01-07\n  ..$ :2013-02-22\n  ..$ :2013-03-17\n  ..$ :2013-03-28\n  ..$ :2013-04-18\n  ..$ :2013-04-23\n  ..$ :2013-05-10\n  ..$ :2013-05-15\n  ..$ :2013-07-01\n  ..$ :2013-10-22\n  ..$ :2013-11-13\n  ..$ :2013-11-28\n  ..$ :2013-12-07\n  ..$ :2013-12-19\n  ..$ :2014-01-10\n  ..$ :2014-01-16\n  ..$ :2014-01-20\n  ..$ :2014-02-05\n  ..$ :2014-02-11\n  ..$ :2014-02-13\n  ..$ :2014-02-26\n  ..$ :2014-03-25\n  ..$ :2014-04-16\n  ..$ :2014-05-05\n  ..$ :2014-05-28\n  ..$ :2014-07-01\n  ..$ :2014-10-07\n  .. [list output truncated]\n $ endDate      :List of 60050\n  ..$ :2010-02-13\n  ..$ :2010-03-06\n  ..$ :2010-04-06\n  ..$ :2010-07-13\n  ..$ :2012-10-16\n  ..$ :2014-11-15\n  ..$ :2014-12-04\n  ..$ :2014-12-26\n  ..$ :2015-01-09\n  ..$ :2015-02-13\n  ..$ :2015-02-23\n  ..$ :2015-03-01\n  ..$ :2015-04-04\n  ..$ :2016-01-19\n  ..$ :2016-03-12\n  ..$ :2016-03-16\n  ..$ :2016-05-03\n  ..$ :2019-03-06\n  ..$ :2019-06-09\n  ..$ :2019-08-14\n  ..$ :2019-12-31\n  ..$ :2010-02-13\n  ..$ :2010-03-06\n  ..$ :2010-04-06\n  ..$ :2010-07-13\n  ..$ :2012-10-16\n  ..$ :2014-11-15\n  ..$ :2014-12-04\n  ..$ :2014-12-26\n  ..$ :2015-01-09\n  ..$ :2015-02-13\n  ..$ :2015-02-23\n  ..$ :2015-03-01\n  ..$ :2015-04-04\n  ..$ :2016-01-19\n  ..$ :2016-03-12\n  ..$ :2016-03-16\n  ..$ :2016-05-03\n  ..$ :2019-03-06\n  ..$ :2019-06-09\n  ..$ :2019-08-14\n  ..$ :2019-12-31\n  ..$ :2010-02-13\n  ..$ :2010-03-06\n  ..$ :2010-04-06\n  ..$ :2010-07-13\n  ..$ :2012-10-16\n  ..$ :2014-11-15\n  ..$ :2014-12-04\n  ..$ :2014-12-26\n  ..$ :2015-01-09\n  ..$ :2015-02-13\n  ..$ :2015-02-23\n  ..$ :2015-03-01\n  ..$ :2015-04-04\n  ..$ :2016-01-19\n  ..$ :2016-03-12\n  ..$ :2016-03-16\n  ..$ :2016-05-03\n  ..$ :2019-03-06\n  ..$ :2019-06-09\n  ..$ :2019-08-14\n  ..$ :2019-12-31\n  ..$ :2010-02-17\n  ..$ :2010-03-10\n  ..$ :2010-07-28\n  ..$ :2011-06-30\n  ..$ :2011-11-06\n  ..$ :2011-11-15\n  ..$ :2012-06-30\n  ..$ :2012-10-15\n  ..$ :2013-01-01\n  ..$ :2013-02-16\n  ..$ :2013-03-11\n  ..$ :2013-03-21\n  ..$ :2013-04-13\n  ..$ :2013-04-20\n  ..$ :2013-05-05\n  ..$ :2013-05-11\n  ..$ :2013-06-30\n  ..$ :2013-10-19\n  ..$ :2013-11-06\n  ..$ :2013-11-23\n  ..$ :2013-11-30\n  ..$ :2013-12-17\n  ..$ :2014-01-06\n  ..$ :2014-01-14\n  ..$ :2014-01-16\n  ..$ :2014-02-01\n  ..$ :2014-02-07\n  ..$ :2014-02-11\n  ..$ :2014-02-22\n  ..$ :2014-03-21\n  ..$ :2014-04-14\n  ..$ :2014-04-19\n  ..$ :2014-05-25\n  ..$ :2014-06-30\n  ..$ :2014-10-05\n  ..$ :2014-10-30\n  .. [list output truncated]\n $ eventDuration: num  44 12 21 96 1 6 7 7 3 6 ...\n $ eventLength  : num  44 12 21 96 1 6 7 7 3 6 ...\n $ Multigauge   : chr  \"\" \"\" \"\" \"\" ...\n - attr(*, \"pandas.index\")=Int64Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,\n                9,\n            ...\n            60040, 60041, 60042, 60043, 60044, 60045, 60046, 60047, 60048,\n            60049],\n           dtype='int64', length=60050)"
  },
  {
    "objectID": "controller/controller_ewr_wrapped_R.html",
    "href": "controller/controller_ewr_wrapped_R.html",
    "title": "Scenario controller",
    "section": "",
    "text": "Load the package\nThe controller primarily sets the paths to scenarios, calls the modules, and saves the output and metadata. In normal use, we set the directory and any other needed parameters (e.g. point at a config file), and the controller functions auto-generate the folder structure, run the ewr, and output the results. This can be taken up a level to the the whole toolkit, where the controller and subsequent steps are all run at once. A detailed stepthrough of what happens in the controller is also available, useful to see what is happening under the hood."
  },
  {
    "objectID": "controller/controller_ewr_wrapped_R.html#set-paths",
    "href": "controller/controller_ewr_wrapped_R.html#set-paths",
    "title": "Scenario controller",
    "section": "Set paths",
    "text": "Set paths\nWe need to set the path to this demonstration. This should all be in a single outer directory project_dir, and there should be an inner directory with the input data /hydrographs. These would typically point to external shared directories. For this simple example though, we put the data inside the repo to make it self contained. The saved data goes to project_dir/module_output automatically. The /hydrographs subdirectory could be made automatic as well, but I’m waiting for the input data format to firm up.\n\nproject_dir = file.path('scenario_example')\nhydro_dir = file.path(project_dir, 'hydrographs')"
  },
  {
    "objectID": "controller/controller_ewr_wrapped_R.html#control-output-and-return",
    "href": "controller/controller_ewr_wrapped_R.html#control-output-and-return",
    "title": "Scenario controller",
    "section": "Control output and return",
    "text": "Control output and return\nTo determine what to save and what to return to the active session, use outputType and returnType, respectively. Each of them can take a list of any of 'none', 'summary', 'annual', 'all', with more I need to add to reflect new EWR functionality (e.g. returnType = list('summary', 'all') in R or returnType = ['summary', 'all] in python). These have to be lists to work right- To make a list in python, need to have unnamed lists in R.\nThere’s an issue with 'annual' in py-ewr- I’m getting an error inside the EWR tool. Until I updated the EWR version, skip that.\n\nreturnType <- list('summary', 'all')\n\n# We use outputtype to save, so only save outputs if params$REBUILD_DATA is TRUE\n# To make a list in python, need to have unnamed lists in R\nif (!params$REBUILD_DATA) {\n  outputType <- list('none')\n}\nif (params$REBUILD_DATA) {\n  outputType <- list('summary', 'all')\n}"
  },
  {
    "objectID": "controller/controller_ewr_wrapped_R.html#run-and-save",
    "href": "controller/controller_ewr_wrapped_R.html#run-and-save",
    "title": "Scenario controller",
    "section": "Run and save",
    "text": "Run and save\nThe above is all user parameters. All the formatting, running, and saving is then handled with the wrapper function prep_run_save_ewrs. See stepthrough for an expanded version used to run test data and expand each step to make testing/changes more transparent.\n\newr_out <- prep_run_save_ewrs_R(scenario_dir = hydro_dir, output_dir = project_dir, outputType = outputType, returnType = returnType)\n\nNow we have a summary and all.\n\newr_out$summary\n\n\n\n  \n\n\n\n\newr_out$all"
  },
  {
    "objectID": "controller/controller_overview.html",
    "href": "controller/controller_overview.html",
    "title": "Controller",
    "section": "",
    "text": "The toolkit takes hydrographs as input data and then processes it through downstream modules, performs aggregation and analyses, and produces outputs. The ‘Controller’ component of the toolkit points to that input data, and sends it off to the modules with arguments controlling how that happens. It may also determine how ongoing processing occurs.\nIn typical use, the controller simply points to the input data and initiates processing steps according to the user. Examples of this for the controller alone and the whole toolkit are available to illustrate this, as well as a stepthrough to better understand what the controller is doing."
  },
  {
    "objectID": "controller/flow_scaling_control.html",
    "href": "controller/flow_scaling_control.html",
    "title": "Flow scaling controller",
    "section": "",
    "text": "This will be the controller example for the flow scaling demonstration"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_in_memory.html",
    "href": "full_toolkit/WERP_toolkit_in_memory.html",
    "title": "WERP_toolkit_demo",
    "section": "",
    "text": "This document provides a template for running through the toolkit in a single document, retaining everything in-memory (no intermediate saving). Intermediate saving is a very simple flip of a switch, demoed in its own doc. See the repo readme for overall structure of the repo and package installation.\nLoad the package"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_in_memory.html#structure",
    "href": "full_toolkit/WERP_toolkit_in_memory.html#structure",
    "title": "WERP_toolkit_demo",
    "section": "Structure",
    "text": "Structure\nTo run the toolkit, we need to provide paths to directories for input data and output data, as well as arguments for the aggregation.\nOne option is to do that in a parameters file, and then treat this as a parameterised notebook.\nThe other option is to have this be the parameterising file, so we can have a bit more text around the parameterisations. Not sure which makes more sense, but they’re not mutually exclusive, and the answer likely depends on whether we’re working interactively or want to fire off 1,000 runs."
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_in_memory.html#directories",
    "href": "full_toolkit/WERP_toolkit_in_memory.html#directories",
    "title": "WERP_toolkit_demo",
    "section": "Directories",
    "text": "Directories\n\nInput and output directories\nUse the scenario_example/ directory created to capture a very simple demonstration case of 46 gauges in three catchments for 10 years.\nNormally scenario_dir should point somewhere external (though keeping it inside or alongside the hydrograph data is a good idea.). But here, I’m generating test data, so I’m keeping it in the repo. I will probably change that when I move to Azure.\n\n# Outer directory for scenario\nscenario_dir = file.path('scenario_example')\n\n# Preexisting data\n# Hydrographs (expected to exist already)\nhydro_dir = file.path(scenario_dir, 'hydrographs')\n# Geographic data (relevant polygons and gauge locations- use canonical in pkg)\ngeo_data_dir <- system.file(\"extdata\", package = 'werptoolkitr')\n\n# Generated data\n# EWR outputs (will be created here in controller, read from here in aggregator)\newr_results <- file.path(scenario_dir, 'module_output', 'EWR')\n# outputs of aggregator. There may be multiple modules\n# NULL doesn't save it, but holds in memory.\nagg_results <- NULL # file.path(scenario_dir, 'agg_output', 'EWR')"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_in_memory.html#controller",
    "href": "full_toolkit/WERP_toolkit_in_memory.html#controller",
    "title": "WERP_toolkit_demo",
    "section": "Controller",
    "text": "Controller\n\nFormat\nFormat will be IQQM, at least for a while\n\n# Options\n# 'Bigmod - MDBA'\n# 'IQQM - NSW 10,000 years'\n# 'Source - NSW (res.csv)'\n\nmodel_format = 'IQQM - NSW 10,000 years'\n\n\n\nClimate info\nI have the functions set up to have default values of MINT, MAXT, DUR, and DRAW. Those can be set though if the user wants.\n\nMINT = (100 - 0)/100\nMAXT = (100 + 0 )/100\nDUR = (100 - 0 )/100\nDRAW = (100 -0 )/100\n\nclimate = 'Standard - 1911 to 2018 climate categorisation'\n\n\n\nControl output and return\nTo determine what to save and what to return to the active session, use outputType and returnType, respectively. Each of them can take a list of any of 'none', 'summary', 'annual', 'all', with more I need to add to reflect new EWR functionality. These have to be lists to work right- To make a list in python, need to have unnamed lists in R. Named lists become dicts.\nthere’s an issue with 'annual' in py-ewr- I’m getting an error inside the EWR tool. Until the bug is found, skip that\nTODO\n\nthis should work either reading off-disk (using outputType other than ‘none’), or keeping in-memory and flowing straight into the aggregator. TEST BOTH.\nuse {targets} or similar to controll rebuilding that output\n\n\noutputType <- list('none')\nreturnType <- list('summary') # list('summary', 'all')"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_in_memory.html#aggregator",
    "href": "full_toolkit/WERP_toolkit_in_memory.html#aggregator",
    "title": "WERP_toolkit_demo",
    "section": "Aggregator",
    "text": "Aggregator\nI’m just going to keep this simple, passing one aggregation list and using the read_and_agg wrapper. See the more detailed documents for the different ways to specify those aggregation lists.\n\nWhat to aggregate\nThe aggregator needs to know which set of EWR outputs to use (to navigate the directory or list structure). It should accept multiple types, but that’s not well tested, so for now just use one.\n\naggType <- 'summary'\n\nWe need to tell it the variable to aggregate, and any grouping variables other than the themes and spatial groups. Typically, scenario will be a grouper, but there may be others.\n\nagg_groups <- 'scenario'\nagg_var <- 'ewr_achieved'\n\nDo we want it to return to the active session? For this demo, I’m keeping nothing interactive.\n\naggReturn <- TRUE\n\n\n\nHow to aggregate\nFundamentally, the aggregator needs paths and two lists\n\nsequence of aggregations\nsequence of aggregation functions (can be multiple per step)\n\nHere, I’m using an interleaved list of theme and spatial aggregations (see the detailed docs for more explanation), and applying only a single aggregation function at each step for simplicity. Those steps are specified a range of different ways to give a small taste of the flexibility here, but see the spatial and theme docs for more examples.\n\naggseq <- list(ewr_code = c('ewr_code_timing', 'ewr_code'),\n               env_obj =  c('ewr_code', \"env_obj\"),\n               resource_plan = resource_plan_areas,\n               Specific_goal = c('env_obj', \"Specific_goal\"),\n               catchment = cewo_valleys,\n               Objective = c('Specific_goal', 'Objective'),\n               mdb = basin,\n               target_5_year_2024 = c('Objective', 'target_5_year_2024'))\n\n\nfunseq <- list(c('CompensatingFactor'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               list(wm = ~weighted.mean(., w = area, \n                                        na.rm = TRUE)),\n               c('ArithmeticMean'),\n               \n               list(wm = ~weighted.mean(., w = area, \n                                    na.rm = TRUE)),\n               c('ArithmeticMean'))"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_in_memory.html#controller-1",
    "href": "full_toolkit/WERP_toolkit_in_memory.html#controller-1",
    "title": "WERP_toolkit_demo",
    "section": "Controller",
    "text": "Controller\n\newr_out <- prep_run_save_ewrs_R(hydro_dir, scenario_dir, model_format, climate,\noutputType, returnType)"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_in_memory.html#aggregator-1",
    "href": "full_toolkit/WERP_toolkit_in_memory.html#aggregator-1",
    "title": "WERP_toolkit_demo",
    "section": "Aggregator",
    "text": "Aggregator\nTODO LET THIS SAVE. And tell it where.\n\naggout <- read_and_agg(datpath = ewr_results, \n             type = aggType,\n             geopath = bom_basin_gauges,\n             causalpath = causal_ewr,\n             groupers = agg_groups,\n             aggCols = agg_var,\n             aggsequence = aggseq,\n             funsequence = funseq,\n             saveintermediate = TRUE,\n             namehistory = FALSE,\n             keepAllPolys = TRUE,\n             returnList = aggReturn,\n             savepath = agg_results)\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(agg_groups)\n\n  # Now:\n  data %>% select(all_of(agg_groups))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\n\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(agg_var)\n\n  # Now:\n  data %>% select(all_of(agg_var))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\n\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\n\nJoining, by = \"gauge\"\nJoining, by = c(\"gauge\", \"ewr_code\", \"ewr_code_timing\", \"PlanningUnitID\")\nJoining, by = \"gauge\"\nJoining, by = c(\"gauge\", \"ewr_code\", \"PlanningUnitID\")\nJoining, by = \"env_obj\"\n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nJoining, by = \"Specific_goal\"\n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nJoining, by = \"Objective\""
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_save_steps.html",
    "href": "full_toolkit/WERP_toolkit_save_steps.html",
    "title": "Run full toolkit (saving)",
    "section": "",
    "text": "This document provides a template for running through the toolkit, saving the output of each step along the way in a single document (e.g. not running the Controller, Aggregator, and Comparer as separate notebooks). Retaining everything in-memory is a very simple flip of a switch, demoed in its own doc."
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_save_steps.html#structure",
    "href": "full_toolkit/WERP_toolkit_save_steps.html#structure",
    "title": "Run full toolkit (saving)",
    "section": "Structure",
    "text": "Structure\nTo run the toolkit, we need to provide paths to directories for input data and output data, as well as arguments for the aggregation.\nOne option is to do that in a parameters file, and then treat this as a parameterised notebook.\nThe other option is to have this be the parameterising file, so we can have a bit more text around the parameterisations. These are not mutually exclusive options, just different interfaces to the code. The answer for any particular analysis likely depends on whether we’re working interactively or want to fire off a large number of parallel runs."
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_save_steps.html#directories",
    "href": "full_toolkit/WERP_toolkit_save_steps.html#directories",
    "title": "Run full toolkit (saving)",
    "section": "Directories",
    "text": "Directories\n\nInput and output directories\nUse the scenario_example/ directory created to capture a very simple demonstration case of 46 gauges in three catchments for 10 years.\nNormally project_dir should point somewhere external (though keeping it inside or alongside the hydrograph data is a good idea.). But here, I’m generating test data, so I’m keeping it in the repo.The flow scaling example takes the more typical approach of pointing to data external to the repo.\n\n# Outer directory for scenario\nproject_dir = file.path('scenario_example')\n\n# Preexisting data\n# Hydrographs (expected to exist already)\nhydro_dir = file.path(project_dir, 'hydrographs')\n\n# Generated data\n# EWR outputs (will be created here in controller, read from here in aggregator)\newr_results <- file.path(project_dir, 'module_output', 'EWR')\n\n# outputs of aggregator. There may be multiple modules\nagg_results <- file.path(project_dir, 'aggregator_output')"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_save_steps.html#controller",
    "href": "full_toolkit/WERP_toolkit_save_steps.html#controller",
    "title": "Run full toolkit (saving)",
    "section": "Controller",
    "text": "Controller\nWe use the default IQQM model format and climate categorisations, though those could be passed here as well (see controller).\n\nControl output and return\nTo determine what to save and what to return to the active session, use outputType and returnType, respectively. Each of them can take a list of any of 'none', 'summary', 'annual', 'all'. For this demonstration I’ll just use summary and not return anything to memory.\n\noutputType <- list('summary')\nreturnType <- list('none') # list('summary', 'all')"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_save_steps.html#aggregator",
    "href": "full_toolkit/WERP_toolkit_save_steps.html#aggregator",
    "title": "Run full toolkit (saving)",
    "section": "Aggregator",
    "text": "Aggregator\nTo keep this simple, we use one aggregation list and the read_and_agg wrapper to only have to pass paths. See the more detailed documents for the different ways to specify those aggregation lists.\n\nWhat to aggregate\nThe aggregator needs to know which set of EWR outputs to use (to navigate the directory or list structure). It should accept multiple types, but that’s not well tested, so for now just use one.\n\naggType <- 'summary'\n\nWe need to tell it the variable to aggregate, and any grouping variables other than the themes and spatial groups. Typically, scenario will be a grouper, but there may be others.\n\nagg_groups <- 'scenario'\nagg_var <- 'ewr_achieved'\n\nDo we want it to return to the active session? For this demo, nothing should return here- we’re saving outputs, not returning them to the session.\n\naggReturn <- FALSE\n\n\n\nHow to aggregate\nFundamentally, the aggregator needs paths and two lists\n\nsequence of aggregations\nsequence of aggregation functions (can be multiple per step)\n\nHere, I’m using an interleaved list of theme and spatial aggregations (see the detailed docs for more explanation), and applying only a single aggregation function at each step for simplicity. Those steps are specified a range of different ways to give a small taste of the flexibility here, but see the spatial and theme docs for more examples.\n\naggseq <- list(ewr_code = c('ewr_code_timing', 'ewr_code'),\n               env_obj =  c('ewr_code', \"env_obj\"),\n               sdl_units = sdl_units,\n               Specific_goal = c('env_obj', \"Specific_goal\"),\n               catchment = cewo_valleys,\n               Objective = c('Specific_goal', 'Objective'),\n               mdb = basin,\n               target_5_year_2024 = c('Objective', 'target_5_year_2024'))\n\n\nfunseq <- list(c('CompensatingFactor'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               c('ArithmeticMean'),\n               list(wm = ~weighted.mean(., w = area, \n                                        na.rm = TRUE)),\n               c('ArithmeticMean'),\n               \n               list(wm = ~weighted.mean(., w = area, \n                                    na.rm = TRUE)),\n               c('ArithmeticMean'))"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_save_steps.html#controller-1",
    "href": "full_toolkit/WERP_toolkit_save_steps.html#controller-1",
    "title": "Run full toolkit (saving)",
    "section": "Controller",
    "text": "Controller\n\nif (REBUILD_DATA) {\n  ewr_out <- prep_run_save_ewrs_R(scenario_dir = hydro_dir, \n                                  output_dir = project_dir, \n                                  outputType = outputType,\n                                  returnType = returnType)\n}"
  },
  {
    "objectID": "full_toolkit/WERP_toolkit_save_steps.html#aggregator-1",
    "href": "full_toolkit/WERP_toolkit_save_steps.html#aggregator-1",
    "title": "Run full toolkit (saving)",
    "section": "Aggregator",
    "text": "Aggregator\n\nif (REBUILD_DATA) {\n  aggout <- read_and_agg(datpath = ewr_results, \n             type = aggType,\n             geopath = bom_basin_gauges,\n             causalpath = causal_ewr,\n             groupers = agg_groups,\n             aggCols = agg_var,\n             aggsequence = aggseq,\n             funsequence = funseq,\n             saveintermediate = TRUE,\n             namehistory = FALSE,\n             keepAllPolys = TRUE,\n             returnList = aggReturn,\n             savepath = agg_results)\n}\n\nIt would be straightforward here to run the comparer as well, but as discussed above, there is not much reason until we settle on a couple canonical outputs."
  },
  {
    "objectID": "full_toolkit/flow_scaling_full.html",
    "href": "full_toolkit/flow_scaling_full.html",
    "title": "Flow scaling full toolkit",
    "section": "",
    "text": "This will be the notebook to run the full toolkit in one go with the flow scaling demonstration. We will likely want a notebook version and a parameterized yaml version (but not an in-memory)."
  },
  {
    "objectID": "full_toolkit/full_toolkit_overview.html",
    "href": "full_toolkit/full_toolkit_overview.html",
    "title": "Full toolkit overview",
    "section": "",
    "text": "The toolkit may be used stepwise, that is calling the Controller, Aggregator, and Comparer. But it can also be called in one go, feeding all necessary parameters in at once. In this case, we can think of the Controller as simply having larger scope, passing arguments all the way through instead of just to the modules. This can be done in-memory, or saving outputs at each step. In either case, it can be controlled interactively in notebooks, or with a params.yml file, which currently operates a parameterised notebook, but could be made to simply be ingested by Rscript at the command line.\nIn typical use, we will likely this full-toolkit approach, but making sure we save the output of at least the aggregator. It is very likely that we will want to make different plots for different purposes, and will not necessarily know what they are a-priori. So we’ll want the ability to run additional Comparer notebooks."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "WERP toolkit demonstration",
    "section": "",
    "text": "This repo provides a templates and documentation for using the toolkit in various ways. The full_toolkit directory provides notebooks for running the full toolkit in single files, either in memory or saving each step and reading back in. Within this repo there are also documents providing templates for each step separately (in controller and aggregation directories). These files allow exploring more of the options for calling, particularly for aggregation. There are also a few templates or examples of notebooks that are not in the toolkit flow per se, but are necessary for it (scenario_creation and causal_networks).\nToolkit flow\nIn use, the toolkit expects that scenario hydrographs will be available and the causal networks are defined."
  },
  {
    "objectID": "index.html#dependencies",
    "href": "index.html#dependencies",
    "title": "WERP toolkit demonstration",
    "section": "Dependencies",
    "text": "Dependencies\nThe {werptoolkitr} package needs to be installed to provide all functions used here. It also provides some necessary data for the causal network relationships, and (at least for now) canonical shapefiles that have been prepped.\nTypically install it from github, which requires a github PAT because it’s private (acquired with set_github_pat), and a subdir argument because the R package is nested. It may also need a ref argument to load a branch or commit other than main.\n\n# GITHUB INSTALL\ncredentials::set_github_pat()\ndevtools::install_github(\"MDBAuth/WERP_toolkit\", ref = 'packaging', subdir = 'werptoolkitr', force = TRUE)\n\nFor rapid development, it can be much easier to install from local (or even just to load_all), but these rely on paths that aren’t portable.\n\n# LOCAL INSTALL- easier for quick iterations, but need a path.\ndevtools::install_local(\"C:/path/to/WERP_toolkit/werptoolkitr\", force = TRUE)\n\n# And for very fast iteration (no building, but exposes too much, often)\ndevtools::load_all(\"C:/path/to/WERP_toolkit/werptoolkitr\")\n\nLoad the package\n\nlibrary(werptoolkitr)"
  },
  {
    "objectID": "overview/spatial_data.html",
    "href": "overview/spatial_data.html",
    "title": "Spatial data",
    "section": "",
    "text": "library(werptoolkitr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "overview/spatial_data.html#visualizing-spatial-data-from-werptoolkitr",
    "href": "overview/spatial_data.html#visualizing-spatial-data-from-werptoolkitr",
    "title": "Spatial data",
    "section": "Visualizing spatial data from werptoolkitr",
    "text": "Visualizing spatial data from werptoolkitr\nThe {werptoolkitr} package provides a standard set of spatial data, generated in data_creation/spatial_data_creation.qmd. Here, we make quick plots of the data so we know what it looks like.\nThe datasets are bom_basin_gauges (points), and basin (the MDB as a single polygon), sdl_units, resource_plan_areas, and cewo_valleys. Relevant to the case study- the original polygon used was the Macquarie-Castlereagh in the sdls. The crs all match from the creation.\n\nBasin\n\nggplot(basin) + geom_sf(fill = 'powderblue')\n\n\n\n\n\n\nResource plan areas\n\nggplot(resource_plan_areas) + geom_sf(aes(fill = SWWRPANAME), show.legend = FALSE) +\n  geom_sf_label(aes(label = SWWRPANAME), size = 3, label.padding = unit(0.1, 'lines')) + \n  colorspace::scale_fill_discrete_qualitative(palette = 'Set2')\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\nThese have ‘SW’ codes\n\nresource_plan_areas\n\n\n\n  \n\n\n\n\n\nSDL plan areas\n\nggplot(sdl_units) + geom_sf(aes(fill = SWSDLName), show.legend = FALSE) +\n  geom_sf_label(aes(label = SWSDLName), size = 3, label.padding = unit(0.1, 'lines')) + \n  colorspace::scale_fill_discrete_qualitative(palette = 'Set2')\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\nThese have ‘SS’ codes.\n\nsdl_units\n\n\n\n  \n\n\n\n\n\nCatchments\n\nggplot(cewo_valleys) + geom_sf(aes(fill = ValleyName), show.legend = FALSE) +\n  geom_sf_label(aes(label = ValleyName), size = 3, label.padding = unit(0.1, 'lines')) + \n  colorspace::scale_fill_discrete_qualitative(palette = 'Set2')\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\nThese have names, ID, and ValleyCodes\n\ncewo_valleys\n\n\n\n  \n\n\n\n\n\nGauges\n\nggplot() + \n  geom_sf(data = basin, fill = 'powderblue') +\n  geom_sf(data = bom_basin_gauges)\n\n\n\n\n\nbom_basin_gauges"
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_testing.html",
    "href": "scenario_creation/development_testing/scaling_testing.html",
    "title": "Scaling functions",
    "section": "",
    "text": "I’m using flow_scaling.qmd to pull the gauges and scale them, but actually getting the scaling relationships is independent, and so doesn’t need to happen in the same script. I’ll do that here.\n\nlibrary(werptoolkitr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(vicwater)\n\nSet up some directories. Once we move to MDBA, these will be easier to point at in a shared way.\n\nscenario_dir <- '../flow_scaling_data'\nhydro_dir <- file.path(scenario_dir, 'hydrographs')\nscaling_dir <- file.path(scenario_dir, 'CC_Scenarios_WRPs')"
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_testing.html#background-and-definitions",
    "href": "scenario_creation/development_testing/scaling_testing.html#background-and-definitions",
    "title": "Scaling functions",
    "section": "Background and definitions",
    "text": "Background and definitions\nBring in David’s climate scenarios NOTE: Only go to 31-Jan-2019.\nPrec and ETp (PE) are historical (not needed here)\nSimR0 is simulated historical runnoff using actual historical Prec and ETp (PE)\nSimR1 - SimR7 are simulated with +7% PE but different Rainfall:\n\n-20%\n-15% (“High change scenario”)\n-10%\n-5% (“Moderate change scenario”)\n+0%\n+5% (“Low change scenario”)\n+10%\n\n\nMake numeric metadata\nThe format of this may change, but we’re going to want something. Lists are going to be easier to yaml than dataframes (though a dataframe is easier to construct).\nThough it sounds like the scenario yamls are likely to not be lists, but single values, ie each one gets their own value to create it, and that’s it (which makes sense).\n\nrain_multiplier <- seq(from = 0.8, to = 1.1, by = 0.05) %>% \n  setNames(paste0('R', 1:7))\n\nscenario_meta <- list(\n  PE_multiplier = 1.07,\n  rain_multiplier = rain_multiplier,\n  scenario_name = names(rain_multiplier)\n)\n\n# Don't run yet, since I don't know the format we'll be using, but this works to create yaml metadata\n# yaml::write_yaml(scenario_meta, file = 'path/to/file.yml')\n\nSuggestion is to compare each scenario to simulated historic baseline, work out the ratio and apply the difference to the gauge records… Would be good to get ‘cease to flow’ events for the scenarios."
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_testing.html#bring-in-the-data",
    "href": "scenario_creation/development_testing/scaling_testing.html#bring-in-the-data",
    "title": "Scaling functions",
    "section": "Bring in the data",
    "text": "Bring in the data\nGet the list of files and read them in (to a list of dfs)\n\nCCSc_FileList <- list.files(scaling_dir, pattern = '.csv', \n                            full.names = TRUE)\n\nscenario_list <- purrr::map(CCSc_FileList,\n                            \\(x) read_csv(file = x, id = 'path')) %>% \n  setNames(stringr::str_extract(CCSc_FileList, \"SS[0-9]+\"))"
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_testing.html#scale",
    "href": "scenario_creation/development_testing/scaling_testing.html#scale",
    "title": "Scaling functions",
    "section": "Scale",
    "text": "Scale\nWe follow the basic method for q-q scaling from climate change australia, with the following modifications\n\nUse 2% bins (e.g. 50 bins) instead of 10 + 10 in final\nMonth-matching quantiles (e.g. 90th %ile for June separate from September). The method given might do that too, it’s unclear."
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_testing.html#stacked",
    "href": "scenario_creation/development_testing/scaling_testing.html#stacked",
    "title": "Scaling functions",
    "section": "Stacked",
    "text": "Stacked\nDo a bit of cleanup separate from the processing pipe\n\ntest_stack <- testdf %>% \n  mutate(sdl = stringr::str_extract(path, \"SS[0-9]+\")) %>% \n  select(sdl, Year, Month, Day, starts_with('Sim')) %>% \n  pivot_longer(cols = starts_with('Sim'), \n               names_to = 'scenario', values_to = 'runoff')\n\n\ntest_q_s <- test_stack %>% \n  group_by(scenario, Month) %>%\n  # Get the quantiles and their means\n  summarise(qmean = get_qmean(runoff)) %>% \n  # Clean up- unnest because each summary is a df of both q and m, \n  tidyr::unnest(cols = qmean) %>% \n  ungroup()\n\n`summarise()` has grouped output by 'scenario', 'Month'. You can override using\nthe `.groups` argument.\n\nrelfun <- function(x,y) {(x-y)/y}\n# Use `werptoolkitr::baseline_compare` so we don't have to manually extract\ntest_q_s <- test_q_s %>% \n  group_by(scenario, Month) %>% \n  baseline_compare(compare_col = 'scenario', base_lev = 'SimR0', \n                   values_col = 'mean', \n                   comp_fun = relfun) %>% \n  ungroup() %>% \n  select(scenario = scenario.x, everything(), \n         change_ratio = relfun_mean, \n         -scenario.y)\n\nAdding missing grouping variables: `scenario`\n\n\n\nPlot\nJust the means, then the change ratios\nThis will be easier stacked. I actually think the whole thing is likely easier stacked.\n\n# just look at a couple quantiles\ntest_q_s %>% \n  filter(quantile %in% c(1,25,50)) %>% \nggplot(mapping = aes(x = scenario, y = mean, fill = as.factor(quantile))) + geom_col(position = position_dodge())\n\n\n\n\nProbably want to look at different things on those dims\nDo the quantiles look smooth\n\n# just look at a couple quantiles\ntest_q_s %>% \nggplot(mapping = aes(x = as.factor(quantile), y = mean, fill = scenario)) + geom_col(position = position_dodge()) + facet_grid(Month ~ scenario)\n\n\n\n\n\ntest_q_s %>% \n  ggplot(mapping = aes(x = quantile, \n                       y = mean, color = scenario)) + \n  geom_line() + \n  facet_wrap('Month')"
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_testing.html#whats-the-process-here",
    "href": "scenario_creation/development_testing/scaling_testing.html#whats-the-process-here",
    "title": "Scaling functions",
    "section": "What’s the process here?",
    "text": "What’s the process here?\n\nfind the quantile of each value in the orig_hydro data for each group-unit (e.g. Month)\n\nNot with get_qmean, because we just want to identify quantile of each value, not summarise to their mean.\n\nmultiply by change ratio for each quantile, group-unit, and scenario\ndo that over all the sdl units\n\n\nfind the quantiles for historical data\nWe don’t want get_qmean, but instead just want to ID the quantile of each value.\nAnd we need to group\nWrite the function. I can probably do this above, and use it in get_qmean\n\nget_q <- function(vals, q_perc) {\n  qs <- quantile(vals, probs = seq(0,1, q_perc), type = 5)\n  binvec <- cut(vals, qs, include.lowest = TRUE, labels = FALSE)\n  return(binvec)\n}\n\nWe need a grouping variable, been using Month, so stick with that for testing. means we need to create it. Obviously this could get more complicated\n\ntest_hydro <- test_hydro %>% \n  mutate(Month = lubridate::month(time))\n\nNow let’s find the quantile of each value\n\ntest_hydro_q <- test_hydro %>% \n  group_by(Month) %>% \n  mutate(quantile = get_q(value, q_perc = 0.02)) %>% \n  ungroup()\n\nPlot check\n\nggplot(test_hydro_q, aes(x = time, y = value, color = quantile)) + \n  geom_point() + geom_line() + \n  scale_color_viridis_c()\n\n\n\n\nLooks right, but we can also check within months\n\nggplot(test_hydro_q, aes(x = lubridate::mday(time), \n                         y = value, color = quantile,\n                         group = lubridate::year(time))) +\n  geom_point() + geom_line() +\n  facet_wrap('Month') + \n  scale_color_viridis_c()\n\n\n\n\n\n\nNEED A MATCH TO SDL UNIT STEP\nI pair them up in flow_scaling.qmd, but that then gets lost by the time they’re here. Can we go back over there and keep it?\n\n\nDo the shift\nThe multiple = all here is because otherwise it throws a warning that it duplicates rows x scenarios. That’s fine. But it might make more sense to do the scenarios as a list anyway, since they’ll get saved different places. Doesn’t matter right this instant.\n\ntest_hydro_q <- test_hydro_q %>% \n  left_join(test_q_s, by = c('Month', 'quantile'), multiple = 'all')\n\nDo the transform\nAll of this change ratio stuff is unnecessarily convoluted- just multiple F/H, don’t do this weird multiply then add thing. It’s exactly the same algebraically.\n\ntest_hydro_q <- test_hydro_q %>% \n  mutate(adj_val = (value*change_ratio + value))\n\nPlot check. too hard to see overplotted.\n\ntest_hydro_q %>% \n  filter(scenario %in% c('SimR1', 'SimR4', 'SimR7')) %>%\nggplot(mapping = aes(x = time, y = adj_val, color = scenario)) + \n  geom_point() + geom_line() + \n  scale_color_brewer(palette = 'Dark2') + \n  facet_wrap('scenario') + theme(legend.position = 'bottom')"
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_testing.html#format-cleanup",
    "href": "scenario_creation/development_testing/scaling_testing.html#format-cleanup",
    "title": "Scaling functions",
    "section": "Format cleanup",
    "text": "Format cleanup\nI want to then separate the scenarios and save a version that’s just time, site, and adj_val, but with ‘site’ as the name. This makes a nested tibble, which we could then loop over to save.\n\nthqs <- test_hydro_q %>% \n  # Just the needed cols\n  dplyr::select(scenario, site, time, adj_val) %>% \n  # pivot so the gauge name is col name\n  tidyr::pivot_wider(names_from = site, values_from = adj_val) %>% \n  # collapse to a list-tibble with one row per scenario\n  tidyr::nest(.by = scenario)\n\nThen I just need to write it out.\nQuestion do I want to do this differently so I can more easily save gauges within scenarios? or just do them one at a time? I think the way the data works (e.g. coming in as single gauges and scenarios all together), it gets convoluted either way, really. I think keep it how it is, and if memory holds it all I could paste them together, otherwise save a million csvs. That might be better anyway given the different date ranges."
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_zeros.html",
    "href": "scenario_creation/development_testing/scaling_zeros.html",
    "title": "Scaling functions",
    "section": "",
    "text": "I’m using flow_scaling.qmd to pull the gauges and scale them, but actually getting the scaling relationships is independent, and so doesn’t need to happen in the same script. I’ll do that here.\n\nlibrary(werptoolkitr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readr)\nlibrary(tidyr)\nlibrary(lubridate)\n\nLoading required package: timechange\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\nlibrary(vicwater)\nlibrary(patchwork)\nlibrary(fitdistrplus)\n\nWarning: package 'fitdistrplus' was built under R version 4.2.3\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nLoading required package: survival\n\n\nSet up some directories. Once we move to MDBA, these will be easier to point at in a shared way.\n\nscenario_dir <- '../flow_scaling_data'\nhydro_dir <- file.path(scenario_dir, 'hydrographs')\nscaling_dir <- file.path(scenario_dir, 'CC_Scenarios_WRPs')"
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_zeros.html#background-and-definitions",
    "href": "scenario_creation/development_testing/scaling_zeros.html#background-and-definitions",
    "title": "Scaling functions",
    "section": "Background and definitions",
    "text": "Background and definitions\nBring in David’s climate scenarios NOTE: Only go to 31-Jan-2019.\nPrec and ETp (PE) are historical (not needed here)\nSimR0 is simulated historical runnoff using actual historical Prec and ETp (PE)\nSimR1 - SimR7 are simulated with +7% PE but different Rainfall:\n\n-20%\n-15% (“High change scenario”)\n-10%\n-5% (“Moderate change scenario”)\n+0%\n+5% (“Low change scenario”)\n+10%\n\n\nMake numeric metadata\nThe format of this may change, but we’re going to want something. Lists are going to be easier to yaml than dataframes (though a dataframe is easier to construct).\nThough it sounds like the scenario yamls are likely to not be lists, but single values, ie each one gets their own value to create it, and that’s it (which makes sense).\n\nrain_multiplier <- seq(from = 0.8, to = 1.1, by = 0.05) %>% \n  setNames(paste0('R', 1:7))\n\nscenario_meta <- list(\n  PE_multiplier = 1.07,\n  rain_multiplier = rain_multiplier,\n  scenario_name = names(rain_multiplier)\n)\n\n# Don't run yet, since I don't know the format we'll be using, but this works to create yaml metadata\n# yaml::write_yaml(scenario_meta, file = 'path/to/file.yml')\n\nSuggestion is to compare each scenario to simulated historic baseline, work out the ratio and apply the difference to the gauge records… Would be good to get ‘cease to flow’ events for the scenarios."
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_zeros.html#bring-in-the-data",
    "href": "scenario_creation/development_testing/scaling_zeros.html#bring-in-the-data",
    "title": "Scaling functions",
    "section": "Bring in the data",
    "text": "Bring in the data\nGet the list of files and read them in (to a list of dfs)\n\nCCSc_FileList <- list.files(scaling_dir, pattern = '.csv', \n                            full.names = TRUE)\n\nscenario_list <- purrr::map(CCSc_FileList,\n                            \\(x) read_csv(file = x, id = 'path')) %>% \n  setNames(stringr::str_extract(CCSc_FileList, \"SS[0-9]+\"))"
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_zeros.html#scale",
    "href": "scenario_creation/development_testing/scaling_zeros.html#scale",
    "title": "Scaling functions",
    "section": "Scale",
    "text": "Scale\nDavid’s email\nHave given this some thought and think I’ve come up with a practical solution.\nI think the easiest way will be to\n\nCompute ranks/percentiles for both the observed and modelled future time series\nConstruct a new time series based on the observed ranks and replaces the observed values with the corresponding value for the same rank in the future time series.\n\nThis will give us a new future time series but if the distributions of the historical modelled and observed time series are different then the time series will not be a realistic representation of the future observations. So then my suggestion is to bias-correct the new future time series to make it look like a future observations time series.\nTo do the bias correction my thought is to set up a regression model between the observations and historical time series where we fudge it to deal with the zero-value problem.\nAssuming that the flow time series are roughly log-normally distributed, we can set up a regression between the values of corresponding ranks Log(obsr + delta) = a + b*log(modelr + delta)\nWhere:\nobsr is the observation for rank r\nmodelr is the modelled value of rank r\ndelta is a small non-zero value 0.1 or 0.01, probably the latter\na and b are the regression coefficients.\nWhen fitting the regression to the observations and historical (no change) streamflow we set all the values where obs = 0 to missing.  I don’t think that the model output will have zero values given they are area-weighted averages of gridded data\nWhen applying the regression to correct bias in the new future time series, then we are likely to generate negative values that can just be reset to zero.\nI think that the above will also allow for the probability of zero values to increase or decrease, as the modelled time series shouldn’t have zero values in it…"
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_zeros.html#prob-dists",
    "href": "scenario_creation/development_testing/scaling_zeros.html#prob-dists",
    "title": "Scaling functions",
    "section": "Prob dists",
    "text": "Prob dists\nBefore we even get to the replacing and unbiasing, what do those probability distributions look like? Are they remotely similar? Logging the data- loses the 0s but this is just a quick look\n\n# hyddens <- test_hydro %>% \n# ggplot(aes(x = log(hyd_vals))) +\n#   geom_density()\n# \n# scenedens <- test_model %>% \n#   ggplot(aes(x = log(SimR0))) +\n#   geom_density()\n# \n# hyddens + scenedens\n\nggplot() +\ngeom_density(data = test_model, \n             mapping = aes(x = log(SimR0)), color = 'dodgerblue') + \n  geom_density(data = test_hydro, \n               mapping = aes(x = log(hyd_vals)), color = 'forestgreen') +\n  xlab('log(value)')\n\nWarning: Removed 4384 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\nAnd the cdfs\n\nggplot() +\nstat_ecdf(data = test_model, \n             mapping = aes(x = log(SimR0)), color = 'dodgerblue') + \n  stat_ecdf(data = test_hydro, \n               mapping = aes(x = log(hyd_vals)), color = 'forestgreen') +\n  xlab('log(value)')\n\nWarning: Removed 4384 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nQuite a bit different. And slightly trimodal, though not nearly as pronounced as orig_hydros[[25]]."
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_zeros.html#rank-replacement",
    "href": "scenario_creation/development_testing/scaling_zeros.html#rank-replacement",
    "title": "Scaling functions",
    "section": "Rank-replacement",
    "text": "Rank-replacement\nThis replaces the values of rank x in the past hydro with values of rank x in modelled. It therefore gets the sequence of the real hydrograph, but the distribution of the modelled runoff.\nWe need to deal with duplicate ranks. The most common will be zeros in the hydrograph, but they could occur in the modelled data too.\n\nsum(duplicated(test_hydro$hydrank))\n\n[1] 14473\n\nsum(duplicated(test_model$histrank))\n\n[1] 178\n\n\nThe modelled data is not duplicated on 0, necessarily\n\ntest_model %>% group_by(histrank) %>% summarise(nr = n(), val = first(SimR0)) %>% filter(nr > 1)\n\n# A tibble: 177 × 3\n   histrank    nr     val\n      <dbl> <int>   <dbl>\n 1     380.     2 0.00394\n 2     394.     2 0.00395\n 3    1534.     2 0.00512\n 4    1546.     2 0.00514\n 5    1608.     2 0.00519\n 6    2130.     2 0.00567\n 7    2240.     2 0.00577\n 8    2362.     2 0.00588\n 9    3106.     2 0.00649\n10    3118.     2 0.00649\n# … with 167 more rows\n\n\nThe hydrographs are usually 0 duplication, but not always\n\ntest_hydro %>% group_by(hydrank) %>% summarise(nr = n(), val = first(hyd_vals)) %>% filter(nr > 1)\n\n# A tibble: 2,574 × 3\n   hydrank    nr   val\n     <dbl> <int> <dbl>\n 1   1410.  2820 0    \n 2   2824.     6 0.001\n 3   2830.     6 0.002\n 4   2834.     2 0.004\n 5   2838      5 0.011\n 6   2848.     2 0.33 \n 7   2852      3 0.799\n 8   2856.     2 1.18 \n 9   2862.     4 1.37 \n10   2868.     2 1.67 \n# … with 2,564 more rows\n\n\nIs this fundamentally the same issue as the data length issue below? We have x ranks in the hydrograph, and x + z ranks in the model. So all the data with rank d (duplicated) should get the same value, which is the same issue as if there were only one value of d and just fewer datapoints in the hydrograph than the data. One argument against the median for the ranks as I suggest is that when 0 is duplicated, we would end up pushing the median of several ranks to it instead of the minimum.\nAnd we need to deal with two situations of data length- hydrographs longer than model, and model longer than hydrograph (more typical). In the first, we need to duplicately assign model rank-values. In the second, we need to assign multiple ranks to a single rank. Maybe the median. Or we can use quantiles instead of ranks to break it up into the same size chunks.\nSimple code for rank-replacing. will need to modify\n\na <- tibble(vals = c(5,1,3), ranks = c(3,1,2))\nb <- tibble(vals = c(4,3,8), ranks = c(2,1,3))\nbina <- match(b$ranks, a$ranks)\nb$vala <- a$vals[bina]\nb\n\n# A tibble: 3 × 3\n   vals ranks  vala\n  <dbl> <dbl> <dbl>\n1     4     2     3\n2     3     1     1\n3     8     3     5\n\n\nAnd the duplication issues. I’m going to move away from tibbles, we can do this with $ and [] more generally to end up with a function. We might still apply that with a mutate to do the grouping though.\n\nh <- c(5,1,1,0,3,0,0,4,8,5,0,0,0,0, NA)\nm <- c(6,10,9,9,4,7,2,5,NA,2,3,6,9,5,2,3,10,9,4)\n\nrankh <- rank(h, na.last = 'keep')\nrankm <- rank(m, na.last = 'keep')\n\nThere’s not actually a clean way to get rankings that aren’t affected by duplication- ties.method first, last, min and max all end up giving each value a different rank and skipping numbers depending on how many dups there are at a given level, and average gives all duplicated values the same rank (good), but skips ranks based on how many duplicates there are. Which isn’t good- we will have different numbers of duplicates in the two datasets.\nSo, we need to cut to unique values, rank them, and use those rankings. Use na.last = 'keep' to not give NAs unique rankings. Though it would also work to drop NA when we get the unique values.\n\nhu <- unique(h)\nmu <- unique(m)\n\nrhu <- rank(hu, na.last = 'keep')\nrmu <- rank(mu, na.last = 'keep')\n\nNow, I was thinking I would map those ranks back to the original data, so we have, for example, 7 rank 1s in h (the zeros). But then we run up against the issue of different lengths. Rank 8 in hu is the highest value, while mu goes to 10.\nSo, do we actually want to go back to findInterval, with the number of intervals being min(length(h), length(m))? I think so (though those should probably be hu and mu. That quickly suggests we could just get back to q-q scaling too, and skip the straight replacements. Maybe- if the debiasing requires replaced values, that won’t work. Get there and see.\nA general quantile function I developed earlier. The trick will be to make q_perc right to give as many values as possible (rankings) instead of quantiles. e.g each value should be in its own quantile for the smallest data. Again, is this necessary? Or can we q-q scale (instead of value-replace) and then debias the zeros?\n\nget_q <- function(vals, q_perc) {\n  qs <- quantile(vals, probs = seq(0,1, q_perc), type = 5, na.rm = TRUE)\n  # cut fails with lots of zeros (well, any duplicate bins)\n  # binvec <- cut(vals, qs, include.lowest = TRUE, labels = FALSE)\n  # findInterval is OK with duplicate bins, but combines them, eg. if there are 10 bins that are all 0, it will call them all q10. \n  binvec <- findInterval(vals, qs, rightmost.closed = TRUE)\n  return(binvec)\n}\n\n\nfewest <- min(length(unique(h)), length(unique(m)))\n\nhi <- get_q(h, q_perc = 1/fewest)\nmi <- get_q(m, q_perc = 1/fewest)\n\nThat doesn’t actually work though- the quantiles goof it up because the duplicates necessarily shift the distribution. Instead, to get as close to a ranking as we can, we need to use the unique values for quantiles, but then findInterval on the full vectors. I think.\n\nqsh <- quantile(hu, probs = seq(0,1, 1/fewest), type = 5, na.rm = TRUE)\nqsm <- quantile(mu, probs = seq(0,1, 1/fewest), type = 5, na.rm = TRUE)\n\nhi <- findInterval(h, qsh, rightmost.closed = TRUE)\nmi <- findInterval(m, qsm, rightmost.closed = TRUE)\n\nThere are still some funny issues there (some quantiles can go missing), though that is primarily because of small test data.\nNow, let’s do the replacement. Use the median (or mean?) of the modelled data? In this dummy case, q4 has both 5 and 6 in it, so we should end up with somethign happening here.\n\ntable(mi, m)\n\n   m\nmi  2 3 4 5 6 7 9 10\n  1 3 0 0 0 0 0 0  0\n  2 0 2 0 0 0 0 0  0\n  3 0 0 2 0 0 0 0  0\n  4 0 0 0 2 2 0 0  0\n  5 0 0 0 0 0 1 0  0\n  6 0 0 0 0 0 0 4  0\n  7 0 0 0 0 0 0 0  2\n\n\nAggregate produces a df, so at this point I probably should switch to dplyr since it’s easier.\nWe need the summary of the ranks for the modelled data, but not for the hydrology- we need a single value to replace for each rank, but we can put it in many rows for the replacement.\n\n# mm <- aggregate(m, by = list(mi), FUN = median)\n\n# \nmdf <- tibble(mod_vals = m, ranks = mi) %>% \n  group_by(ranks) %>% \n  summarise(mod_vals = median(mod_vals))\n\nhdf <- tibble(hyd_vals = h, ranks = hi)\n\n# we could use `match` and indexing, but we're already in dplyr so\n\nhdf <- left_join(hdf, mdf, by = 'ranks')\n\nggplot(hdf, aes(x = hyd_vals, y = mod_vals)) + geom_point()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nMake all that a function so we can apply it to the actual data. The binning is very similar to before, it just uses unique values instead of all values to approximate a ranking."
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_zeros.html#re-build-data-cleanly",
    "href": "scenario_creation/development_testing/scaling_zeros.html#re-build-data-cleanly",
    "title": "Scaling functions",
    "section": "Re-build data cleanly",
    "text": "Re-build data cleanly\nwe had ranks in above, we want to do that differently now, so start from scratch\n\nqc_limit <- 150\n\ntest_model <- scenario_list$SS20\ntest_hydro <- orig_hydro[[20]]\n\ntest_hydro <- test_hydro %>% \n  mutate(hyd_vals = ifelse(quality_codes_id > qc_limit, NA, value))\n\ntest_model <- test_model %>%\n  mutate(date = lubridate::make_date(Year, Month, Day))\n\nThe simple binning function\n\nget_q <- function(vals, q_perc) {\n  qs <- quantile(unique(vals), probs = seq(0,1, q_perc), type = 5, na.rm = TRUE)\n  binvec <- findInterval(vals, qs, rightmost.closed = TRUE)\n  return(binvec)\n}\n\nData arrangement functions- actually, sort this out later, I think we can probably roll in the regression and apply across the sims, and so we want to sort it out here\n\n# Get the number of 'ranks' (bins)\nfewest <- min(length(unique(test_hydro$hyd_vals)), length(unique(test_model$SimR0)))\n\n# Rank the hydrograph\ntest_hydro <- test_hydro %>%\n  mutate(ranks = get_q(hyd_vals, 1/fewest))\n\n# Rank the model outputs\nmodel_rankvals <- test_model %>%\n  mutate(ranks = get_q(SimR0, 1/fewest)) %>% \n  group_by(ranks) %>% \n  # tempting to apply this `across(stars_with('Sim'))`, but the ranks will differ and so we need to be careful. Might be as easy as `across` in the ranks too?\n  summarise(mod_vals = median(SimR0))\n\n# replace (really, add another column)\ntest_hydro <- left_join(test_hydro, model_rankvals, by = 'ranks')"
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_zeros.html#check-plots",
    "href": "scenario_creation/development_testing/scaling_zeros.html#check-plots",
    "title": "Scaling functions",
    "section": "Check plots",
    "text": "Check plots\nDo the rankings seem to work?\n\nggplot(test_hydro, aes(x = hyd_vals, y = mod_vals)) + geom_line()\n\nWarning: Removed 1564 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFunny steps, but that’s likely to be expected.\nWhat does the timeseries look like?\n\nggplot(test_hydro, aes(x = time)) + geom_line(aes(y = hyd_vals), color = 'forestgreen') + geom_line(aes(y = mod_vals*500), color = 'dodgerblue')\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\nRemoved 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nThe variance is quite obviously different here, and that’s potentially an issue.\nWe already know the distributions look different from above. How do they look relative to rankings?Those are very different distributions, even when rescaled. Which is not surprising.\n\n# yes, I should pivot longer but i want to go quick.\nggplot(test_hydro, aes(x = ranks)) + geom_line(aes(y = hyd_vals), color = 'forestgreen') + geom_line(aes(y = mod_vals*500), color = 'dodgerblue')\n\nWarning: Removed 1564 rows containing missing values (`geom_line()`).\nRemoved 1564 rows containing missing values (`geom_line()`).\n\n\n\n\n\nSo, now the question is whether we can get the bias-correction regression to make the blue look like the green."
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_zeros.html#the-model-distributions",
    "href": "scenario_creation/development_testing/scaling_zeros.html#the-model-distributions",
    "title": "Scaling functions",
    "section": "The model distributions",
    "text": "The model distributions\nAbove we compared the ecdfs for the SimR0 and the flow, but let’s look at how consistent the distributions are for the other Sims. We don’t care about time, so just let it fall off in the pivot. The green line is the hydrograph values.\n\nsimpivot <- test_model %>% \n  pivot_longer(cols = starts_with('Sim'), \n               names_to = 'Sim', values_to = 'value')\n\n\nggplot(simpivot) +\n  stat_ecdf(mapping = aes(x = log(value), color = Sim)) + \n  stat_ecdf(data = test_hydro, \n               mapping = aes(x = log(hyd_vals)), color = 'forestgreen')\n\nWarning: Removed 4384 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nCan we fit those distributions?\nTry hydrograph first\n\ncleanhydro <- test_hydro$hyd_vals[!is.na(test_hydro$hyd_vals) & \n                                    test_hydro$hyd_vals > 0]\nfit_hydro <- MASS::fitdistr(cleanhydro, densfun = 'lognormal')\n\nxvals <- min(test_hydro$hyd_vals, na.rm = T):round(max(test_hydro$hyd_vals, na.rm = T))\n\npfit <- plnorm(xvals, fit_hydro$estimate[1], fit_hydro$estimate[2])\ndfit <- dlnorm(xvals, fit_hydro$estimate[1], fit_hydro$estimate[2])\nlogfit <- tibble(xval = xvals, fitcdf = pfit, fitpdf = dfit)\n\nNot perfect, but that’s not going to be possible.\n\nggplot(test_hydro) +\n  stat_ecdf(mapping = aes(x = log(hyd_vals)), color = 'forestgreen') +\n  geom_line(data = logfit, mapping = aes(x = log(xval), y = fitcdf), \n            color = 'firebrick') + \n  stat_ecdf(data = test_log, mapping = aes(x = pred_log_hyd), color = 'dodgerblue')\n\nWarning: Removed 4384 rows containing non-finite values (`stat_ecdf()`).\n\n\nWarning: Removed 1564 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nPDF. It doesn’t look super great, really.\n\nggplot(test_hydro) +\n  geom_density(mapping = aes(x = hyd_vals), color = 'forestgreen') +\n  geom_line(data = logfit, mapping = aes(x = xval, y = fitpdf), \n            color = 'firebrick') + \n  coord_cartesian(xlim = c(-1, 1000))\n\nWarning: Removed 1564 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\nWe can’t just log(x) for the lognorm pdf because of the nonlinear transform. But we can look at it by directly building a normal pdf on the lognorm scale.\n\ncleanhydrolog <- log(cleanhydro)\n\nfit_hydrolog <- MASS::fitdistr(cleanhydrolog, densfun = 'normal')\n\nxvals2 <- seq(min(cleanhydrolog, na.rm = T), \n              round(max(cleanhydrolog, na.rm = T)), \n              by = 0.1)\n\npfitlog <- pnorm(xvals2, fit_hydrolog$estimate[1], fit_hydrolog$estimate[2])\ndfitlog <- dnorm(xvals2, fit_hydrolog$estimate[1], fit_hydrolog$estimate[2])\n\nlogfitlog <- tibble(xval = xvals2, fitcdf = pfitlog, fitpdf = dfitlog)\n\n\nggplot() +\n  geom_density(data = tibble(hydro = cleanhydrolog), aes(x = hydro), color = 'forestgreen') +\n  geom_line(data = logfitlog, aes(x = xvals2, y = fitpdf), color = 'firebrick')\n\n\n\n\nThat’s much better. So why isn’t it working for the unlogged with lognormal. Is it because truncated?\nThe rnorm with those dims works/matches. SO what’s up with the lognormal? Truncated doesn’t really make sense. Will need to look into it a bit more.\n\ntestcurve <- rnorm(length(cleanhydrolog), \n                   fit_hydrolog$estimate[1], fit_hydrolog$estimate[2])\n\nggplot() +\n  geom_density(data = tibble(hydro = cleanhydrolog), aes(x = hydro), color = 'forestgreen') +\n    geom_density(data = tibble(testdist = testcurve), aes(x = testdist), color = 'purple') +\n  geom_line(data = logfitlog, aes(x = xvals2, y = fitpdf), color = 'firebrick')\n\n\n\n\nNow, how abot sim0\n\ncleansim <- test_model$SimR0[!is.na(test_model$SimR0) & \n                                    test_model$SimR0 > 0]\nfit_sim <- MASS::fitdistr(cleansim, densfun = 'lognormal')\n\nxvalsim <- exp(seq(-6.5, 3, by = 0.01))\npfitsim <- plnorm(xvalsim, fit_sim$estimate[1], fit_sim$estimate[2])\n\nlogfitsim <- tibble(xval = xvalsim, fitcdf = pfitsim)\n\nInteresting. Thats a worse fit to the lognormal than the hydrographs were.\n\nggplot(test_model) +\n  stat_ecdf(mapping = aes(x = log(SimR0)), color = 'dodgerblue') +\n  geom_line(data = logfitsim, mapping = aes(x = log(xval), y = fitcdf), color = 'firebrick')\n\n\n\n\n\nFitting censored data\nLet’s assume the hydrograph data is left-censored (essentially a detection limit, below which data is zero). That should yield better distributional fits than above where we just threw the zeros out and fit what was left. We can get the estimates with fitdistrplus::fitdistcens. I’ll do this with test_hydro directly (and compare with other fits to check it makes sense).\nFirst, we need a new dataframe to define the censoring. We use hyd_vals because that has dropped the values that are just bad. What do we want to say the ‘detection limit’ is? I guess the smallest nonzero value\n\ndetectionlimit <- 1 #min(test_hydro$hyd_vals[test_hydro$hyd_vals > 0], na.rm = TRUE)\nsum(test_hydro$hyd_vals < detectionlimit, na.rm = TRUE)\n\n[1] 2854\n\n\nCreate the needed dataframe\n\ncensframe <- test_hydro |> \n  dplyr::filter(!is.na(hyd_vals)) |> # Throw out the NAs, we want a distribution\n  dplyr::mutate(left = ifelse(hyd_vals == 0, NA, hyd_vals),\n                right = ifelse(hyd_vals == 0, detectionlimit, hyd_vals)) |> \n  dplyr::select(left, right) |> \n  data.frame() # annoyingly tibbles break fitdistcens\n\nFit and get the parameters. Get the naive version too, to see how different it is. This is the same as above, but I’m trying to keep this self-contained.\n\nfit_cens <- fitdistrplus::fitdistcens(censdata = censframe, distr = 'lnorm')\n\nfit_censW <- fitdistrplus::fitdistcens(censdata = censframe, distr = 'weibull')\n\n# fitdistr just needs the vector\nfit_naive <- test_hydro |> \n  dplyr::filter(!is.na(hyd_vals) & hyd_vals > 0) |> \n  dplyr::select(hyd_vals) |> \n  pull() |> \n  MASS::fitdistr(densfun = 'lognormal')\n\nfit_cens\n\nFitting of the distribution ' lnorm ' on censored data by maximum likelihood \nParameters:\n        estimate\nmeanlog 5.205442\nsdlog   2.041805\n\nfit_naive\n\n     meanlog        sdlog   \n  5.664658806   1.251399081 \n (0.006598844) (0.004666087)\n\n\nThose are really very different. Let’s make some plots to compare the distributions. Get the pdf and cdf we’d get for both those distributions.\n\ndf_dists <- tibble(x = seq(0,10000, by = 0.1), \n                 cdf_cens = plnorm(x, \n                             fit_cens$estimate['meanlog'],\n                             fit_cens$estimate['sdlog']),\n                 cdf_naive = plnorm(x, \n                             fit_naive$estimate['meanlog'],\n                             fit_naive$estimate['sdlog']),\n                 cdf_weib = pweibull(x, \n                             fit_censW$estimate['shape'],\n                             fit_censW$estimate['scale']),\n                 pdf_cens = dlnorm(x, \n                             fit_cens$estimate['meanlog'],\n                             fit_cens$estimate['sdlog']),\n                 pdf_naive = dlnorm(x, \n                             fit_naive$estimate['meanlog'],\n                             fit_naive$estimate['sdlog']),\n                 # And random data\n                 rand_cens = rlnorm(length(x), \n                             fit_cens$estimate['meanlog'],\n                             fit_cens$estimate['sdlog']),\n                 rand_naive = rlnorm(length(x), \n                             fit_naive$estimate['meanlog'],\n                             fit_naive$estimate['sdlog']))\n\nPlot CDFs. The censored fit is terrible.\n\nggplot() + \n  stat_ecdf(data = test_hydro, mapping = aes(x = hyd_vals),\n            color = 'black') + \n  # stat_ecdf(data = df_dists, mapping = aes(x = rand_cens), \n  #           color = 'darkseagreen') +\n  geom_line(data = df_dists, mapping = aes(x = x, y = cdf_cens),\n            color = 'darkgreen') +\n  # stat_ecdf(data = df_dists, mapping = aes(x = rand_naive), \n  #           color = 'cyan') +\n  geom_line(data = df_dists, mapping = aes(x = x, y = cdf_naive),\n            color = 'dodgerblue') +\n    geom_line(data = df_dists, mapping = aes(x = x, y = cdf_weib),\n            color = 'firebrick') +\n  geom_vline(xintercept = detectionlimit) +\n  coord_cartesian(xlim = c(-1, 1000))\n\nWarning: Removed 1564 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nWait, this is always going to be crap if fit with plnorm (or weibull). Because those distributions are actually limited at 0, but the data is levelling off and wants to go below zero. So, we do need to go normal manually first. Or shift up, or something.\n\n\nShifting up\nDoes it work to just shift up?\nFirst, we need a new dataframe to define the censoring. We use hyd_vals because that has dropped the values that are just bad. What do we want to say the ‘detection limit’ is? I guess the smallest nonzero value\n\ndetectionlimit <- 5000\n\ntest_hydro$hyd_vals_shift <- test_hydro$hyd_vals + detectionlimit\n\nCreate the needed dataframe\n\ncensframe <- test_hydro |> \n  dplyr::filter(!is.na(hyd_vals)) |> # Throw out the NAs, we want a distribution\n  dplyr::mutate(left = ifelse(hyd_vals_shift == detectionlimit, NA, hyd_vals_shift),\n                right = ifelse(hyd_vals_shift == detectionlimit, detectionlimit, hyd_vals_shift)) |> \n  dplyr::select(left, right) |> \n  data.frame() # annoyingly tibbles break fitdistcens\n\nFit and get the parameters. Get the naive version too, to see how different it is. This is the same as above, but I’m trying to keep this self-contained.\n\nfit_cens <- fitdistrplus::fitdistcens(censdata = censframe, distr = 'lnorm')\n\nfit_censW <- fitdistrplus::fitdistcens(censdata = censframe, distr = 'weibull')\n\n# fitdistr just needs the vector\nfit_naive <- test_hydro |> \n  dplyr::filter(!is.na(hyd_vals_shift) & hyd_vals_shift > 0) |> \n  dplyr::select(hyd_vals_shift) |> \n  pull() |> \n  MASS::fitdistr(densfun = 'lognormal')\n\nfit_cens\n\nFitting of the distribution ' lnorm ' on censored data by maximum likelihood \nParameters:\n         estimate\nmeanlog 8.6085499\nsdlog   0.1512424\n\nfit_censW\n\nFitting of the distribution ' weibull ' on censored data by maximum likelihood \nParameters:\n         estimate\nshape    4.138743\nscale 5956.796003\n\nfit_naive\n\n     meanlog         sdlog    \n  8.6152940284   0.1433097510 \n (0.0007277045) (0.0005145648)\n\n\nThose are really very different. Let’s make some plots to compare the distributions. Get the pdf and cdf we’d get for both those distributions.\n\ndf_dists <- tibble(x = seq(0,10000, by = 0.1), \n                 cdf_cens = plnorm(x, \n                             fit_cens$estimate['meanlog'],\n                             fit_cens$estimate['sdlog']),\n                 cdf_naive = plnorm(x, \n                             fit_naive$estimate['meanlog'],\n                             fit_naive$estimate['sdlog']),\n                 cdf_weib = pweibull(x, \n                             fit_censW$estimate['shape'],\n                             fit_censW$estimate['scale']),\n                 pdf_cens = dlnorm(x, \n                             fit_cens$estimate['meanlog'],\n                             fit_cens$estimate['sdlog']),\n                 pdf_naive = dlnorm(x, \n                             fit_naive$estimate['meanlog'],\n                             fit_naive$estimate['sdlog']),\n                 # And random data\n                 rand_cens = rlnorm(length(x), \n                             fit_cens$estimate['meanlog'],\n                             fit_cens$estimate['sdlog']),\n                 rand_naive = rlnorm(length(x), \n                             fit_naive$estimate['meanlog'],\n                             fit_naive$estimate['sdlog']))\n\nPlot CDFs. The censored fit is terrible.\n\nggplot() + \n  stat_ecdf(data = test_hydro, mapping = aes(x = hyd_vals_shift),\n            color = 'black') + \n  # stat_ecdf(data = df_dists, mapping = aes(x = rand_cens), \n  #           color = 'darkseagreen') +\n  geom_line(data = df_dists, mapping = aes(x = x, y = cdf_cens),\n            color = 'darkgreen') +\n  # stat_ecdf(data = df_dists, mapping = aes(x = rand_naive), \n  #           color = 'cyan') +\n  geom_line(data = df_dists, mapping = aes(x = x, y = cdf_naive),\n            color = 'dodgerblue') +\n    geom_line(data = df_dists, mapping = aes(x = x, y = cdf_weib),\n            color = 'firebrick') +\n  geom_vline(xintercept = detectionlimit) +\n  coord_cartesian(xlim = c(-1, 10000))\n\nWarning: Removed 1564 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nThose are actually a lot better. Does this actually work? Or should we do it on the logged scale and normal dists? How much to shift?\n\n\nOptimized upshifts\nI’ve done a bunch of testing elsewhere with known distributions, and come up with a set of functions to determine the shift (and spit out a dataframe for diagnostic plots).\n\nfitshift <- function(cleandata, shift_up) {\n  # Handle the zero case- we just use the next value up\n    if (shift_up == 0) {rightlim <- min(cleandata[cleandata>0])\n    } else {\n      rightlim <- shift_up}\n  \n  inshift <- cleandata + shift_up\n    \n    upcens <- tibble(left = ifelse(inshift <= shift_up, NA, inshift),\n                right = ifelse(inshift <= shift_up, rightlim, inshift))\n    \n    suppressWarnings(fit_up <- fitdistcens(censdata = data.frame(upcens),\n                                           distr = 'lnorm'))\n    \n    return(fit_up)\n}\n\n\nopt_up <- function(shift_up, cleandata) {\n  \n  fit_up <- fitshift(cleandata, shift_up)\n  \n  return(-fit_up$loglik)\n}\n\n\noptshift <- function(rawdata) {\n  \n  # This is about distributions, NOT data order, so get rid of NAs\n  rawna <- na.omit(rawdata)\n  \n  # get the optimal shift\n  shift <- optimize(opt_up, interval = c(0, 1000), cleandata = rawna)\n  \n  # Get the fit at that shift (would be nice to kick this out of opt_up somehow)\n  \n  fit_up <- fitshift(rawna, shift$minimum)\n  \n # Create a df for output\n  # The shifted data\n  shiftdf <- tibble(orig_data = rawdata, \n                    shift_data = rawdata + shift$minimum, \n                    optimum_shift = shift$minimum)\n  \n\n   # This isn't ideal, but we can shove the cdf on here too, it just has rows that don't mean the same thing. prevents us saving a list though.\n  shiftdf <- shiftdf |> \n    mutate(x = row_number()/10,\n           meanlog = fit_up$estimate['meanlog'],\n           sdlog = fit_up$estimate['sdlog'],\n           cdf_up = plnorm(x, \n                             fit_up$estimate['meanlog'],\n                             fit_up$estimate['sdlog']),\n           pdf_up = dlnorm(x, \n                             fit_up$estimate['meanlog'],\n                             fit_up$estimate['sdlog']),\n           # Some diagnostics\n           fitloglik = fit_up$loglik)\n  \n  # and a shifted-back version of the cdf/pdf just needs a shifted x. The\n  # backshift of the data is just the original `rawdata`.\n  shiftdf <- shiftdf |> \n    mutate(x_back = x-shift$minimum)\n\n}\n\nNow, let’s try running that for the data\n\noptimal_fit <- optshift(test_hydro$hyd_vals)\n\nPlot that- how are we doing? It’s OK, likely is the best fit, the distribution just isn’t lognormal.\n\nggplot(optimal_fit) +\n  stat_ecdf(aes(x = shift_data)) +\n  geom_line(aes(x = x, y = cdf_up), linetype = 2) +\n  coord_cartesian(xlim = c(0,2000))\n\nWarning: Removed 1564 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nHow does that look compared to the non-shifted fit? I’ll shift this fit back to where it should be. They’re better in different places. But the shifted version is the only one that can possibly handle the truncation. And we know from the optimisation that it has a better log-likelihood, since 0 was an option.\n\nggplot(optimal_fit) +\n  stat_ecdf(aes(x = orig_data), \n            color = 'firebrick') +\n  geom_line(aes(x = x_back, y = cdf_up), \n            color = 'firebrick', linetype = 2) +\n  geom_line(data = logfit, mapping = aes(x = xval, y = fitcdf), \n            color = 'dodgerblue') +\n  coord_cartesian(xlim = c(-70,3000))\n\nWarning: Removed 1564 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\n\nWeibull?\nDoes a weibull fit better, now that we have the shift? No\n\n# fitshift_w <- function(rawdata, shift_up) {\n#   # Handle the zero case- we just use the next value up\n#     if (shift_up == 0) {rightlim <- min(rawdata[rawdata>0])\n#     } else {\n#       rightlim <- shift_up}\n#   \n#   inshift <- rawdata + shift_up\n#     \n#     upcens <- tibble(left = ifelse(inshift <= shift_up, NA, inshift),\n#                 right = ifelse(inshift <= shift_up, rightlim, inshift))\n#     \n#     suppressWarnings(fit_up <- fitdistcens(censdata = data.frame(upcens),\n#                                            distr = 'weibull'))\n#     \n#     return(fit_up)\n# }\n# \n# opt_up_w <- function(shift_up, rawdata) {\n#   \n#   fit_up <- fitshift_w(rawdata, shift_up)\n#   \n#   return(-fit_up$loglik)\n# }\n# \n# \n# optshift_w <- function(rawdata) {\n#   \n#   # This is about distributions, NOT data order, so get rid of NAs\n#   rawdata <- na.omit(rawdata)\n#   \n#   # get the optimal shift\n#   shift <- optimize(opt_up_w, interval = c(0, 1000), rawdata = rawdata)\n#   \n#   # Get the fit at that shift (would be nice to kick this out of opt_up somehow)\n#   \n#   fit_up <- fitshift_w(rawdata, shift$minimum)\n#   \n#  # Create a df for output\n#   # The shifted data\n#   shiftdf <- tibble(orig_data = rawdata, \n#                     shift_data = rawdata + shift$minimum, \n#                     optimum_shift = shift$minimum)\n#   \n# \n#    # This isn't ideal, but we can shove the cdf on here too, it just has rows that don't mean the same thing. prevents us saving a list though.\n#   shiftdf <- shiftdf |> \n#     mutate(x = row_number()/10,\n#            cdf_up = pweibull(x, \n#                              fit_up$estimate['shape'],\n#                              fit_up$estimate['scale']),\n#            pdf_up = dweibull(x, \n#                              fit_up$estimate['shape'],\n#                              fit_up$estimate['scale']),\n#            # Some diagnostics\n#            fitloglik = fit_up$loglik)\n#   \n#   # and a shifted-back version of the cdf/pdf just needs a shifted x. The\n#   # backshift of the data is just the original `rawdata`.\n#   shiftdf <- shiftdf |> \n#     mutate(x_back = x-shift$minimum)\n# \n# }\n\n\n# optimal_fit_w <- optshift_w(test_hydro$hyd_vals)\n\n\n# ggplot(optimal_fit_w) +\n#   stat_ecdf(aes(x = shift_data)) +\n#   geom_line(aes(x = x, y = cdf_up), linetype = 2) +\n#   coord_cartesian(xlim = c(0,2000))"
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_zeros.html#shifting-between-distributions",
    "href": "scenario_creation/development_testing/scaling_zeros.html#shifting-between-distributions",
    "title": "Scaling functions",
    "section": "Shifting between distributions",
    "text": "Shifting between distributions\nCouple questions-\n\nif we have data with one distribution, how do we give it another known distribution. I seem to remember somethign about f(g(x)) and g(x)^-1, but conceptually, what are we doing?\nEven if we can do that, is there a way to shift them to yield numbers below zero? or above, for that matter? Or are we just re-developing a more complex q-q?\n\nWe have some x-value (say, flow). We want to shift it to a new distribution. I think conceptually, we go up to its value in the plots above, then over to the corresponding p(x) on the other distribution, then down to get a new value.\nSo, for the hydrographs, start with cleanhydro, the values of the hydrographs\n\n# # Get the probs for each hydrograph value- this is the ecdf, which creates a *function* that takes x as arguments and returns probabilities\n# phydro <- ecdf(log(cleanhydro))\n# phydrop <- phydro(log(cleanhydro))\n# \n# # Get the hydrograph value for the matching probs for the fit lognormal\n# lnh <- qlnorm(phydrop, fit_hydro$estimate[1], fit_hydro$estimate[2])\n# \n# shifthyd <- tibble(hyd_vals = cleanhydro, fit_val = lnh)\n\n# Get the probs for each hydrograph value- this is the ecdf, which creates a *function* that takes x as arguments and returns probabilities\n\n# This is messy to use the dataframe from above, but I'm tired of re-writing things.\n# cleanraw <- test_hydro$hyd_vals %>% na.omit()\nphydro <- ecdf(optimal_fit$shift_data)\n\noptimal_fit <- optimal_fit %>% \n  mutate(p_shift = phydro(shift_data),\n         q_shiftln = qlnorm(p_shift, \n                            meanlog,\n                            sdlog),\n         q_back = q_shiftln-optimum_shift)\n  \n# lnh <- qlnorm(phydrop, fit_up$estimate['meanlog'], fit_hydro$estimate['sdlog'])\n\n# # Deal with the shift\n# lnh <- lnh - shift_by$minimum\n# \n# shifthyd <- tibble(hyd_vals = cleanraw, fit_val = lnh)\n# \n# xfit <- seq(0, 1000, by = 0.1)\n# shiftfit <- plnorm(xfit, \n#                              fit_up$estimate['meanlog'],\n#                              fit_up$estimate['sdlog'])\n# \n# shiftdf <- tibble(x = xfit, cdf = shiftfit, x_back = xfit - shift_by$minimum)\n\nThat looks like a pretty good translation.\n\nggplot(optimal_fit) +\n  # ecdf of the hydrograph data\n  stat_ecdf(mapping = aes(x = orig_data), color = 'forestgreen') +\n  # lognormal cdf\n  geom_line(aes(x = x_back, y = cdf_up), \n            color = 'firebrick', linetype = 2) +\n  # ecdf of the transformed data, should match the lognormal cdf\n  stat_ecdf(aes(x = q_back),\n            color = 'dodgerblue', linetype = 2) +\n  coord_cartesian(xlim = c(-10, 1000))\n\nWarning: Removed 1564 rows containing non-finite values (`stat_ecdf()`).\n\n\nWarning: Removed 1565 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nSo that’s almost dead on. What does it look like as a timeseries next to the real data? This is dangerous to just glue time on, but the order should be preserved.\n\noptimal_fit$time <- test_hydro$time\n\n\nnewhydro <- ggplot(optimal_fit, aes(x = time)) + \n  geom_line(mapping = aes(y = orig_data), color = 'forestgreen') +\n  geom_line(mapping = aes(y = q_back), color = 'dodgerblue', linetype = 2)\nnewhydro\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\nRemoved 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nYikes. some of those are a bit extreme, huh? Lognormal tails are dangerous.\n\nnewhydro + coord_cartesian(ylim = c(0, 5000))\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\nRemoved 1 row containing missing values (`geom_line()`).\n\n\n\n\n\n\nhyddata <- ggplot(optimal_fit, aes(x = time)) + \n  geom_line(mapping = aes(y = orig_data), color = 'forestgreen')\n\nfitdata <- ggplot(optimal_fit, aes(x = time)) +\n  geom_line(mapping = aes(y = q_back), color = 'dodgerblue')\n\nhyddata + fitdata\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\nRemoved 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nYikes. why are we just blowing the top off?\n\nggplot(optimal_fit, aes(x = orig_data, y = q_back)) + \n  geom_point() + \n  geom_abline(slope = 1, intercept = 0)\n\nWarning: Removed 1564 rows containing missing values (`geom_point()`).\n\n\n\n\n\nSo, that works OK for low values, but blows up high. Basically because the hydrograph distribution is NOT a lognormal. Is this better or worse than the linear regression method? Not sure. The empirical distribution doesn’t fit a lognormal, and the relationship between runoff models and hydrograph is nonlinear. So in both cases, a direct hydrograph –> transform –> backtransform (even with no shifts in the distribution to reflect the scenarios) yields changes to the distribution. Which is worse/better? Not sure, they cause different sorts of errors. Might come down to which one is able to do the scenario shifts the best.\nWe could also do something like q-q away from 0, and one of these near it- e.g. fit the linear regression for the lower 10% of the data or only do the probs–>lognormal dist shift for that part of the data.\nAnd we’re still left with the issue of what to do with this if it works- can we ever shift this in a way to give more or less zeros?"
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_zeros.html#fit-dists-but-different-function",
    "href": "scenario_creation/development_testing/scaling_zeros.html#fit-dists-but-different-function",
    "title": "Scaling functions",
    "section": "Fit dists, but different function",
    "text": "Fit dists, but different function\nIs there any advantage to trying to manually log the data and then fit? Not sure. Since we’re fitting to minimize loglikelihood, the fits should be the same. But it might let us do a better job of the shifts, because we could then treat it as a truncated distribution, rather than a shift and backshift. I think I should try this- once I get everything set up for the lognormal, it should be OK.\nCould try other distributions, but Weibull sure was terrible. Not sure what else that’s parametric."
  },
  {
    "objectID": "scenario_creation/development_testing/scaling_zeros.html#linearize-part-of-the-model-data-relationship-not-all-of-it",
    "href": "scenario_creation/development_testing/scaling_zeros.html#linearize-part-of-the-model-data-relationship-not-all-of-it",
    "title": "Scaling functions",
    "section": "Linearize part of the model-data relationship, not all of it",
    "text": "Linearize part of the model-data relationship, not all of it\nCould I do the regression method for the bottom 10%, and then the other 90% use q-q (probably, since it’s the least parametric) or the distribution fits?"
  },
  {
    "objectID": "scenario_creation/flow_scaling.html#process",
    "href": "scenario_creation/flow_scaling.html#process",
    "title": "Flow scaling demonstration",
    "section": "Process",
    "text": "Process\nTo create the scenario hydrographs, we need to\n\nIdentify gauges in the EWR tool, since that’s the module that currently exists\nPull their historical data\nMap them to SDL units, because the scaling simulations are done at that scale\n\nThis document stops at this point, along with some visualisations\n\nScale them. This is done in scaling_scenarios.qmd.\n\nAt that point, the scenario hydrographs will be created, and we can then run them through the toolkit."
  },
  {
    "objectID": "scenario_creation/flow_scaling.html#toolkit-relevance",
    "href": "scenario_creation/flow_scaling.html#toolkit-relevance",
    "title": "Flow scaling demonstration",
    "section": "Toolkit relevance",
    "text": "Toolkit relevance\nThe creation of flow scenarios is not part of the toolkit proper. Instead, the toolkit expects to ingest hydrographs and then handles the ongoing response models, aggregation, and analyses. Thus, hydrographs are an essential input to the toolkit. The point of this code is to generate those hydrographs."
  },
  {
    "objectID": "scenario_creation/flow_scaling.html#set-up-paths",
    "href": "scenario_creation/flow_scaling.html#set-up-paths",
    "title": "Flow scaling demonstration",
    "section": "Set up paths",
    "text": "Set up paths\nFor the smaller demo the data inside the repo so users can see what’s being produced. For this, we’ll follow a more typical use-case where both the input and output data are external, and so we need their path. This will likely end up on MDBA blob, but for now, I’ll just send it up a level locally. We’ll create an internal directory for the hydrographs, since the other output goes in the scenario_dir as well.\n\nscenario_dir <- '../flow_scaling_data'\nhydro_dir <- file.path(scenario_dir, 'hydrographs')\nscaling_dir <- file.path(scenario_dir, 'CC_Scenarios_WRPs')\n\n\nif (!dir.exists(hydro_dir)) {dir.create(hydro_dir, recursive = TRUE)}"
  },
  {
    "objectID": "scenario_creation/flow_scaling.html#gauges-in-ewr",
    "href": "scenario_creation/flow_scaling.html#gauges-in-ewr",
    "title": "Flow scaling demonstration",
    "section": "Gauges in EWR",
    "text": "Gauges in EWR\nWhich gauges are actually in the EWR tool? The EWR tool has a function, so use that. Use python to access the EWR table. Access the python objects with py$objname.\n\nTODO THIS FAILS AS OF 1.0.4. I have rolled back to ewr version 1.0.1, since the necessary file just doesn’t exist in 1.0.4 (and in about half the branches on github). This needs to be updated and tested.\nError messages:\nFileNotFoundError: [Errno 2] No such file or directory: 'py_ewr/parameter_metadata/NSWEWR.csv'\n\nError in py_get_attr_impl(x, name, silent) : \n  AttributeError: module '__main__' has no attribute 'ewrs'\n\n\nfrom py_ewr.data_inputs import get_EWR_table\nfrom py_ewr.observed_handling import categorise_gauges\newrs, badewrs = get_EWR_table()\ndistinctgauges = ewrs['Gauge'].unique()\n# Separate into flow gauges, level gauges, and stage gauges\ncatgauges = categorise_gauges(distinctgauges)\n\nGet those gauge numbers into an R object, and ask how many. py$ewrsis the full EWR table. So we have made them unique, and then used py_ewe.observed_handling.categorise_gauges to separate them into flow, level, and stage gauges (as documented in that function).\nPull the full list into R to use for things like mapping, and then categorized list too.\n\newrgauges = unique(py$ewrs$Gauge)\nlength(ewrgauges)\n\n[1] 149\n\ngauge_cats <- py$catgauges %>% \n  setNames(c('flow', 'level', 'stage'))\n\nI could leave that as a list, but a dataframe ends up helping later on with other arguments (e.g. state and variable) and is easier to map.\n\ncat_gauges <- gauge_cats %>% \n  stack() %>% \n  rename(gauge = values, type = ind) %>% \n  mutate(var_to = case_when(\n    type == 'flow' ~ 141,\n    type == 'level' ~ 130,\n    type == 'stage' ~ 100\n  ))\n\n149 gauges doesn’t seem too bad."
  },
  {
    "objectID": "scenario_creation/flow_scaling.html#map-gauges",
    "href": "scenario_creation/flow_scaling.html#map-gauges",
    "title": "Flow scaling demonstration",
    "section": "Map gauges",
    "text": "Map gauges\nThe gauges with their locations as an sf object are in werptoolkitr::bom_basin_gauges.\nJoin to the categorized table. right_join maintains the sf object but only has rows for cat_gauges.\n\ngeo_gauges <- right_join(bom_basin_gauges, cat_gauges)\n\nJoining with `by = join_by(gauge)`\n\nnrow(geo_gauges  %>% \n  filter(!st_is_empty(geometry)))\n\n[1] 141\n\n\nThe st_is_empty filter is because some of the gauges are things like ‘Bills Pipe’ and ‘Pump direct from river’, and so don’t actually have locations. They aren’t pullable from NSW, but we can leave them in for now.\nNote that this contains level gauges (and stage). I include the stage gauges in the flow scaling, but not the level. All three types are likely to respond to runoff, with flow being the most directly related, and stage more nonlinear. Level changes will be much more difficult to translate, and so we drop them, at least for the moment.\n\n# The commented out code labels the SDL units, but it's too busy.\nggplot() +\n  geom_sf(data = sdl_units, \n          mapping = aes(fill = SWSDLName), \n          show.legend = FALSE) +\n  # geom_sf_label(data = sdl_units, \n  #               mapping = aes(label = SWSDLName), \n  #               size = 3, \n  #               label.padding = unit(0.1, 'lines')) +\n  geom_sf(data = geo_gauges, mapping = aes(color= type)) +\n  colorspace::scale_fill_discrete_qualitative(palette = 'Harmonic') +\n  colorspace::scale_color_discrete_diverging(palette = 'Lisbon')"
  },
  {
    "objectID": "scenario_creation/flow_scaling.html#mapping-gauges-to-sdl-unit",
    "href": "scenario_creation/flow_scaling.html#mapping-gauges-to-sdl-unit",
    "title": "Flow scaling demonstration",
    "section": "Mapping gauges to SDL unit",
    "text": "Mapping gauges to SDL unit\nTo do the flow-scaling, we’ll need to know the SDL unit. While we’re here working with both, let’s add that on to geo_gauges (though we end up doing it a bit differently in the scaling notebook.\n\ngeo_gauges <- st_intersection(geo_gauges, sdl_units)\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries"
  },
  {
    "objectID": "scenario_creation/flow_scaling.html#set-up-for-the-pull",
    "href": "scenario_creation/flow_scaling.html#set-up-for-the-pull",
    "title": "Flow scaling demonstration",
    "section": "Set up for the pull",
    "text": "Set up for the pull\nI’m being explicit about daily means and var_to arguments to match those used internally by mdba_gauge_getter in the EWR tool, since that’s what the EWR expects. I’m using returnformat = sitelist to return a list of gauges instead of a long dataframe because that makes it easier to find (and bypass) site failures.\nIf we didn’t need a state argument, we could use the gauge_cats list as site_list, with a length-3 vector of the var_to values, since that list is already set up. But we also need the state argument, and that’s on a gauge-by-gauge basis. So instead I call get_ts_traces2 for each combination of gauge type and state, since there are gauges in Vic, Qld, and NSW.\n\ntable(geo_gauges$owner, geo_gauges$type)\n\n                                                           \n                                                            flow level stage\n  NSW - NSW Department of Industry – Lands and Water         133     4     0\n  QLD - Department of Natural Resources, Mines and Energy      1     0     0\n  VIC - Department of Environment, Land, Water and Planning    2     0     1\n\n\nAfter lots of testing, I’ve decided to use datasource = A because CP gives unstable 504 Gateway timeouts. It’s tempting to just furrr::future_pmap over the rows of geo_gauges, but that takes about 20x as long, likely a combination of data-passing with furrr and additional API calls to get periods of record. We shouldn’t need to do this much, but still, it’s likely we’ll need to more than once.\nWe create a dataframe with the arguments to use with purrr::pmap. The gauge column is a list of tibbles, and for each one we need to unlist it to make it a character vector- get_ts_traces doesn’t accept tibbles of gauge numbers.\n\ngauges_to_pull <- geo_gauges %>% \n  st_drop_geometry() %>% \n  select(gauge, owner, var_to) %>% \n  nest_by(owner, var_to, .key = 'gauge')\ngauges_to_pull\n\n\n\n  \n\n\n\nUse a simple wrapper with matching names to make calling the main function easier by feeding defaults.\n\nwrap_traces <- function(owner,var_to,gauge) {\n  traces <- get_ts_traces2(state = owner, \n                            site_list = unlist(gauge, use.names = FALSE), \n                            var_list = var_to,\n                            start_time = 'all',\n                            end_time = 'all',\n                            interval = 'day',\n                            data_type = 'mean',\n                           datasource = 'A',\n                            returnformat = 'sitelist',\n                            .errorhandling = 'pass')\n  \n}\n\nThis purrr::pmap works most of the time, but occasionally hits a 504 Gateway Timeout, which seem to happen much more frequently with datasource = 'CP, but are also just haphazard. The safely lets it finish, but still annoying to have to re-do any missing pieces. Takes about 2 minutes.\n\nsystem.time(all_gauges <- purrr::pmap(gauges_to_pull, purrr::safely(wrap_traces)))\n\n   user  system elapsed \n   8.45    1.07  115.62 \n\n\nThat produces a list with 141 gauges that’s 494.3 MB (on 13 Mar 2023) in 125 seconds. Surprisingly fast, really. I’m going to save it though, both to avoid doing it every time and to have a fixed set of data to work with for reproducibility."
  },
  {
    "objectID": "scenario_creation/flow_scaling.html#cleanup",
    "href": "scenario_creation/flow_scaling.html#cleanup",
    "title": "Flow scaling demonstration",
    "section": "Cleanup",
    "text": "Cleanup\nBecause we used safely, we need to parse out errors. First, are there any?\n\nwhich(purrr::map_lgl(all_gauges, \\(x) !is.null(x$error)))\n\ninteger(0)\n\n\nExtract just the $result. I could re-map this to all_gauges so I don’t have so much in memory, but not going to bother.\n\nall_results <- purrr::map(all_gauges,\n                          \\(x) purrr::pluck(x, 'result')) %>% \n  purrr::flatten()\n\nUse is.null to catch those that errored and didn’t return.\n\ndatarows <- purrr::map_int(all_results, \n                           \\(x) if (is.null(x)) {0} else {nrow(x)})\ndatarows\n\n 412107  425020  425022  425023  409003  409017  409019  409020  409023  409024 \n  17278   15967   19924   15969   48659   34075   31778   31778   31777   34071 \n 409025  409048  410001  410005  410006  410007  410008  410014  410015  410016 \n  22747   14415   56435   47925   19286   16126   22402   16088   16685   40347 \n 410021  410033  410040  410093  410130  410134  412002  412004  412005  412011 \n  18960   39033   17581   17030   15741   16379   47826   47402   47360   41155 \n 412012  412016  412033  412036  412038  412039  412042  412046  412122  412124 \n  42384   38909   31148   43806   47286   30069   27735   38721    7182    7182 \n 412163  412188  412189  416001  416003  416006  416007  416008  416010  416011 \n   7013    7650    5746   18212   37248   19379   18918   19420   18358   18698 \n 416012  416020  416027  416032  416037  416039  416040  416047  416048  416050 \n  18977   16398   15912   19770   17773   18345    9976   14104   13084   12915 \n 416052  416072  417001  418002  418004  418011  418013  418026  418037  418048 \n  13042    4212   19060   31288   34196   16010   24878   20085   19974   14924 \n 418049  418052  418053  418055  418063  418066  418068  418070  418074  418076 \n  13225   15479   14757   15589   13594   12670   12427   10550    9442    9413 \n 418078  418079  418085  419001  419006  419007  419012  419015  419016  419020 \n   9382    9388    7292   19846   18258   18122   24894   15993   18096   16835 \n 419021  419022  419026  419027  419028  419032  419039  419045  419049  419091 \n  18815   19222   18817   18765   14768   16543   18290   18543   18317    9396 \n 420020  421001  421004  421011  421012  421019  421022  421023  421088  421090 \n   7741   65924   40243   19998   30769   24522   24847   29326   17374   13481 \n 421146  422001  422002  422003  422004  422005  422013  422015  422016  422025 \n  12916   18262   47596   15448   15590   28732   21260   23023   21257    8567 \n 422026  422027  422028  423001  423002  423004  423005  425003  425004  425007 \n   8442    8443    8600   36931   36931   10818   10817   46729   43390   30313 \n 425008  425010  425012  425013  425019  425039  425900 416201A  414209  409207 \n  33035   13216   17929   18804   22208    8599   10149   38418    1590   17622 \n 414203 \n  17361 \n\n\nCheck missing data\n\nany(datarows == 0)\n\n[1] FALSE\n\n\nCheck error_num\n\nany(purrr::map(all_results, \\(x) sum(x$error_num > 0)) %>% unlist())\n\n[1] FALSE\n\n\n\nGauge 412036 returns duplicate entries for every day from 1990-2004. The actual flow values differ for each pair and it’s not clear which to use. It seems to be an issue straight from the API- they come in that way, no matter the datasource or subset of the period. I’ve left them in here, but need to cut out one or both when we use the data. See scaling notebook.\n\n\nDrop to just EWR columns\nAll we really need for the EWR tool is date and value, but we also keep the quality code id and variable on the data too so it’s clear what we have until the data gets used.\n\njust_values <- all_results %>% \n  purrr::map(\\(x) dplyr::select(x, time, site, value, variable, \n                                quality_codes_id, quality_codes))\n\nSave the data\n\nif (REBUILD_DATA) {\n  saveRDS(just_values, file = file.path(hydro_dir, 'extracted_flows.rds'))\n}"
  },
  {
    "objectID": "scenario_creation/flow_scaling.html#todo",
    "href": "scenario_creation/flow_scaling.html#todo",
    "title": "Flow scaling demonstration",
    "section": "TODO",
    "text": "TODO\n\nsave format\n\nCurrently separate files since each sequence has a different date range\nCould have one file per scenario with cols for gauges, but would have lots of empty cells when dates don’t match\nwhatever the standard netcdf format is, once we know what it is"
  },
  {
    "objectID": "scenario_creation/flow_scaling_methods.html",
    "href": "scenario_creation/flow_scaling_methods.html",
    "title": "Flow scaling methods",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(patchwork)"
  },
  {
    "objectID": "scenario_creation/flow_scaling_methods.html#do-the-scaling",
    "href": "scenario_creation/flow_scaling_methods.html#do-the-scaling",
    "title": "Flow scaling methods",
    "section": "Do the scaling",
    "text": "Do the scaling\nThis was set up to purrr::map over all the sdl units and gauges, so has some functions that we can just bring over here, but are overkill for single gauge and sdl unit.\nFirst, some quantile-finding functions\n\n# identify quantiles\nget_q <- function(vals, q_perc) {\n  qs <- quantile(vals, probs = seq(0,1, q_perc), type = 5, na.rm = TRUE)\n  binvec <- findInterval(vals, qs, rightmost.closed = TRUE)\n  return(binvec)\n}\n\n# get the mean of a quantile\nget_qmean <- function(vals, q_perc = 0.02) {\n  binvec <- get_q(vals, q_perc)\n  qmean <- aggregate(x = vals, by = list(binvec), FUN = mean) %>% \n    setNames(c('quantile', 'mean'))\n} \n\nThen a set of functions to scale the models. This groups by month, though that could be changed.\n\n# get the scalings from a dataframe of scenarios\nget_scalings <- function(unitdf) {\n  # Stack\n  stackdf <- unitdf %>% \n  mutate(sdl = stringr::str_extract(path, \"SS[0-9]+\")) %>% \n  select(sdl, Year, Month, Day, starts_with('Sim')) %>% \n  pivot_longer(cols = starts_with('Sim'), \n               names_to = 'scenario', values_to = 'runoff')\n  \n  # Quantile means\n  q_s <- stackdf %>% \n  group_by(scenario, Month) %>%\n    summarize(qmean = get_qmean(runoff)) %>%\n    # reframe is the new way, but needs dplyr 1.1 which breaks lots of the functions\n # reframe(qmean = get_qmean(runoff)) %>%\n  tidyr::unnest(cols = qmean)\n  \n  # Get the relative change\n  q_s <- q_s %>% \n  group_by(scenario, Month) %>% \n  werptoolkitr::baseline_compare(compare_col = 'scenario', base_lev = 'SimR0', \n                   values_col = 'mean', \n                   comp_fun = `/`) %>% \n  ungroup() %>% \n  select(scenario = scenario.x, everything(), \n         relative_change = `/_mean`, \n         -scenario.y)\n}\n\nAnd we scale the scenarios\n\nqq_model <- get_scalings(test_model)\n\n`summarise()` has grouped output by 'scenario', 'Month'. You can override using\nthe `.groups` argument.\nAdding missing grouping variables: `scenario`\n\n\nNow we write a function to scale the gauge- this finds quantiles and applies the scalings from qq_model. This was again designed to map over all gauges in all sdl units, so is overkill here. I’ve cleaned out some of that, but didn’t want to spend too much time cleaning up the simple case.\n\nscale_gauges <- function(gaugedata, scaled_model, qc_limit = 150) {\n\n  # Set bad data to NA\n  gaugedata[gaugedata$quality_codes_id > qc_limit, 'value'] <- NA\n\n  # make an NA quantile for each scenario so the join works properly\n  if (any(is.na(gaugedata$value))) {\n    nafill <- scaled_model %>%\n      distinct(scenario, Month) %>%\n      mutate(quantile = NA, mean = NA, ref_mean = NA, relative_change = NA)\n\n    scaled_model <- bind_rows(scaled_model, nafill)\n  }\n\n\n  # do the transforms\n\n  gaugedata <- gaugedata %>%\n    # get the time units right\n    mutate(Month = lubridate::month(time)) %>%\n    rename(Date = time) %>%  # To match other inputs\n    group_by(Month) %>%\n    # get quantiles- make a dummy so NA quantiles exist and get join-crossed with scenarios\n    mutate(quantile = get_q(value, q_perc = 0.02)) %>%\n    ungroup() %>%\n    # join to scalings\n    left_join(scaled_model,\n              by = c('Month', 'quantile'),\n              multiple = 'all') %>% # Says it's OK to duplicate rows x scenarios\n    # get the adjusted levels\n    mutate(adj_val = value*relative_change) %>%\n    # Just the needed cols\n    dplyr::select(scenario, site, Date, adj_val) %>%\n    # pivot so the gauge name is col name\n    tidyr::pivot_wider(names_from = site, values_from = adj_val) %>%\n    # collapse to a list-tibble with one row per scenario\n    tidyr::nest(sc = scenario) %>% \n    tidyr::unnest(cols = sc)\n  \n  return(gaugedata)\n}\n\nNow we scale the gauge data\n\nqq_hydro <- scale_gauges(test_hydro, scaled_model = qq_model)\n\nThat was meant to have gauge-name-cols, but here we just have one so change the name to syntactic\n\nnames(qq_hydro)[2] <- 'value'"
  },
  {
    "objectID": "scenario_creation/flow_scaling_methods.html#outcome-distributions",
    "href": "scenario_creation/flow_scaling_methods.html#outcome-distributions",
    "title": "Flow scaling methods",
    "section": "Outcome distributions",
    "text": "Outcome distributions\n\nCompare baselines\nSince we don’t actually shift anything at baseline here, unlike the other methods, this is just looking at the input data.\nSequences. The time period differs, and that’s fine.\n\nhyd_base <- qq_hydro %>% \n  dplyr::filter(scenario == 'SimR0') %>% \n  ggplot(aes(x = Date, y = value)) +\n  geom_line()\n\nmod_base <-\n  ggplot(test_model, aes(x = time, y = SimR0)) + \n  geom_line()\n\nhyd_base + mod_base\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nCDFs- comparing to the model data, not the qq-scaled model, which is just used to transform. Top is on the raw scale, bottom is logged. This will be more relevant later, but it sets the stage of how the baseline distributions work.\n\nhyd_cdf <- qq_hydro %>% \n  dplyr::filter(scenario == 'SimR0') %>%\n  ggplot(aes(x = value)) +\n  stat_ecdf()\n\nmod_cdf <- ggplot(test_model, aes(x = SimR0)) + \n  stat_ecdf()\n\nhyd_cdf_ln <- qq_hydro %>% \n  dplyr::filter(scenario == 'SimR0') %>%\n  ggplot(aes(x = log(value))) +\n  stat_ecdf()\n\nmod_cdf_ln <- ggplot(test_model, aes(x = log(SimR0))) + \n  stat_ecdf()\n\n(hyd_cdf + mod_cdf) / (hyd_cdf_ln + mod_cdf_ln)\n\nWarning: Removed 1564 rows containing non-finite values (`stat_ecdf()`).\n\n\nWarning: Removed 4384 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nPDFs\n\nhyd_pdf <- qq_hydro %>% \n  dplyr::filter(scenario == 'SimR0') %>%\n  ggplot(aes(x = value)) +\n  geom_density()\n\nmod_pdf <- ggplot(test_model, aes(x = SimR0)) + \n  geom_density()\n\nhyd_pdf_ln <- qq_hydro %>% \n  dplyr::filter(scenario == 'SimR0') %>%\n  ggplot(aes(x = log(value))) +\n  geom_density()\n\nmod_pdf_ln <- ggplot(test_model, aes(x = log(SimR0))) + \n  geom_density()\n\n(hyd_pdf + mod_pdf) / (hyd_pdf_ln + mod_pdf_ln)\n\nWarning: Removed 1564 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 4384 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n\n\nCompare scenario shifts\nModelled timeseries- too hard to see with color, so facetting.\n\nmod_timeseries <- test_model %>% \n  pivot_longer(cols = starts_with('Sim'), names_to = 'scenario') %>% \n  ggplot(aes(x = time, y = value)) + \n  geom_line() + facet_wrap('scenario')\nmod_timeseries\n\n\n\n\nShifted hydrograph timeseries\n\nqq_timeseries <- ggplot(qq_hydro, aes(x = Date, y = value)) + \n  geom_line() + facet_wrap('scenario')\nqq_timeseries\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nDistributions- again, linear on top, log on bottom, hydrographs on left, modelled runoff on right.\n\nhyd_cdf_qq <- qq_hydro %>%\n  ggplot(aes(x = value, color = scenario)) +\n  stat_ecdf()\n\nmod_cdf_qq <- test_model %>% \n  pivot_longer(cols = starts_with('Sim'), names_to = 'scenario') %>% \n  ggplot(aes(x = value, color = scenario)) + \n  stat_ecdf()\n\nhyd_cdf_qq_ln <- qq_hydro %>%\n  ggplot(aes(x = log(value), color = scenario)) +\n  stat_ecdf()\n\nmod_cdf_qq_ln <- test_model %>% \n  pivot_longer(cols = starts_with('Sim'), names_to = 'scenario') %>% \n  ggplot(aes(x = log(value), color = scenario)) + \n  stat_ecdf()\n\n(hyd_cdf_qq + mod_cdf_qq) / (hyd_cdf_qq_ln + mod_cdf_qq_ln)\n\nWarning: Removed 12512 rows containing non-finite values (`stat_ecdf()`).\n\n\nWarning: Removed 35072 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nPDFs\n\nhyd_pdf_qq <- qq_hydro %>%\n  ggplot(aes(x = value, color = scenario)) +\n  geom_density()\n\nmod_pdf_qq <- test_model %>% \n  pivot_longer(cols = starts_with('Sim'), names_to = 'scenario') %>% \n  ggplot(aes(x = value, color = scenario)) + \n  geom_density()\n\nhyd_pdf_qq_ln <- qq_hydro %>%\n  ggplot(aes(x = log(value), color = scenario)) +\n  geom_density()\n\nmod_pdf_qq_ln <- test_model %>% \n  pivot_longer(cols = starts_with('Sim'), names_to = 'scenario') %>% \n  ggplot(aes(x = log(value), color = scenario)) + \n  geom_density()\n\n(hyd_pdf_qq + mod_pdf_qq) / (hyd_pdf_qq_ln + mod_pdf_qq_ln)\n\nWarning: Removed 12512 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 35072 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n\n\nZeros\nThe number of zeros doesn’t change.\n\nqq_hydro %>% \n  group_by(scenario) %>% \n  summarise(n_zeros = sum(value == 0, na.rm = TRUE))\n\n# A tibble: 8 × 2\n  scenario n_zeros\n  <chr>      <int>\n1 SimR0       2820\n2 SimR1       2820\n3 SimR2       2820\n4 SimR3       2820\n5 SimR4       2820\n6 SimR5       2820\n7 SimR6       2820\n8 SimR7       2820\n\n\nWhat if we ask about lowering flows? Can we put on a threshold and increase zeros? The minimum detected is 0.001\n\nqq_hydro %>% filter(scenario == 'SimR0' & value > 0) %>% summarise(minpos = min(value, na.rm = 0))\n\n# A tibble: 1 × 1\n  minpos\n   <dbl>\n1  0.001\n\n\nThat gives a few but not many more zeros in the lower runoff scenarios. Still, we wouldn’t pick anything useful up there.\n\nqq_hydro %>% \n  group_by(scenario) %>% \n  summarise(n_pseudo_zeros = sum(value < 0.001, na.rm = TRUE))\n\n# A tibble: 8 × 2\n  scenario n_pseudo_zeros\n  <chr>             <int>\n1 SimR0              2820\n2 SimR1              2826\n3 SimR2              2826\n4 SimR3              2826\n5 SimR4              2826\n6 SimR5              2826\n7 SimR6              2820\n8 SimR7              2820\n\n\nIf we used a higher threshold (I’d be surprised if 0.001 is a real measurement). A bit of shifting happening here. Still pretty low proportionally, but at least we would get a bit of variation.\n\nqq_hydro %>% \n  group_by(scenario) %>% \n  summarise(n_pseudo_zeros = sum(value < 1, na.rm = TRUE))\n\n# A tibble: 8 × 2\n  scenario n_pseudo_zeros\n  <chr>             <int>\n1 SimR0              2854\n2 SimR1              2868\n3 SimR2              2866\n4 SimR3              2861\n5 SimR4              2856\n6 SimR5              2854\n7 SimR6              2854\n8 SimR7              2853"
  },
  {
    "objectID": "scenario_creation/flow_scaling_methods.html#conclusions",
    "href": "scenario_creation/flow_scaling_methods.html#conclusions",
    "title": "Flow scaling methods",
    "section": "Conclusions",
    "text": "Conclusions\nThe qq scaling does a good job creating distributions that look like hydrograph distributions, complete with funny lumps. And their shape does change, but in a similar way to the shapes of the modelled runoff distributions.\nThe catch is that the number of zeros is constant, and so any cease to flow conditions cannot change."
  },
  {
    "objectID": "scenario_creation/flow_scaling_methods.html#do-the-scaling-1",
    "href": "scenario_creation/flow_scaling_methods.html#do-the-scaling-1",
    "title": "Flow scaling methods",
    "section": "Do the scaling",
    "text": "Do the scaling\nWe want to rank-match, but the data is different size and there are duplicate values. So to deal with that, I wrote a slightly different quantile function that divides the unique values into quantiles instead of all values, generating ranks, and then assigns all values to the appropriate rank to handle duplicated values. And to deal with the different-sized data, we calculate the number of quantiles from the length of the shortest dataset (hydrograph or model), so the ranks may not be exactly one value per rank.\n\nget_q_unique <- function(vals, q_perc) {\n  qs <- quantile(unique(vals), probs = seq(0,1, q_perc), type = 5, na.rm = TRUE)\n  binvec <- findInterval(vals, qs, rightmost.closed = TRUE)\n  return(binvec)\n}\n\nNow, get the ranks. We use the function above, and for duplicated values within a rank for the model, we return the median, which is what we will use to replace the hydrograph. There are different numbers of unique values in the different scenarios, so use the minimum. We could do this separately for all the scenarios, but it becomes horrible to track the various ranks.\n\n# Get the number of 'ranks' (bins) as the shortest set of values\nuniquemodel <- test_model %>% \n  pivot_longer(cols = starts_with('Sim')) %>% \n  group_by(name) %>% \n  summarise(nunique = n_distinct(value))\n\nfewest <- min(length(unique(test_hydro$hyd_vals)),\n              min(uniquemodel$nunique))\n\n# Rank the hydrograph\ntest_hydro <- test_hydro %>%\n  mutate(ranks = get_q_unique(hyd_vals, 1/fewest))\n  \nmodel_rankvals <- test_model %>%\n  pivot_longer(cols = starts_with('Sim'), names_to = 'scenario') %>% \n  group_by(scenario) %>% \n  mutate(ranks = get_q_unique(value, 1/fewest)) %>% \n  group_by(scenario, ranks) %>%\n  summarise(mod_vals = median(value))\n\n`summarise()` has grouped output by 'scenario'. You can override using the\n`.groups` argument.\n\n# replace (really, add another column)\nrank_hydro <- left_join(test_hydro, model_rankvals, by = 'ranks')\n\n\nBrief diagnostics of the hydro-model replacement\nThe values of the ranks follow quite clearly different distributions, even when rescaled.\n\nrank_hydro %>% \n  filter(scenario == 'SimR0') %>% \nggplot(aes(x = ranks)) + geom_line(aes(y = hyd_vals), color = 'forestgreen') + geom_line(aes(y = mod_vals*500), color = 'dodgerblue')\n\n\n\n\nThat relationship is clearly nonlinear on both the linear and log-log scales. I’ve thrown the linear fit on here, but it’s clearly a bit silly.\n\nlinrr <- rank_hydro %>% \n  filter(scenario == 'SimR0') %>% \n  ggplot(aes(x = mod_vals, y = hyd_vals)) + \n  geom_point()\n\nbump0 <- rank_hydro %>% \n  filter(scenario == 'SimR0') %>% \n  ggplot(aes(x = log(mod_vals + 0.01), y = log(hyd_vals + 0.01))) + \n  geom_point() +\n  geom_smooth(method = 'lm')\n\ndrop0 <- rank_hydro %>% \n  filter(scenario == 'SimR0') %>% \n  ggplot(aes(x = log(mod_vals), y = log(hyd_vals))) + \n  geom_point() +\n  geom_smooth(method = 'lm')\n\nlinrr + bump0 + drop0\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2820 rows containing non-finite values (`stat_smooth()`).\n\n\n\n\n\n\n\nGet the relationship\nLet’s ignore how terrible that fit is, do the translation, and then look at what we get.\nI’m going to build the regression relationship for log-log with 0s thrown out, rather than piling them all up at 0.01 or something arbitrary.\n\nrank_log <- rank_hydro %>% \n  filter(scenario == 'SimR0') %>%  \n  dplyr::filter(hyd_vals > 0) %>% \n  mutate(log_hyd = log(hyd_vals),\n         log_mod = log(mod_vals))\n\nlogregression <- lm(log_hyd ~ log_mod, data = rank_log)\n\nApply that to regain hydrograph values from the scenario values\n\nrank_hydro_shift <- rank_hydro %>% \n  mutate(log_mod = log(mod_vals),\n         log_hyd = log(hyd_vals)) %>% \n  modelr::add_predictions(logregression, 'pred_log_hyd') %>% \n  mutate(pred_hyd = exp(pred_log_hyd))\n\nIn theory, we’d have to decide now where to put the zero point, since pred_hyd here will get very small but never zero, since it’s predicted on the log scale. An obvious choice would be anything below the minumum in the original data min(rank_hydro_shift$hyd_vals[rank_hydro_shift$hyd_vals > 0], na.rm = TRUE). That number is 0.001, though that seems to be an artifact of something, and 1 is probably reasonable. But the predictions never get close- the minimum predicted value is\n\nmin(rank_hydro_shift$pred_hyd, na.rm = TRUE)\n\n[1] 22.34629\n\n\nwhich is unsurprising, from looking at those linear fits above. But it means we never predict zeros."
  },
  {
    "objectID": "scenario_creation/flow_scaling_methods.html#outcome-distributions-1",
    "href": "scenario_creation/flow_scaling_methods.html#outcome-distributions-1",
    "title": "Flow scaling methods",
    "section": "Outcome distributions",
    "text": "Outcome distributions\n\nCompare baselines\nWe’ve compared the hydrograph to the baseline model data above, but here I want to look at hydrograph data to generated ‘hydrograph data’ out of the transform with SimR0.\nTimeseries. The massive overestimates are really blowing things up here.\n\norig_time <- rank_hydro_shift %>% \n  dplyr::filter(scenario == 'SimR0') %>% \n  ggplot(aes(x = time, y = hyd_vals)) +\n  geom_line(color = 'forestgreen') \n\npred_time <- rank_hydro_shift %>% \n  dplyr::filter(scenario == 'SimR0') %>% \n  ggplot(aes(x = time, y = pred_hyd)) +\n  geom_line(color = 'dodgerblue') \n\norig_time + pred_time\n\n\n\n\nCDFs of the original data to the regression-predicted. Top is on the raw scale, bottom is logged.\nNote the big probability mass at the bottom- this is because all the zeros have the same rank, and so get the same predicted value. This will occur no matter what- even if we find a better regression. This mass is not htere in the data because the zeros are just dropped. If we had data + 0.01, it’d show up.\nNote the massive difference in x-axes here\n\nhyd_cdf <- rank_hydro_shift %>% \n  dplyr::filter(scenario == 'SimR0') %>%\n  ggplot(aes(x = hyd_vals)) +\n  stat_ecdf()\n\nreg_cdf <- rank_hydro_shift %>% \n  dplyr::filter(scenario == 'SimR0') %>%\n  ggplot(aes(x = pred_hyd)) +\n  stat_ecdf()\n\nhyd_cdf_ln <- rank_hydro_shift %>% \n  dplyr::filter(scenario == 'SimR0') %>%\n  ggplot(aes(x = log(hyd_vals))) +\n  stat_ecdf()\n\nreg_cdf_ln <- rank_hydro_shift %>% \n  dplyr::filter(scenario == 'SimR0') %>%\n  ggplot(aes(x = log(pred_hyd))) +\n  stat_ecdf()\n\n(hyd_cdf + reg_cdf) / (hyd_cdf_ln + reg_cdf_ln)\n\nWarning: Removed 2820 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nPDFs. Note the massive difference in x-axes here too, so I’ve made a version explicitly comparing them.\n\nhyd_pdf <- rank_hydro_shift %>% \n  dplyr::filter(scenario == 'SimR0') %>%\n  ggplot(aes(x = hyd_vals)) +\n  geom_density()\n\nreg_pdf <- rank_hydro_shift %>% \n  dplyr::filter(scenario == 'SimR0') %>%\n  ggplot(aes(x = pred_hyd)) +\n  geom_density()\n\nhyd_pdf_ln <- rank_hydro_shift %>% \n  dplyr::filter(scenario == 'SimR0') %>%\n  ggplot(aes(x = log(hyd_vals))) +\n  geom_density()\n\nreg_pdf_ln <- rank_hydro_shift %>% \n  dplyr::filter(scenario == 'SimR0') %>%\n  ggplot(aes(x = log(pred_hyd))) +\n  geom_density()\n\nhyd_reg_comp <- rank_hydro_shift %>% \n  dplyr::filter(scenario == 'SimR0') %>%\n  ggplot() +\n  geom_density(aes(x = log(hyd_vals)), color = 'forestgreen') +\n  geom_density(aes(x = log(pred_hyd)), color = 'dodgerblue') +\n  coord_cartesian(x = c(0, 10))\n\n(hyd_pdf + reg_pdf) / (hyd_pdf_ln + reg_pdf_ln) /\n  (hyd_reg_comp)\n\nWarning: Removed 2820 rows containing non-finite values (`stat_density()`).\nRemoved 2820 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n\n\nCompare scenario shifts\nThese contain the same lines as above, but I think it’s valuable to isolate the baseline transform before looking at the scenario shifts. These are the distributions of the ‘hydrographs’ that reflect the scenarios. I’ve included an x-limited panel on the linear scale since the values reach such absurd levels.\n\nreg_cdf_all <- rank_hydro_shift %>% \n  ggplot(aes(x = pred_hyd, color = scenario)) +\n  stat_ecdf()\n\nreg_cdf_ln_all <- rank_hydro_shift %>% \n  ggplot(aes(x = log(pred_hyd), color = scenario)) +\n  stat_ecdf()\n\n(reg_cdf_all + reg_cdf_all + coord_cartesian(xlim = c(0, 5000))) / reg_cdf_ln_all\n\nWarning: Removed 1564 rows containing non-finite values (`stat_ecdf()`).\nRemoved 1564 rows containing non-finite values (`stat_ecdf()`).\nRemoved 1564 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nPDFs. Again, added an x-limited version to see what’s happening low. It’s clear the linear fits are causing major issues at both the bottom and the top of the distribution.\n\nreg_pdf_all <- rank_hydro_shift %>% \n  ggplot(aes(x = pred_hyd, color = scenario)) +\n  geom_density()\n\nreg_pdf_ln_all <- rank_hydro_shift %>% \n  ggplot(aes(x = log(pred_hyd), color = scenario)) +\n  geom_density()\n\n(reg_pdf_all + reg_pdf_all + coord_cartesian(xlim = c(0, 5000))) / reg_pdf_ln_all\n\nWarning: Removed 1564 rows containing non-finite values (`stat_density()`).\nRemoved 1564 rows containing non-finite values (`stat_density()`).\nRemoved 1564 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n\n\nZeros\nThere are no zeros in the data.\n\nrank_hydro_shift %>% \n  group_by(scenario) %>% \n  summarise(n_zeros = sum(pred_hyd == 0, na.rm = TRUE))\n\n# A tibble: 9 × 2\n  scenario n_zeros\n  <chr>      <int>\n1 SimR0          0\n2 SimR1          0\n3 SimR2          0\n4 SimR3          0\n5 SimR4          0\n6 SimR5          0\n7 SimR6          0\n8 SimR7          0\n9 <NA>           0\n\n\nThe number of zeros is essentially arbitrary though, dependent on the zero point and the regression. So, for example if we make the zero point 25, the number changes.\n\nrank_hydro_shift %>% \n  group_by(scenario) %>% \n  summarise(n_zeros = sum(pred_hyd <= 25, na.rm = TRUE))\n\n# A tibble: 9 × 2\n  scenario n_zeros\n  <chr>      <int>\n1 SimR0          0\n2 SimR1       2920\n3 SimR2       2845\n4 SimR3          0\n5 SimR4          0\n6 SimR5          0\n7 SimR6          0\n8 SimR7          0\n9 <NA>           0\n\n\nThe problem is that it’s never continuous- all the zeros always get the same value, that value just changes.\nSo, we can add zeros if we use scenarios where the new_zero_val here is below the one for SimR0, but it doesn’t make sense to add water to those where it’s higher. That is only two scenarios though, so maybe that’s the way to go.\n\nrank_hydro_shift %>% \n  dplyr::filter(hyd_vals == 0) %>% \n  group_by(scenario) %>% \n  summarise(new_zero_val = unique(pred_hyd))\n\n# A tibble: 8 × 2\n  scenario new_zero_val\n  <chr>           <dbl>\n1 SimR0            28.4\n2 SimR1            22.3\n3 SimR2            23.9\n4 SimR3            25.4\n5 SimR4            26.4\n6 SimR5            27.5\n7 SimR6            28.6\n8 SimR7            29.7\n\n\nLet’s say we were to make the SimR0 value the zero value. How much variation in the number of zeros would we get? 450-ish from top to bottom.\n\nrank_hydro_shift %>%\n  group_by(scenario) %>% \n  summarise(n_pseudo_zeros = sum(pred_hyd < 28.44, na.rm = TRUE))\n\n# A tibble: 9 × 2\n  scenario n_pseudo_zeros\n  <chr>             <int>\n1 SimR0              2820\n2 SimR1              3268\n3 SimR2              2994\n4 SimR3              2873\n5 SimR4              2853\n6 SimR5              2834\n7 SimR6                 0\n8 SimR7                 0\n9 <NA>                  0"
  },
  {
    "objectID": "scenario_creation/flow_scaling_methods.html#do-the-scaling-2",
    "href": "scenario_creation/flow_scaling_methods.html#do-the-scaling-2",
    "title": "Flow scaling methods",
    "section": "Do the scaling",
    "text": "Do the scaling\nWe fit a lognormal, but also shift the data off zero by some optimum amount. This is equivalent to saying there’s some unknown part of the distribution that extends below 0- we just have to shift up to allow the lognormal to fit it. We could do something similar by pre-logging the data and fitting a truncated normal.\nIn testing, I developed some functions that do this optimisation and fit the lognormals. This is ugly because it returns a dataframe instead of easy-to-use parameters, but I’m not going to bother changing it now. Easy enough to later if we go this way. The dataframe is handy to keep track of the shifts.\n\nfitshift <- function(cleandata, shift_up) {\n  # Handle the zero shift case- we just use the next value up\n    if (shift_up == 0) {rightlim <- min(cleandata[cleandata>0])\n    } else {\n      rightlim <- shift_up}\n  \n  inshift <- cleandata + shift_up\n    \n    upcens <- tibble(left = ifelse(inshift <= shift_up, NA, inshift),\n                right = ifelse(inshift <= shift_up, rightlim, inshift))\n    \n    suppressWarnings(fit_up <- fitdistrplus::fitdistcens(censdata = data.frame(upcens),\n                                           distr = 'lnorm'))\n    \n    return(fit_up)\n}\n\n\nopt_up <- function(shift_up, cleandata) {\n  \n  fit_up <- fitshift(cleandata, shift_up)\n  \n  return(-fit_up$loglik)\n}\n\n\noptshift <- function(rawdata) {\n  \n  # This is about distributions, NOT data order, so get rid of NAs\n  rawna <- na.omit(rawdata$hyd_vals)\n  \n  # get the optimal shift\n  shift <- optimize(opt_up, interval = c(0, 1000), cleandata = rawna)\n  \n  # Get the fit at that shift (would be nice to kick this out of opt_up somehow)\n  \n  fit_up <- fitshift(rawna, shift$minimum)\n  \n # Create a df for output\n  # The shifted data\n  shiftdf <- tibble(time = rawdata$time,\n    orig_data = rawdata$hyd_vals, \n                    shift_data = rawdata$hyd_vals + shift$minimum, \n                    optimum_shift = shift$minimum)\n  \n\n   # This isn't ideal, but we can shove the cdf on here too, it just has rows that don't mean the same thing. prevents us saving a list though.\n  shiftdf <- shiftdf |> \n    mutate(x = row_number()/10,\n           meanlog = fit_up$estimate['meanlog'],\n           sdlog = fit_up$estimate['sdlog'],\n           cdf_up = plnorm(x, \n                             fit_up$estimate['meanlog'],\n                             fit_up$estimate['sdlog']),\n           pdf_up = dlnorm(x, \n                             fit_up$estimate['meanlog'],\n                             fit_up$estimate['sdlog']),\n           # Some diagnostics\n           fitloglik = fit_up$loglik)\n  \n  # and a shifted-back version of the cdf/pdf just needs a shifted x. The\n  # backshift of the data is just the original `rawdata`.\n  shiftdf <- shiftdf |> \n    mutate(x_back = x-shift$minimum)\n\n}\n\nNow we use that to fit the data. This gives us the best lognormal fit for the input hydrograph.\n\noptimal_fit <- test_hydro %>% \n  select(time, hyd_vals) %>% \n  optshift()\n\nWe also need the best lognormal fit for the model runoff for all scenarios\n\n# make a df from the fit so we can return it\nfittabler <- function(x) {\n  # uncensored, but use fitdistrplus function instead of MASS for consistency\n  fitout <- fitdistrplus::fitdist(x, distr = 'lnorm')\n  \n  fittable <- tibble(meanlog = fitout$estimate['meanlog'],\n                     sdlog = fitout$estimate['sdlog'],\n                     loglik = fitout$loglik)\n  \n  return(fittable)\n}\n\nscene_fits <- test_model %>% \n  pivot_longer(starts_with('Sim'), names_to = 'scenario') %>% \n  group_by(scenario) %>% \n  summarize(fit_sims = fittabler(value)) %>% \n  ungroup() %>% \n  unnest(fit_sims)\n\n\nBrief diagnostics- how are the fits?\nThe hydrograph- the red line is the best fit curve, shifted back, so we can see what it would do below 0.\n\nggplot(optimal_fit) +\n  # ecdf of the hydrograph data\n  stat_ecdf(mapping = aes(x = orig_data), color = 'forestgreen') +\n  # lognormal cdf\n  geom_line(aes(x = x_back, y = cdf_up), \n            color = 'firebrick', linetype = 2) +\n  coord_cartesian(xlim = c(-10, 1000))\n\nWarning: Removed 1564 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nScenarios\nFirst we need to make the cdfs\n\nmakecdf_df <- function(scenario, meanlog, sdlog, loglik, x = seq(0, 30, by = 0.01)) {\n  cdf <- plnorm(x, meanlog, sdlog)\n  pdf <- dlnorm(x, meanlog, sdlog)\n  \n  return(tibble(x = x, cdf = cdf, pdf = pdf, scenario = scenario))\n}\n\nscenecdf <- purrr::pmap(scene_fits, makecdf_df) %>% \n  bind_rows()\n\nThen we rearrange some data and plot\n\nmodel_long <- test_model %>% \n  pivot_longer(starts_with('Sim'), names_to = 'scenario')\n\n\nggplot() +\n  stat_ecdf(data = model_long, mapping = aes(x = value, color = scenario)) +\n  geom_line(data = scenecdf, mapping = aes(x = x, y = cdf, \n                                           color = scenario),\n            linetype = 2) +\n  coord_cartesian(xlim = c(0, 1))\n\n\n\n\nThat’s really hard to see, what if we just limit it to the lowest and highest?\n\nggplot() +   \n  stat_ecdf(data = filter(model_long, scenario %in% c('SimR1', 'SimR7')),             mapping = aes(x = value, color = scenario)) +   \n  geom_line(data = filter(scenecdf, scenario %in% c('SimR1', 'SimR7')),             mapping = aes(x = x, y = cdf,\n            color = scenario),\n            linetype = 2) +\n  coord_cartesian(xlim = c(0, 1))\n\n\n\n\n\n\nShift the distributions\nThe means and sds here are on the log scale, and so can be treated like normal parameters. AND, because they’re on the log scale, arithmetic changes in mean yield multiplicative changes in the data.\nSo, to shift NORMAL distributions, we just add means and so we need the difference from each scenario mean to the SimR0. And to shift the sds, we use a multiplicative shift.\nBasically, the mean shift needs to be calculated as the mean_new - mean_reference and the sd shift needs to be sd_new/sd_reference, where *new are the SimR1...7, and *reference are the SimR0. And then to apply them to the hydrograph distribution, the new sd is old_sd*sd_shift and new mean is old_mean + mean_shift.\nSo, let’s get those shift values for each scenario.\nI need to fix werptoolkitr to allow extra columns, but need to get this done more so, do the mean and log separately and glue together.\n\nscene_fits_mean <- scene_fits %>% \n  dplyr::select(scenario, meanlog) %>% \n  werptoolkitr::baseline_compare(compare_col = 'scenario', \n                   base_lev = 'SimR0', \n                   values_col = 'meanlog', \n                   comp_fun = 'difference')\n\nscene_fits_sd <- scene_fits %>% \n  dplyr::select(scenario, sdlog) %>% \n  werptoolkitr::baseline_compare(compare_col = 'scenario', \n                   base_lev = 'SimR0', \n                   values_col = 'sdlog', \n                   comp_fun = 'relative')\n\nscene_fits_shift <- scene_fits %>% \n  left_join(scene_fits_mean) %>% \n  left_join(scene_fits_sd)\n\nJoining, by = c(\"scenario\", \"meanlog\")\nJoining, by = c(\"scenario\", \"sdlog\")\n\n\nI don’t really want to be operating out of a dataframe just to get single numbers, but I think I will for the moment to keep things consistent and clean up later.\nShift the hydrograph parameters to match the shifts in the runoff parameters\n\nhydro_shift_dist <- scene_fits_shift %>% \n  mutate(hydro_meanlog = optimal_fit$meanlog[1] + difference_meanlog,\n         hydro_sdlog = optimal_fit$sdlog[1] * relative_sdlog,\n         hydro_shift = optimal_fit$optimum_shift[1])\n\nAnd now we use those to translate the hydrograph to each of the distributions. We do that with a pmap over the parameters we just got. There’s a lot of extra junk in this df, so I’m tossing some (the in-built distributions, which always were weird to be in here).\n\n# We need the empirical cdf to get the probability for each x\nphydro <- ecdf(optimal_fit$shift_data)\n\nscene_trans <- function(scenario, hydro_meanlog, hydro_sdlog, hydro_shift) {\n  trans_hydro <- optimal_fit %>% \n  mutate(scenario = scenario,\n         p_shift = phydro(shift_data),\n         q_shiftln = qlnorm(p_shift, \n                            hydro_meanlog,\n                            hydro_sdlog),\n         q_back = q_shiftln-hydro_shift)\n}\n  \n\n\n\nhydro_trans <- hydro_shift_dist %>% \n  dplyr::select(scenario, hydro_meanlog, hydro_sdlog, hydro_shift) %>% \n  purrr::pmap(scene_trans) %>% \n  bind_rows() %>% \n  select(scenario, time, orig_data, shift_data, q_back, q_shiftln, optimum_shift)"
  },
  {
    "objectID": "scenario_creation/flow_scaling_methods.html#outcome-distributions-2",
    "href": "scenario_creation/flow_scaling_methods.html#outcome-distributions-2",
    "title": "Flow scaling methods",
    "section": "Outcome distributions",
    "text": "Outcome distributions\n\nCompare baselines\nTimeseries- there are some very high values, but if we clip them off (bottom panel), it actually looks pretty good.\n\norig_time <- hydro_trans %>% \n  dplyr::filter(scenario == 'SimR0') %>% \n  ggplot(aes(x = time, y = orig_data)) +\n  geom_line(color = 'forestgreen') \n\npred_time <- hydro_trans %>% \n  dplyr::filter(scenario == 'SimR0') %>% \n  ggplot(aes(x = time, y = q_back)) +\n  geom_line(color = 'dodgerblue') \n\norig_time / pred_time / (pred_time + coord_cartesian(ylim = c(0, 10000)))\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\nRemoved 1 row containing missing values (`geom_line()`).\nRemoved 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nCDF. The fit for the baseline matches 0 in the data to 4.54 in the fit distribution, and so we end up with the probability spike from all those zeros right at the beginning. IFor this baseline case, we want to check the fit and the spike makes that difficult. So I also create a ‘corrected’ version that brings the 0s back to 0, and look at it on the log scale where it’s easier to see. This is fudging a bit, and we probably should think carefully about whether to do this if we move forward with this approach.\n\ntrans_cdf <- hydro_trans %>% \n    dplyr::filter(scenario == 'SimR0') %>% \n  ggplot() +\n  stat_ecdf(mapping = aes(x = orig_data), color = 'forestgreen') +\n  stat_ecdf(mapping = aes(x = q_back), color = 'dodgerblue')\n\ntrans_cdf_ln <- hydro_trans %>% \n    dplyr::filter(scenario == 'SimR0') %>%  \n  ggplot() +\n  stat_ecdf(mapping = aes(x = log(orig_data)), color = 'forestgreen') +\n  stat_ecdf(mapping = aes(x = log(q_back)), color = 'dodgerblue')\n\ntrans_cdf_ln_corrected <- hydro_trans %>% \n    dplyr::filter(scenario == 'SimR0') %>%  \n  ggplot() +\n  stat_ecdf(mapping = aes(x = log(orig_data)), color = 'forestgreen') +\n  stat_ecdf(mapping = aes(x = log(q_back-min(q_back, na.rm = TRUE))), color = 'dodgerblue')\n\n((trans_cdf + coord_cartesian(xlim = c(0, 2000))) +  trans_cdf_ln) / trans_cdf_ln_corrected\n\nWarning: Removed 1564 rows containing non-finite values (`stat_ecdf()`).\n\n\nWarning: Removed 1565 rows containing non-finite values (`stat_ecdf()`).\n\n\nWarning: Removed 4384 rows containing non-finite values (`stat_ecdf()`).\n\n\nWarning: Removed 1565 rows containing non-finite values (`stat_ecdf()`).\n\n\nWarning: Removed 4384 rows containing non-finite values (`stat_ecdf()`).\n\n\nWarning: Removed 4385 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nPDF. The probability spike shows up even more strongly in the log-scale pdf. We also see pretty clearly that this distribution is just fundamentally not lognormal- it is too peaky and asymmetric and multimodal. The multimodality really makes it unlikely that just changing distribution would fix things.\n\ntrans_pdf <- hydro_trans %>% \n    dplyr::filter(scenario == 'SimR0') %>% \n  ggplot() +\n  geom_density(mapping = aes(x = orig_data), color = 'forestgreen') +\n geom_density(mapping = aes(x = q_back), color = 'dodgerblue')\n\ntrans_pdf_ln <- hydro_trans %>% \n    dplyr::filter(scenario == 'SimR0') %>%  \n  ggplot() +\n  geom_density(mapping = aes(x = log(orig_data)), color = 'forestgreen') +\n  geom_density(mapping = aes(x = log(q_back)), color = 'dodgerblue')\n\ntrans_pdf_ln_corrected <- hydro_trans %>% \n    dplyr::filter(scenario == 'SimR0') %>%  \n  ggplot() +\n  geom_density(mapping = aes(x = log(orig_data)), color = 'forestgreen') +\n  geom_density(mapping = aes(x = log(q_back-min(q_back, na.rm = TRUE))), color = 'dodgerblue')\n\n((trans_pdf + coord_cartesian(xlim = c(0, 2000))) +  trans_pdf_ln) / trans_pdf_ln_corrected\n\nWarning: Removed 1564 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 1565 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 4384 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 1565 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 4384 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 4385 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n\n\nCompare scenario shifts\nTimeseries. I’ve clipped y at 10000 again, just because the tail really blows out.\n\npred_scene_time <- hydro_trans %>% \n  ggplot(aes(x = time, y = q_back)) +\n  geom_line() + facet_wrap('scenario') \n\npred_scene_time + coord_cartesian(ylim = c(0, 10000))\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nCDF- Notice the issues with all the 0s being given the same positive number for the increased water scenarios. And perhaps most concerning, the real hydrograph distribution is more different than any of the fit distributions than they are from each other.\n\ntrans_cdf_all <- hydro_trans %>% \n  ggplot(aes(x = q_back, color = scenario)) +\n  stat_ecdf() +\n  stat_ecdf(mapping = aes(x = orig_data), color = 'black')\n\ntrans_cdf_ln_all <- hydro_trans %>% \n  ggplot(aes(x = log(q_back), color = scenario)) +\n  stat_ecdf() +\n  stat_ecdf(mapping = aes(x = log(orig_data)), color = 'black')\n\ntrans_cdf_all + trans_cdf_ln_all\n\nWarning: Removed 12520 rows containing non-finite values (`stat_ecdf()`).\n\n\nWarning: Removed 12512 rows containing non-finite values (`stat_ecdf()`).\n\n\nWarning in log(q_back): NaNs produced\n\nWarning in log(q_back): NaNs produced\n\n\nWarning: Removed 35705 rows containing non-finite values (`stat_ecdf()`).\n\n\nWarning: Removed 35072 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nPDFs. Again, the increased water scenarios have a massive density spike because all zeros get the same number. Here, we can really see clearly that the real hydrograph distribution in black differs from all the transformed distributions (including the R0) much more than the scenario transforms differ from each other. So the act of making the hydrograph a lognormal is a bigger shift than the scenarios are.\n\ntrans_pdf_all <- hydro_trans %>% \n  ggplot(aes(x = q_back, color = scenario)) +\n  geom_density() +\n  geom_density(mapping = aes(x = orig_data), color = 'black')\n\ntrans_pdf_ln_all <- hydro_trans %>% \n  ggplot(aes(x = log(q_back), color = scenario)) +\n  geom_density() +\n  geom_density(mapping = aes(x = log(orig_data)), color = 'black')\n\ntrans_pdf_all + trans_pdf_ln_all\n\nWarning: Removed 12520 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 12512 rows containing non-finite values (`stat_density()`).\n\n\nWarning in log(q_back): NaNs produced\n\nWarning in log(q_back): NaNs produced\n\n\nWarning: Removed 35705 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 35072 rows containing non-finite values (`stat_density()`)."
  },
  {
    "objectID": "scenario_creation/flow_scaling_methods.html#compare-scenarios",
    "href": "scenario_creation/flow_scaling_methods.html#compare-scenarios",
    "title": "Flow scaling methods",
    "section": "Compare scenarios",
    "text": "Compare scenarios\nTImeseries- I’ve marked the zeros with cyan.\n\nqq_reg_timeseries <- ggplot(qq_reg, aes(x = Date, y = value)) + \n  geom_line() + facet_wrap('scenario') +\n  geom_point(data = filter(qq_reg, value == 0), color = 'cyan3')\nqq_reg_timeseries\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nCDF\n\nhyd_cdf_qq_reg <- qq_reg %>%\n  ggplot(aes(x = value, color = scenario)) +\n  stat_ecdf()\n\nhyd_cdf_qq_reg_ln <- qq_reg %>%\n  ggplot(aes(x = log(value), color = scenario)) +\n  stat_ecdf()\n\nhyd_cdf_qq_reg + hyd_cdf_qq_reg_ln\n\nWarning: Removed 12512 rows containing non-finite values (`stat_ecdf()`).\n\n\nWarning: Removed 33170 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nPDF. This is yielding weird spikes just above zero, and I’m not a huge fan of introducing nonmonotonicity. Could we fix that with a different cutoff? It is shifting the zeros. Will need to think more about it. I’ve provided a bottom panel to zoom in to the linear scale near zero to see what kind of flows we’re talking about.\n\nhyd_pdf_qq_reg <- qq_reg %>%\n  ggplot(aes(x = value, color = scenario)) +\n  geom_density()\n\nhyd_pdf_qq_reg_ln <- qq_reg %>%\n  ggplot(aes(x = log(value), color = scenario)) +\n  geom_density()\n\n(hyd_pdf_qq_reg + hyd_pdf_qq_reg_ln) /  (hyd_pdf_qq_reg + coord_cartesian(xlim = c(0, 500)))\n\nWarning: Removed 12512 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 33170 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 12512 rows containing non-finite values (`stat_density()`)."
  },
  {
    "objectID": "scenario_creation/flow_scaling_methods.html#zeros-2",
    "href": "scenario_creation/flow_scaling_methods.html#zeros-2",
    "title": "Flow scaling methods",
    "section": "Zeros",
    "text": "Zeros\nNow we get almost 500 difference in the number of zeros across the distribution. That’s on par with the regression method, and the overall distribution looks much more like a hydrograph, even with the weird nonmonotonicity at the bottom..\n\nqq_reg %>% \n  group_by(scenario) %>% \n  summarise(n_zeros = sum(value == 0, na.rm = TRUE))\n\n# A tibble: 8 × 2\n  scenario n_zeros\n  <chr>      <int>\n1 SimR0       2832\n2 SimR1       3324\n3 SimR2       3079\n4 SimR3       2890\n5 SimR4       2864\n6 SimR5       2843\n7 SimR6       2826\n8 SimR7          0"
  },
  {
    "objectID": "scenario_creation/scenario_creation_demo_R.html#toolkit-relevance",
    "href": "scenario_creation/scenario_creation_demo_R.html#toolkit-relevance",
    "title": "Creating simple scenarios",
    "section": "Toolkit relevance",
    "text": "Toolkit relevance\nThe creation of flow scenarios is not part of the toolkit proper. Instead, the toolkit expects to ingest hydrographs and then handles the ongoing response models, aggregation, and analyses. Thus, hydrographs are an essential input to the toolkit. The point of this code is to generate those hydrographs.\nThis notebook creates a minimal set of hydrographs to test and demonstrate the toolkit. The primary needs are multiple guages in multiple catchments (or other spatial units), and scenarios defined by different hydrographs for the same gauge."
  },
  {
    "objectID": "scenario_creation/scenario_creation_demo_R.html#process",
    "href": "scenario_creation/scenario_creation_demo_R.html#process",
    "title": "Creating simple scenarios",
    "section": "Process",
    "text": "Process\nWe pull a limited set of gauges for a limited time period to keep this dataset small. Primarily, we identify a set of gauges in two catchments, pull them for a short time period, and adjust them to create two simple modified scenarios, with the original data serving as the baseline scenario. Along the way, we examine the data in various ways to visualise what we’re doing and where.\nA larger and more complex set of scenarios is created in the flow scaling demonstration, without as much visualisation."
  },
  {
    "objectID": "scenario_creation/scenario_creation_demo_R.html#paths-and-other-data",
    "href": "scenario_creation/scenario_creation_demo_R.html#paths-and-other-data",
    "title": "Creating simple scenarios",
    "section": "Paths and other data",
    "text": "Paths and other data\nThe shapefiles used to see what we’re doing and do the selecting were produced with within the WERP_toolkit package to keep consistency. It’s possible we’ll add more shapefile creation and move all the canonical versions and their creation to their own data package or repo.\nSet the data directory to make that easy to change. These should usually point to external shared directories. For this simple example though, we put the data inside the repo to make it self contained. The larger example sends them externally, which would be more typical.\n\nscenario_dir <- 'scenario_example'\nhydro_dir <- file.path(scenario_dir, 'hydrographs')"
  },
  {
    "objectID": "scenario_creation/scenario_creation_demo_R.html#language-note",
    "href": "scenario_creation/scenario_creation_demo_R.html#language-note",
    "title": "Creating simple scenarios",
    "section": "Language note",
    "text": "Language note\nThis notebook was originally built using only python, and there is still a python-only version, though it is maintained less frequently. Using Python makes a lot of sense because the underlying data here uses python packages. I’ve moved the active version of this notebook to R, however, when the toolkit became an R package and the flow scaling demonstration ended up using R gauge pullers. There is still some remaining python in here (pulling gauges and some minor EWR functions). This notebook provides an example of how to mix R and python code chunks, which we do fairly frequently.\nWe can access python objects in R with py$objectnameand access R objects in python with r.objectname .\nIt takes -forever- to do a type translation on the DATETIME column in the gauge data. It’s unclear why (can’t replicate it with any other datetime py object). We work around that by changing it to something simple while still in python, and change it back to datetime in R."
  },
  {
    "objectID": "scenario_creation/scenario_creation_demo_R.html#spatial-datasets",
    "href": "scenario_creation/scenario_creation_demo_R.html#spatial-datasets",
    "title": "Creating simple scenarios",
    "section": "Spatial datasets",
    "text": "Spatial datasets\nWe use spatial datasets provided by {werptoolkitr}, which creates a standard set in data_creation/spatial_data_creation.qmd. These are visualised in a separate notebook. Relevant to this scenario creation, we are interested in the gauges, (werptoolkitr::bom_basin_gauges) since this is what were contained in the EWR tool. We use the sdl_units dataset to obtain a subset of gauges for these simple scenarios. Relevant to the case study- the original polygon used was the Macquarie-Castlereagh in the resource_plan_areas, though we seem to use sdl units elsewhere, so I’ll use them here."
  },
  {
    "objectID": "scenario_creation/scenario_creation_demo_R.html#get-relevant-gauges",
    "href": "scenario_creation/scenario_creation_demo_R.html#get-relevant-gauges",
    "title": "Creating simple scenarios",
    "section": "Get relevant gauges",
    "text": "Get relevant gauges\nCut the bom_basin_gauges from the whole country to just those four catchments\n\ndemo_gauges <- st_intersection(bom_basin_gauges, catch_demo)\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nHow many are there?\n\ndemo_gauges %>% nrow()\n\n[1] 295\n\n\nThat’s a fair number, but they won’t all be in the EWR.\n\nExtract their names\nTo feed to the gauge puller, we need their gauge numbers.\n\ngaugenums <- demo_gauges$gauge\n\n\n\nFind those relevant to toolkit\nWe have the list of gauges, but now we need to cut the list down to those in the EWR tool. There’s not any point in pulling gauges that do not appear later in the toolkit.\nWhich gauges are actually in the EWR tool? The EWR tool has a function, so use that.\n\nTODO THIS FAILS AS OF 1.0.4. I have rolled back to ewr version 1.0.1, since the necessary file just doesn’t exist in 1.0.4 (and in about half the branches on github). This needs to be updated and tested.\nError messages:\nFileNotFoundError: [Errno 2] No such file or directory: 'py_ewr/parameter_metadata/NSWEWR.csv'\n\nError in py_get_attr_impl(x, name, silent) : \n  AttributeError: module '__main__' has no attribute 'ewrs'\n\n\nfrom py_ewr.data_inputs import get_EWR_table\newrs, badewrs = get_EWR_table()\n\nWhat are those gauges, and which are in both the ewr and the desired catchments?\n\newrgauges = py$ewrs$Gauge\newr_demo_gauges = gaugenums[gaugenums %in% ewrgauges]\nlength(ewr_demo_gauges)\n\n[1] 47\n\n\n47 isn’t too many.\n\n\nGet all the gauge data\nNow we have a list of gauges, we need their hydrographs. We need a reasonable time span to account for temporal variation, but not too long- this is a simple case. Let’s choose 10 years.\n\nstarttime = lubridate::ymd(20100101)\nendtime = lubridate::ymd(20191231)\n\nPull the gauges with mdba_gauge_getter. The type-translation that happens in here is because translating from python time to R time is extremely slow for this particular case (though not in general).\n\nimport mdba_gauge_getter as gg\ndemo_levs = gg.gauge_pull(r.ewr_demo_gauges, start_time_user = r.starttime, end_time_user = r.endtime)\n\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 1 of 10\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 2 of 10\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 3 of 10\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 4 of 10\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 5 of 10\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 6 of 10\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 7 of 10\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 8 of 10\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 9 of 10\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 10 of 10\n\ndemo_ids = demo_levs.SITEID.unique()\nlen(demo_ids)\n\n# I think this will work, the above is running\n\n46\n\ndemo_levs['Date'] = demo_levs['DATETIME'].astype(str)\n\nDo a bit of cleanup- for some reason demo_levs['VALUE'] is an object and not numeric, and 'DATETIME' needs to be named Date for the EWR tool to read it. I copy the py object to R for this manipulation and visualisation, but we could just proceed in python if we wanted.\n\ndemo_levs <- py$demo_levs\n\ndemo_levs$VALUE = as.numeric(demo_levs$VALUE)\n\n# # In python, we just need to change the name of the date column. Here, we need to change the python datetime.date objects to R dates\n# \n# # Really slow\n# # MUCH faster to just make the dates characters in python, and back to dates here.\n# rdates <- purrr::map(demodates, py_to_r) %>% \n#   tibble(.name_repair = ~'Date') %>%  \n#   unnest(cols = Date)\n# \n# demo_levs <- bind_cols(rdates, demo_levs)\ndemo_levs <- dplyr::select(demo_levs, -DATETIME) %>% \n  dplyr::mutate(Date = lubridate::ymd(Date))\n\n\n\nMap the gauges\n\ndemo_geo = bom_basin_gauges %>% dplyr::filter(gauge %in% py$demo_ids)\n\nLooks reasonable. Probably overkill for testing, but can do a cut down version too.\n\n(ggplot() + \ngeom_sf(data = basin, fill = 'lightsteelblue') +\ngeom_sf(data = catch_demo, mapping = aes(fill = SWSDLID)) +\ngeom_sf(data = demo_geo, color = 'black') +\nscale_fill_brewer(type = 'qual', palette = 8))"
  },
  {
    "objectID": "scenario_creation/scenario_creation_overview.html",
    "href": "scenario_creation/scenario_creation_overview.html",
    "title": "Scenario creation",
    "section": "",
    "text": "The toolkit proper begins with hydrographs as inputs. The creation of those hydrographs, and particularly their modification to create scenarios is therefore typically a step that occurs prior to the use of the toolkit. For the demonstrations here, however, we generate some example scenarios from historical hydrographs. In a very simple case used for most of the capacity demonstration simply multiplies short hydrographs by 4 and 0.25 and puts them in a standard toolkit input format using the simple scenario notebook.\nA more complex set of scenarios designed to test the toolkit and module sensitivity and be a more complete example is created from scaling historical flows according to future runoff scenarios. This setup is done in notebooks that pull the historical data and scale that data and save it in the standard format."
  }
]