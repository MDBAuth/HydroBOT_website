---
title: "Read_and_agg in detail"
author: "Galen Holt"
format:
  html:
    df-print: paged
    code-link: true # Doesn't do anything without either cran or pkgdown site, btu that's still kind of useful for external (especially {pkgname})
---

```{r}
#| include: false
source("R/helpers.R")
make_hydro_csv()
make_ewr_output()
```

```{r}
#| message: false
library(HydroBOT)
library(dplyr)
library(ggplot2)
```

## Overview

The most common way to run the Aggregator is with `read_and_agg()`, which automates data read-in, processing, parallelisation, metadata, and saving. This requires that the output of the response models is saved out, which is almost always the case for both recordkeeping and processing purposes. This vastly simplifies automated, consistent running over many scenarios. 

To do the same analyses as in the [multi aggregate example](using_multi_aggregate.qmd) but using `read_and_agg()`, we give it the path to the data instead of the data itself. If the directory contains multiple files, `read_and_agg()` provides capacity to operate over those files in parallel.

::: callout-important
Subdirectories should represent scenarios, because scenario outcomes are not interdependent (should be compared, not combined). In contrast, other dimensions (e.g. location) *are* interdependent. If directories are not scenarios but separate units that should be aggregated, e.g. gauges which should be aggregated to basin, parallelisation will not work and many other steps will be more difficult. See [more information](/workflows/scenarios_and_directories.qmd).
:::

We can return output to the active session with `returnList` and use `savepath` to save a .rds file. In most cases, we would save outputs so developing and adjusting Comparer outputs does not rely on re-running the aggregations.

The `read_and_agg()` function saves metadata files (yaml and json) that allows replication of this step with `run_hydrobot_params()`, see [here](/workflows/workflow_parameters.qmd). These files build on metadata from earlier steps if possible, including any available metadata from the Controller about module parameters and the scenarios.

## Demonstration

Here, we perform the same set of aggregations as the primary example for [multi_aggregate](using_multi_aggregate.qmd), but do so from the paths to the EWR outputs and note differences. See that notebook for much more detail about how aggregation itself works.

### Directories

First, we need to provide a set of paths to point to the input data, in this case the outputs from the EWR tool for the small demonstration, created by [a controller notebook](/controller/controller_overview.qmd).

Note that we specify a path here for the aggregator results.

```{r}
project_dir <- "hydrobot_scenarios"
hydro_dir <- file.path(project_dir, 'hydrographs')
ewr_results <- file.path(project_dir, "module_output", "EWR")
agg_results <- file.path(project_dir, "aggregator_output", "demo")
```

We can see that those outputs (the csvs) are in scenario-based subdirectories, with the yaml and json metadata for the Controller in the outer directory.

```{r}
list.files(ewr_results, recursive = TRUE)
```

### Scenario information

This will be attached to metadata, typically. For this demonstration, we just use it for plot clarity and the data is simple.

```{r}
multipliers <- c(1.1, 1.5, 2, 3, 4)

scenemults <- c(1 / rev(multipliers), 1, multipliers)

scenenames <- c(
  paste0("down", as.character(rev(multipliers))),
  "base",
  paste0("up", as.character(multipliers))
) |>
  stringr::str_replace("\\.", "_")


scenarios <- tibble::tibble(scenario = scenenames, delta = scenemults)

scene_pal <- make_pal(unique(scenarios$scenario), palette = "ggsci::nrc_npg", refvals = "base", refcols = "black")
```

### Aggregation sequences

We use the same examples as in [multi_aggregate](using_multi_aggregate.qmd). These cover all three dimensions. It begins temporal (all_time), then has two theme aggregations (ewr_code and env_obj), then spatial to sdl_units, two more theme-dimension (Specific_goal, Objective), a spatially-weighted aggregation to the basin, and finally to the theme level of 5-year management targets.

```{r}
aggseq <- list(
  all_time = 'all_time',
  ewr_code = c("ewr_code_timing", "ewr_code"),
  env_obj = c("ewr_code", "env_obj"),
  sdl_units = sdl_units,
  Specific_goal = c("env_obj", "Specific_goal"),
  Objective = c("Specific_goal", "Objective"),
  basin = basin,
  target_5_year_2024 = c("Objective", "target_5_year_2024")
)

funseq <- list(
  all_time = 'ArithmeticMean',
  ewr_code = "CompensatingFactor",
  env_obj = "ArithmeticMean",
  sdl_units = "ArithmeticMean",
  Specific_goal = "ArithmeticMean",
  Objective = "ArithmeticMean",
  basin = 'SpatialWeightedMean',
  target_5_year_2024 = "ArithmeticMean"
  )
```

### Do the aggregation

Now, we use `read_and_agg()` to read the data in, aggregate it, and save it out. We also return it here for making some simple example plots. Rather than using `auto_ewr_PU = TRUE`, we use `group_until` and `pseudo_spatial` as defined in more detail [elsewhere](pseudo_spatial_group_until.qmd).

```{r}
agged_dat <- read_and_agg(
  datpath = ewr_results,
  type = "achievement",
  geopath = bom_basin_gauges,
  causalpath = causal_ewr,
  groupers = "scenario",
  aggCols = "ewr_achieved",
  group_until = list(
    SWSDLName = is_notpoint,
    planning_unit_name = is_notpoint,
    gauge = is_notpoint
  ),
  pseudo_spatial = "sdl_units",
  aggsequence = aggseq,
  funsequence = funseq,
  saveintermediate = TRUE,
  namehistory = FALSE,
  keepAllPolys = FALSE,
  returnList = TRUE,
  savepath = agg_results,
  add_max = FALSE
)
```
That has the same information as the example in [multi_aggregate](using_multi_aggregate.qmd), with 9 levels of aggregation: 

```{r}
names(agged_dat)
```
We will only show one example sheet here.

```{r}
#| message: false
agged_dat$sdl_units |>
  dplyr::filter(env_obj %in% c("EF1", "WB1", "NF1")) %>%
  dplyr::left_join(scenarios) %>%
  plot_outcomes(
    outcome_col = "ewr_achieved",
    plot_type = "map",
    colorgroups = NULL,
    colorset = "ewr_achieved",
    pal_list = list("scico::berlin"),
    pal_direction = -1,
    facet_col = "scenario",
    facet_row = "env_obj",
    sceneorder = c("down4", "base", "up4"),
    underlay_list = list(
      underlay = sdl_units,
      underlay_pal = "grey90"
    )
  )
```

## Metadata

A key advantage of `read_and_agg()` is that it records metadata for data provenance (which is also [runnable](/workflows/workflow_parameters.qmd)).

For the run here, that yaml is at `"hydrobot_scenarios/aggregator_output/demo/agg_metadata.yml"`, and contains the following information:

``` yaml

{{< include /hydrobot_scenarios/aggregator_output/demo/agg_metadata.yml >}}

```

## Parallelization

Since aggregation should happen along the theme, space, and time dimensions, but not scenarios, we can process in parallel over scenarios. The `read_and_agg()` function provides this parallelisation internally and seamlessly, provided the user has the suggested package {furrr} (and its dependency, {future}). In that case, parallelising is as easy as setting a `future::plan` and the argument `rparallel = TRUE`. The exact same run as above can be done in parallel as follows:

```{r}
#| message: false

future::plan(future::multisession)

agged_dat_p <- read_and_agg(
  datpath = ewr_results,
  type = "achievement",
  geopath = bom_basin_gauges,
  causalpath = causal_ewr,
  groupers = "scenario",
  aggCols = "ewr_achieved",
  group_until = list(
    SWSDLName = is_notpoint,
    planning_unit_name = is_notpoint,
    gauge = is_notpoint
  ),
  pseudo_spatial = "sdl_units",
  aggsequence = aggseq,
  funsequence = funseq,
  saveintermediate = TRUE,
  namehistory = FALSE,
  keepAllPolys = FALSE,
  returnList = TRUE,
  savepath = agg_results,
  add_max = FALSE,
  rparallel = TRUE
)
```

That output is the same as earlier, but now it's been read-in and processed in parallel over scenarios. This toy example is only marginally faster, but parallelisation yields large speedups for larger jobs. Because scenarios run independently, massive parallelisation is possible, up to one scenario per core. Speedups can be very large, even on local machines, but are particularly useful on HPCs.

## Sub-directories and multiple aggsequences

Sometimes we might want multiple aggregation sequences to address different parts of a question, or to compare the impact of aggregation itself. Doing that is as simple as creating the multiple aggregation sequences, and typically saving them to a different directory, from which they can be read for the comparer. 

For example, if we wanted to aggregate to Targets (Native fish, Waterbirds, etc) instead of the Objectives and long-run targets for the same EWR output as above, and also use different aggregation functions, we could set up new sequences. Here, we also call the spatial data as characters and define our own Median function, as explained in more detail [here](aggregation_syntax.qmd) and [here](using_multi_aggregate.qmd).
 
```{r}
aggseq_new <- list(
  all_time = 'all_time',
  ewr_code = c("ewr_code_timing", "ewr_code"),
  env_obj = c("ewr_code", "env_obj"),
  sdl_units = "sdl_units",
  Target = c("env_obj", "Target"),
  basin = "basin"
)

Median <- function(x) {
  median(x, na.rm = TRUE)
}

funseq_new <- list(
  all_time = 'ArithmeticMean',
  ewr_code = "LimitingFactor",
  env_obj = "Median",
  sdl_units = "Max",
  Target = "Min",
  basin = 'SpatialWeightedMean'
  )
```

Then, to avoid overwriting the earlier version, since in this example we would want to look at both sets of aggregations, we change the `savepath` argument (along with `aggsequence` and `funsequence`:

```{r}
agged_dat_new <- read_and_agg(
  datpath = ewr_results,
  type = "achievement",
  geopath = bom_basin_gauges,
  causalpath = causal_ewr,
  groupers = "scenario",
  aggCols = "ewr_achieved",
  group_until = list(
    SWSDLName = is_notpoint,
    planning_unit_name = is_notpoint,
    gauge = is_notpoint
  ),
  pseudo_spatial = "sdl_units",
  aggsequence = aggseq_new,
  funsequence = funseq_new,
  saveintermediate = TRUE,
  namehistory = FALSE,
  keepAllPolys = FALSE,
  returnList = TRUE,
  savepath = file.path(project_dir, 'aggregator_output', 'second'),
  add_max = FALSE
)
```
## Value to aggregate

We have demonstrated everything here by aggregating EWR data with the `ewr_achieved` column. However, any numeric column can be aggregated, as we show in @sec-dummy-module for made up module outputs. Here, we also show aggregation on a different EWR metric, the achivement of interevent requirements: 

```{r}
agged_interevent <- read_and_agg(
  datpath = ewr_results,
  type = "achievement",
  geopath = bom_basin_gauges,
  causalpath = causal_ewr,
  groupers = "scenario",
  aggCols = "interevent_achieved",
  group_until = list(
    SWSDLName = is_notpoint,
    planning_unit_name = is_notpoint,
    gauge = is_notpoint
  ),
  pseudo_spatial = "sdl_units",
  aggsequence = aggseq,
  funsequence = funseq,
  saveintermediate = TRUE,
  namehistory = FALSE,
  keepAllPolys = FALSE,
  returnList = TRUE,
  savepath = agg_results,
  add_max = FALSE
)
```
That has the same sheets as above

```{r}
names(agged_interevent)
```

And the map is similar but not identical (interevents are closely related to the frequency, so this is not surprising).

```{r}
#| message: false
agged_interevent$sdl_units |>
  dplyr::filter(env_obj %in% c("EF1", "WB1", "NF1")) %>%
  dplyr::left_join(scenarios) %>%
  plot_outcomes(
    outcome_col = "interevent_achieved",
    plot_type = "map",
    colorgroups = NULL,
    colorset = "interevent_achieved",
    pal_list = list("scico::berlin"),
    pal_direction = -1,
    facet_col = "scenario",
    facet_row = "env_obj",
    sceneorder = c("down4", "base", "up4"),
    underlay_list = list(
      underlay = sdl_units,
      underlay_pal = "grey90"
    )
  )
```

## Using un-integrated response models {#sec-dummy-module}

In some cases, response models may not yet be integrated into HydroBOT, or may not be able to be integrated (e.g. if they are proprietary or unscriptable). In these cases, we can still use `read_and_agg()` for processing with the Aggregator and then Comparer. All we need is a csv of module outputs. For aggregation along the theme, space, and time dimensions, it needs to have information about those dimensions. It should have a column identified by `readr::read_csv()` as time, a column that matches a column in the causal network, and a column that matches a column in a spatial dataframe (sf object), as these last two are accomplished with joins. It will typically have a 'scenario' column as well. We will demonstrate here using `read_and_agg()` with the same examples used for `multi_aggregate()` as demonstrated in [that notebook](using_multi_aggregate.qmd). The difference here is that `read_and_agg()` has to read the files from disk, rather than recieve ready-prepared dataframes.  

### Setup

First, we create some dummy 'module' data, as in [the multi_aggregate example](using_multi_aggregate.qmd), but here we save it out.

We need to know the spatial units to create the data:

```{r}
austates <- rnaturalearth::ne_states(country = 'australia') |> 
  dplyr::select(state = name, geometry)

all_aus <- rnaturalearth::ne_countries(country = 'australia') |> 
  dplyr::select(geounit)
```

Then we create the data and save it out

```{r}
#| label: save-non-module

# This is all copied from multi_aggregate except the last saving line.

  # add a date column
  state_inputs <- austates |>
    dplyr::mutate(date = lubridate::ymd('20000101'))

  # add some values
  withr::with_seed(17,
                   state_inputs <- state_inputs |>
                     dplyr::mutate(value = runif(nrow(state_inputs)))
  )

  withr::with_seed(17,
                   # add some more days, each with different values
                   state_inputs <- purrr::map(0:10,
                                              \(x) dplyr::mutate(state_inputs,
                                                                 date = date + x,
                                                                 value = value * rnorm(nrow(state_inputs),
                                                                                       mean = x, sd = x/2))) |>
                     dplyr::bind_rows()
                   )
  
    # add a scenario column, each with different values
  state_inputs <- purrr::imap(letters[1:4],
                              \(x,y) dplyr::mutate(state_inputs,
                                            scenario = x,
                                            value = value + y)) |>
    dplyr::bind_rows()

    # add a theme-relevant column, each with different values
  state_inputs <- purrr::imap(c("E", "F", "G", "H", "I", "J"),
                              \(x,y) dplyr::mutate(state_inputs,
                                            theme1 = x,
                                            value = value + y)) |>
    dplyr::bind_rows()


  # This bit saves it out so we can use read_and_agg.
  purrr::map(unique(state_inputs$scenario),
             \(x) dir.create(file.path(project_dir, 'module_output', 'fake_module', x), 
                             recursive = TRUE))
  
  si <- state_inputs |> 
    sf::st_drop_geometry() |> 
    split(state_inputs$scenario) |> 
    purrr::iwalk(
      \(x,y) readr::write_csv(x, file.path(project_dir, 'module_output', 'fake_module',
                                       y, paste0("fakeout_", y, '.csv')))
      )
    
  
```

And we need a simple causal network.

```{r}
# make a simple 'causal' network
  state_theme <- tibble::tibble(theme1 = c("E", "F", "G", "H", "I", "J"),
                                theme2 = c("vowel", "consonant", "consonant",
                                           "consonant", "vowel", "consonant")) |> 
    list()
```

### Using the Aggregator

First, we set up some aggregation steps. Will just use means throughout.

```{r}
# This will aggregate into weeks, then to type, and then to the country.
  ausseq <- list(
    week = 'week',
    theme2 = c('theme1', 'theme2'),
    all_aus = all_aus
  )

  # just use mean, since there are no NA in the data.
  ausfuns <- list(
    week = 'mean',
    type = 'mean',
    all_aus = 'mean'
  )
```

Do the aggregation. Note that this warns about some built-in checks for the EWR module that are not relevant here.

```{r}
  # Do the aggregation
  ausagg <-  ausagg <- read_and_agg(
  datpath = file.path(project_dir, 'module_output', 'fake_module'),
  type = "everything",
  geopath = austates,
  causalpath = state_theme,
  groupers = "scenario",
  aggCols = "value",
    aggsequence = ausseq,
  funsequence = ausfuns,
  saveintermediate = TRUE,
  namehistory = FALSE,
  keepAllPolys = FALSE,
  returnList = TRUE,
  add_max = FALSE,
  savepath = file.path(project_dir, 'aggregator_output', 'dummy')
)
```

Quick plots of a couple levels

```{r}
ausagg$week |> 
  filter(theme1 == 'E') |> 
    plot_outcomes(
    outcome_col = "value",
    plot_type = "map",
    colorgroups = NULL,
    colorset = "value",
    pal_list = list("scico::berlin"),
    pal_direction = -1,
    facet_col = "scenario",
    facet_row = "date"
  )
```

```{r}
ausagg$theme2 |> 
  filter(date == lubridate::ymd("2000-01-03")) |> 
    plot_outcomes(
    outcome_col = "value",
    plot_type = "map",
    colorgroups = NULL,
    colorset = "value",
    pal_list = list("scico::berlin"),
    pal_direction = -1,
    facet_col = "scenario",
    facet_row = "theme2"
  )
```

```{r}
ausagg$all_aus |> 
  filter(date == lubridate::ymd("2000-01-03")) |> 
    plot_outcomes(
    outcome_col = "value",
    plot_type = "map",
    colorgroups = NULL,
    colorset = "value",
    pal_list = list("scico::berlin"),
    pal_direction = -1,
    facet_col = "scenario",
    facet_row = "theme2"
  )
```

## Gauge and scenario filtering

We usually want to apply analyses to the full set of scenarios or gauges. However, sometimes we might only want a subset. Using the `gaugefilter` and `scenariofilter` arguments can provide this functionality. Howevern this is a bit dangerous because they are simple regex on the filenames, and so are not actually specific to gauges or scenarios and are only named that way to make the regex simpler if multiple matches are desired. Moreover, if the files do not have a pattern, e.g. there are not filenames with gauge numbers in them and you try to send those numbers in `gaugefilter`, it will error. Since these examples do not have unique gauge files, we only use `scenariofilter`.  

```{r}
#| message: false

smallreadagg <- read_and_agg(
  datpath = ewr_results, 
  type = "achievement",
  geopath = bom_basin_gauges,
  causalpath = causal_ewr,
  groupers = c("scenario", "gauge"),
  aggCols = "ewr_achieved",
  aggsequence = aggseq,
  funsequence = funseq,
  auto_ewr_PU = TRUE,
    add_max = FALSE,
  namehistory = FALSE,
  gaugefilter = NULL,
  scenariofilter = "base"
)

table(smallreadagg$gauge, smallreadagg$scenario)
```

```{r}
#| include: false
#| label: cleanup
withr::deferred_run()
```

