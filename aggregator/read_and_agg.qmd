---
title: "Read_and_agg in detail"
author: "Galen Holt"
format:
  html:
    df-print: paged
    code-link: true # Doesn't do anything without either cran or pkgdown site, btu that's still kind of useful for external (especially {pkgname})
---

```{r}
#| include: false
source("R/helpers.R")
make_hydro_csv()
make_ewr_output()
```

```{r}
#| message: false
library(HydroBOT)
library(dplyr)
library(ggplot2)
```

## Overview

In practice, we often won't call `multi_aggregate()` directly, but will use `read_and_agg()` to run `multi_aggregate()`, since it automates data read-in, processing, parallelisation, metadata, and saving. This requires that the output of the response models is saved out, which is almost always the case for both recordkeeping and processing purposes. To do the same analyses as in the [multi aggregate example](using_multi_aggregate.qmd) but using `read_and_agg()`, we give it the path to the data instead of the data itself. If the directory contains multiple files, `read_and_agg()` provides capacity to operate over those files in parallel.

::: callout-important
Subdirectories should represent scenarios, because scenario outcomes are not interdependent (should be compared, not combined). In contrast, other dimensions (e.g. location) *are* interdependent. If directories are not scenarios but separate units that should be aggregated, e.g. gauges which should be aggregated to basin, parallelisation will not work and many other steps will be more difficult. See [more information](/workflows/scenarios_and_directories.qmd).
:::

We can return output to the active session with `returnList` and use `savepath` to save a .rds file. In most cases, we would save outputs so developing and adjusting Comparer outputs does not rely on re-running the aggregations.

The `read_and_agg()` function saves metadata files (yaml and json) that allows replication of this step with `run_hydrobot_params()`, see [here](/workflows/workflow_parameters.qmd). These files build on metadata from earlier steps if possible, including any available metadata from the Controller about module parameters and the scenarios.

## Demonstration

Here, we perform the same set of aggregations as the primary example for [multi_aggregate](using_multi_aggregate.qmd), but do so from the paths to the EWR outputs and note differences. See that notebook for much more detail about how aggregation itself works.

### Directories

First, we need to provide a set of paths to point to the input data, in this case the outputs from the EWR tool for the small demonstration, created by [a controller notebook](/controller/controller_overview.qmd).

Note that we specify a path here for the aggregator results.

```{r}
project_dir <- "hydrobot_scenarios"
hydro_dir <- file.path(project_dir, 'hydrographs')
ewr_results <- file.path(project_dir, "module_output", "EWR")
agg_results <- file.path(project_dir, "aggregator_output", "demo")
```

We can see that those outputs (the csvs) are in scenario-based subdirectories, with the yaml and json metadata for the Controller in the outer directory.

```{r}
list.files(ewr_results, recursive = TRUE)
```

### Scenario information

This will be attached to metadata, typically. For this demonstration, we just use it for plot clarity and the data is simple.

```{r}
multipliers <- c(1.1, 1.5, 2, 3, 4)

scenemults <- c(1 / rev(multipliers), 1, multipliers)

scenenames <- c(
  paste0("down", as.character(rev(multipliers))),
  "base",
  paste0("up", as.character(multipliers))
) |>
  stringr::str_replace("\\.", "_")


scenarios <- tibble::tibble(scenario = scenenames, delta = scenemults)

scene_pal <- make_pal(unique(scenarios$scenario), palette = "ggsci::nrc_npg", refvals = "base", refcols = "black")
```

### Aggregation sequences

We use the same examples as in [multi_aggregate](using_multi_aggregate.qmd). These cover all three dimensions. It begins temporal (all_time), then has two theme aggregations (ewr_code and env_obj), then spatial to sdl_units, two more theme-dimension (Specific_goal, Objective), a spatially-weighted aggregation to the basin, and finally to the theme level of 5-year management targets.

```{r}
aggseq <- list(
  all_time = 'all_time',
  ewr_code = c("ewr_code_timing", "ewr_code"),
  env_obj = c("ewr_code", "env_obj"),
  sdl_units = sdl_units,
  Specific_goal = c("env_obj", "Specific_goal"),
  Objective = c("Specific_goal", "Objective"),
  basin = basin,
  target_5_year_2024 = c("Objective", "target_5_year_2024")
)

funseq <- list(
  all_time = 'ArithmeticMean',
  ewr_code = "CompensatingFactor",
  env_obj = "ArithmeticMean",
  sdl_units = "ArithmeticMean",
  Specific_goal = "ArithmeticMean",
  Objective = "ArithmeticMean",
  basin = 'SpatialWeightedMean',
  target_5_year_2024 = "ArithmeticMean"
  )
```

### Do the aggregation

Now, we use `read_and_agg()` to read the data in, aggregate it, and save it out. We also return it here for making some simple example plots. Rather than using `auto_ewr_PU = TRUE`, we use `group_until` and `pseudo_spatial` as defined in more detail [elsewhere](pseudo_spatial_group_until.qmd).

```{r}
agged_dat <- read_and_agg(
  datpath = ewr_results,
  type = "achievement",
  geopath = bom_basin_gauges,
  causalpath = causal_ewr,
  groupers = "scenario",
  aggCols = "ewr_achieved",
  group_until = list(
    SWSDLName = is_notpoint,
    planning_unit_name = is_notpoint,
    gauge = is_notpoint
  ),
  pseudo_spatial = "sdl_units",
  aggsequence = aggseq,
  funsequence = funseq,
  saveintermediate = TRUE,
  namehistory = FALSE,
  keepAllPolys = FALSE,
  returnList = TRUE,
  savepath = agg_results,
  add_max = FALSE
)
```
That has the same information as the example in [multi_aggregate](using_multi_aggregate.qmd), with 9 levels of aggregation: 

```{r}
names(agged_dat)
```
We will only show one example sheet here.

```{r}
#| message: false
agged_dat$sdl_units |>
  dplyr::filter(env_obj %in% c("EF1", "WB1", "NF1")) %>%
  dplyr::left_join(scenarios) %>%
  plot_outcomes(
    outcome_col = "ewr_achieved",
    plot_type = "map",
    colorgroups = NULL,
    colorset = "ewr_achieved",
    pal_list = list("scico::berlin"),
    pal_direction = -1,
    facet_col = "scenario",
    facet_row = "env_obj",
    sceneorder = c("down4", "base", "up4"),
    underlay_list = list(
      underlay = sdl_units,
      underlay_pal = "grey90"
    )
  )
```

## Metadata

A key advantage of `read_and_agg()` is that it records metadata for data provenance (which is also [runnable](/workflows/workflow_parameters.qmd)).

For the run here, that yaml is at `"hydrobot_scenarios/aggregator_output/demo/agg_metadata.yml"`, and contains the following information:

``` yaml

{{< include hydrobot_scenarios/aggregator_output/demo/agg_metadata.yml >}}

```

## Parallelization

Since aggregation should happen along the theme, space, and time dimensions, but not scenarios, we can process in parallel over scenarios. The `read_and_agg()` function provides this parallelisation internally and seamlessly, provided the user has the suggested package {furrr} (and its dependency, {future}). In that case, parallelising is as easy as setting a `future::plan` and the argument `rparallel = TRUE`. The exact same run as above can be done in parallel as follows:

```{r}
#| message: false

future::plan(future::multisession)

agged_dat_p <- read_and_agg(
  datpath = ewr_results,
  type = "achievement",
  geopath = bom_basin_gauges,
  causalpath = causal_ewr,
  groupers = "scenario",
  aggCols = "ewr_achieved",
  group_until = list(
    SWSDLName = is_notpoint,
    planning_unit_name = is_notpoint,
    gauge = is_notpoint
  ),
  pseudo_spatial = "sdl_units",
  aggsequence = aggseq,
  funsequence = funseq,
  saveintermediate = TRUE,
  namehistory = FALSE,
  keepAllPolys = FALSE,
  returnList = TRUE,
  savepath = agg_results,
  add_max = FALSE,
  rparallel = TRUE
)
```

That output is the same as earlier, but now it's been read-in and processed in parallel over scenarios. This toy example is only marginally faster, but parallelisation yields large speedups for larger jobs. Because scenarios run independently, massive parallelisation is possible, up to one scenario per core. Speedups can be very large, even on local machines, but are particularly useful on HPCs.

## Sub-directories and multiple aggsequences

Sometimes we might want multiple aggregation sequences to address different parts of a question, or to compare the impact of aggregation itself. Doing that is as simple as creating the multiple aggregation sequences, and typically saving them to a different directory, from which they can be read for the comparer. 

For example, if we wanted to aggregate to Targets (Native fish, Waterbirds, etc) instead of the Objectives and long-run targets for the same EWR output as above, and also use different aggregation functions, we could set up new sequences. Here, we also call the spatial data as characters and define our own Median function, as explained in more detail [here](aggregation_syntax.qmd) and [here](using_multi_aggregate.qmd).
 
```{r}
aggseq_new <- list(
  all_time = 'all_time',
  ewr_code = c("ewr_code_timing", "ewr_code"),
  env_obj = c("ewr_code", "env_obj"),
  sdl_units = "sdl_units",
  Target = c("env_obj", "Target"),
  basin = "basin"
)

Median <- function(x) {
  median(x, na.rm = TRUE)
}

funseq_new <- list(
  all_time = 'ArithmeticMean',
  ewr_code = "LimitingFactor",
  env_obj = "Median",
  sdl_units = "Max",
  Target = "Min",
  basin = 'SpatialWeightedMean'
  )
```

Then, to avoid overwriting the earlier version, since in this example we would want to look at both sets of aggregations, we change the `savepath` argument (along with `aggsequence` and `funsequence`:

```{r}
agged_dat_new <- read_and_agg(
  datpath = ewr_results,
  type = "achievement",
  geopath = bom_basin_gauges,
  causalpath = causal_ewr,
  groupers = "scenario",
  aggCols = "ewr_achieved",
  group_until = list(
    SWSDLName = is_notpoint,
    planning_unit_name = is_notpoint,
    gauge = is_notpoint
  ),
  pseudo_spatial = "sdl_units",
  aggsequence = aggseq_new,
  funsequence = funseq_new,
  saveintermediate = TRUE,
  namehistory = FALSE,
  keepAllPolys = FALSE,
  returnList = TRUE,
  savepath = file.path(project_dir, 'aggregator_output', 'second'),
  add_max = FALSE
)
```

## Using un-integrated response models

At present, there are small parts of `read_and_agg()` that are EWR-specific, and so integrating another response module would be necessary to use `read_and_agg()` for modules other than the EWR tool. However, this is possible with `multi_aggregate()` as demonstrated in [that notebook](using_multi_aggregate.qmd). 

```{r}
#| include: false
austates <- rnaturalearth::ne_states(country = 'australia') |> 
  dplyr::select(state = name, geometry)
```

```{r}
#| include: false
all_aus <- rnaturalearth::ne_countries(country = 'australia') |> 
  dplyr::select(geounit)
```

```{r}
#| include: false
#| label: save-non-module

# This is all copied from multi_aggregate except the last saving line.

  # add a date column
  state_inputs <- austates |>
    dplyr::mutate(date = lubridate::ymd('20000101'))

  # add some values
  withr::with_seed(17,
                   state_inputs <- state_inputs |>
                     dplyr::mutate(value = runif(nrow(state_inputs)))
  )

  withr::with_seed(17,
                   # add some more days, each with different values
                   state_inputs <- purrr::map(0:10,
                                              \(x) dplyr::mutate(state_inputs,
                                                                 date = date + x,
                                                                 value = value * rnorm(nrow(state_inputs),
                                                                                       mean = x, sd = x/2))) |>
                     dplyr::bind_rows()
                   )
  
    # add a scenario column, each with different values
  state_inputs <- purrr::imap(letters[1:4],
                              \(x,y) dplyr::mutate(state_inputs,
                                            scenario = x,
                                            value = value + y)) |>
    dplyr::bind_rows()

    # add a theme-relevant column, each with different values
  state_inputs <- purrr::imap(c("E", "F", "G", "H", "I", "J"),
                              \(x,y) dplyr::mutate(state_inputs,
                                            theme1 = x,
                                            value = value + y)) |>
    dplyr::bind_rows()


  # This bit saves it out so we can use read_and_agg.
  purrr::map(unique(state_inputs$scenario),
             \(x) dir.create(file.path(project_dir, 'module_output', 'fake_module', x), 
                             recursive = TRUE))
  
  si <- state_inputs |> 
    sf::st_drop_geometry() |> 
    split(state_inputs$scenario) |> 
    purrr::iwalk(
      \(x,y) readr::write_csv(x, file.path(project_dir, 'module_output', 'fake_module',
                                       y, paste0("fakeout_", y, '.csv')))
      )
    
  
```

```{r}
#| include: false
# make a simple 'causal' network
  state_theme <- tibble::tibble(theme1 = c("E", "F", "G", "H", "I", "J"),
                                theme2 = c("vowel", "consonant", "consonant",
                                           "consonant", "vowel", "consonant")) |> 
    list()
```



### Using the Aggregator

First, we set up some aggregation steps. Will just use means throughout.

```{r}
# This will aggregate into weeks, then to type, and then to the country.
  ausseq <- list(
    week = 'week',
    theme2 = c('theme1', 'theme2'),
    all_aus = all_aus
  )

  # just use mean, since there are no NA in the data.
  ausfuns <- list(
    week = 'mean',
    type = 'mean',
    all_aus = 'mean'
  )
```

Do the aggregation

```{r}
  # Do the aggregation
  ausagg <-  multi_aggregate(
    dat = state_inputs,
    causal_edges = state_theme,
    groupers = "scenario",
    aggCols = "value",
    aggsequence = ausseq,
    funsequence = ausfuns,
    namehistory = FALSE,
    saveintermediate = TRUE
  )
```

Quick plots of a couple levels

```{r}
ausagg$week |> 
  filter(theme1 == 'E') |> 
    plot_outcomes(
    outcome_col = "value",
    plot_type = "map",
    colorgroups = NULL,
    colorset = "value",
    pal_list = list("scico::berlin"),
    pal_direction = -1,
    facet_col = "scenario",
    facet_row = "date"
  )
```

```{r}
ausagg$theme2 |> 
  filter(date == lubridate::ymd("2000-01-03")) |> 
    plot_outcomes(
    outcome_col = "value",
    plot_type = "map",
    colorgroups = NULL,
    colorset = "value",
    pal_list = list("scico::berlin"),
    pal_direction = -1,
    facet_col = "scenario",
    facet_row = "theme2"
  )
```

```{r}
ausagg$all_aus |> 
  filter(date == lubridate::ymd("2000-01-03")) |> 
    plot_outcomes(
    outcome_col = "value",
    plot_type = "map",
    colorgroups = NULL,
    colorset = "value",
    pal_list = list("scico::berlin"),
    pal_direction = -1,
    facet_col = "scenario",
    facet_row = "theme2"
  )
```

## Gauge and scenario filtering

We usually want to apply analyses to the full set of scenarios or gauges. However, sometimes we might only want a subset. Using the `gaugefilter` and `scenariofilter` arguments can provide this functionality, but is typically not recommended. 

```{r}
#| message: false

smallreadagg <- read_and_agg(
  datpath = ewr_results, type = "summary",
  geopath = bom_basin_gauges,
  causalpath = causal_ewr,
  groupers = c("scenario", "gauge"),
  aggCols = "ewr_achieved",
  aggsequence = aggseq,
  funsequence = funseq,
  namehistory = FALSE,
  gaugefilter = NULL,
  scenariofilter = "base"
)

table(smallreadagg$gauge, smallreadagg$scenario)
```

For a one-off that fits in memory, this is slower than filtering the data after it's in-memory, since the read-in happens first. The advantage comes when we don't want (or can't fit) all of the original data in memory, such as parallelisation over scenarios.

The `read_and_agg` function is also helpful if we just want to use paths as arguments instead of reading the data in and then calling `multi_aggregate`. In that case, we might not use any `*filter` arguments, in which case it works just like `multi_aggregate`, but takes paths instead of objects as arguments. For example, saving all intermediate and no filtering can be done with the path to data `ewr_results`.

```{r}
#| message: false
readallsteps <- read_and_agg(
  datpath = ewr_results, type = "summary",
  geopath = bom_basin_gauges,
  causalpath = causal_ewr,
  groupers = c("scenario", "gauge"),
  aggCols = "ewr_achieved",
  aggsequence = aggseq,
  funsequence = funseq,
  saveintermediate = TRUE,
  namehistory = FALSE
)

names(readallsteps)
```


```{r}
#| include: false
#| label: cleanup
withr::deferred_run()
```

